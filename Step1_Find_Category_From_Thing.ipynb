{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "5c165dca-8c41-4c82-985e-1ac572f80569",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "85b2482b",
    "execution_start": 1650592247699,
    "execution_millis": 19171,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1456
   },
   "source": "%load_ext autoreload\n%autoreload 2\n\nimport wikipedia # used for search of terms, provides: DisambiguationError\n\nimport wikipediaapi # used because can get categorymembers in a given category\nwiki_wiki = wikipediaapi.Wikipedia('en')\n# Might be able to use pywikibot for everything, or at least more\n# https://stackoverflow.com/questions/71023854/how-to-find-subcategories-and-subpages-on-wikipedia-using-pywikibot\nimport pywikibot as pw # used to get AND filter hidden categories for an article\n\nimport graphviz\n\nimport stanza\nstanza.download('en') # download English model\n\n!jupyter nbextension install --py widgetsnbextension\n!jupyter nbextension enable --py widgetsnbextension\n\nnlp = stanza.Pipeline(lang='en', processors='tokenize,lemma,pos,depparse')\n\n# https://www.sbert.net/docs/pretrained_models.html\nfrom sentence_transformers import SentenceTransformer, util\n#model = SentenceTransformer('all-MiniLM-L12-v2')\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\nimport numpy as np\nimport pickle\n\nimport datetime as dt\n\nimport re\nimport requests\nimport time\nfrom collections import Counter\nfrom os.path import exists as file_exists\n ",
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae7c184aa3084612a2d8694ec1ec8747"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "2022-04-22 01:50:50 INFO: Downloading default packages for language: en (English)...\n2022-04-22 01:50:51 INFO: File exists: /root/stanza_resources/en/default.zip.\n2022-04-22 01:50:57 INFO: Finished downloading models and saved to /root/stanza_resources.\nInstalling /root/venv/lib/python3.7/site-packages/widgetsnbextension/static -> jupyter-js-widgets\nUp to date: /usr/local/share/jupyter/nbextensions/jupyter-js-widgets/extension.js\nUp to date: /usr/local/share/jupyter/nbextensions/jupyter-js-widgets/extension.js.map\n- Validating: \u001b[32mOK\u001b[0m\n\n    To initialize this nbextension in the browser every time the notebook (or other app) loads:\n    \n          jupyter nbextension enable widgetsnbextension --py\n    \nEnabling notebook extension jupyter-js-widgets/extension...\nPaths used for configuration of notebook: \n    \t/deepnote-config/jupyter/nbconfig/notebook.json\nPaths used for configuration of notebook: \n    \t\n      - Validating: \u001b[32mOK\u001b[0m\nPaths used for configuration of notebook: \n    \t/deepnote-config/jupyter/nbconfig/notebook.json\n2022-04-22 01:50:59 INFO: Loading these models for language: en (English):\n========================\n| Processor | Package  |\n------------------------\n| tokenize  | combined |\n| pos       | combined |\n| lemma     | combined |\n| depparse  | combined |\n========================\n\n2022-04-22 01:50:59 INFO: Use device: cpu\n2022-04-22 01:50:59 INFO: Loading: tokenize\n2022-04-22 01:50:59 INFO: Loading: pos\n2022-04-22 01:50:59 INFO: Loading: lemma\n2022-04-22 01:50:59 INFO: Loading: depparse\n2022-04-22 01:51:00 INFO: Done loading processors!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "95217fc6-a700-4ed4-87b9-e97f1026db9e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d66942a1",
    "execution_start": 1650592266875,
    "execution_millis": 166,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 171
   },
   "source": "# # https://pypi.org/project/Wikipedia-API/\n# def print_categorymembers(categorymembers, level=0, max_level=1):\n#         for c in categorymembers.values():\n#             print(\"%s: %s (ns: %d)\" % (\"*\" * (level + 1), c.title, c.ns))\n#             if c.ns == wikipediaapi.Namespace.CATEGORY and level < max_level:\n#                 print_categorymembers(c.categorymembers, level=level + 1, max_level=max_level)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "8b0d34e6-532b-4744-9d6e-44fde774afd8",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6297f86",
    "execution_start": 1650592267045,
    "execution_millis": 49,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 153
   },
   "source": "\n# search_term = \"Elon Musk\"\n\n# search_results = wikipedia.search(search_term)\n# print(search_results[:10])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3d7cf02f-cbde-406b-b7c1-5559a8bb33e6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9e5dbf8",
    "execution_start": 1650592267097,
    "execution_millis": 53,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 351
   },
   "source": "# print(search_results)\n# #https://github.com/goldsmith/Wikipedia/issues/295\n\n# first_search_term = search_results[0]\n\n# #https://github.com/goldsmith/Wikipedia/issues/295\n# try:\n#     page = wikipedia.page(first_search_term, auto_suggest=False)    \n# except wikipedia.DisambiguationError:\n#     print(\"Oops! DisambiguationError, trying next result\")\n#     first_search_term = search_results[1]\n#     page = wikipedia.page(first_search_term, auto_suggest=False)\n\n# # add gui for manual disambiguation\n# print(page.summary)\n# #print(page.categories)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "aa29c97d-251c-43e1-b517-43d3fdd5db2f",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e0a9f4cd",
    "execution_start": 1650592267152,
    "execution_millis": 51,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 153
   },
   "source": "\n# # Using stanza instead of nltk to save memory\n# doc = nlp(page.summary)\n# first_sentence = doc.sentences[0]\n# first_sentence.text",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "467d5151-97bf-40c9-8a80-8572e67e8f80",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7a25e8ba",
    "execution_start": 1650592267205,
    "execution_millis": 50,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 297
   },
   "source": "# print (\"{:<15} | {:<10} | {:<15} \".format('Token', 'Relation', 'Head'))\n# print (\"-\" * 50)\n  \n# # Convert sentence object to dictionary  \n# #sent_dict = doc.sentences[0].to_dict()\n# sent_dict = first_sentence.to_dict()\n\n# # iterate to print the token, relation and head\n# for word in sent_dict:\n#   print (\"{:<15} | {:<10} | {:<15} \"\n#          .format(str(word['text']),str(word['deprel']),\\\n#           str(sent_dict[word['head']-1]['text'] \\\n#           if word['head'] > 0 else 'ROOT')))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "cfa66664c707410ba0b34f5913b4767e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4c620edb",
    "execution_start": 1650592267257,
    "execution_millis": 52,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "# mvp_flag",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "db68176a-7c78-47a5-b4e8-ee382b2ba040",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a5fc6046",
    "execution_start": 1650592267357,
    "execution_millis": 39,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1143
   },
   "source": "# # Look for the ROOT word of the dependency tree\n# # hopefully not the first wrod\n# root_id = 0\n# root_word = ''\n# for word in sent_dict:\n#     if word['head'] == 0:\n#         root_id = word['id']\n#         root_word = word['text']\n#         break    \n\n# print(\"first root id:\", root_id)\n# print(\"first root word:\", root_word)\n\n# # Lost TV series for some reason has ROOT as first word, so use what it's connected to as ROOT instead\n# if root_id in [1]:\n#     for word in sent_dict:\n#         if (word['head'] == 1) & (word['deprel'] in ['nsubj:pass', 'parataxis']):\n#             root_id = word['id']\n#             root_word = word['text']\n#             break\n\n# print(\"new root id:\", root_id)\n# print(\"new root word:\", root_word)\n \n\n# # Get all modifiers of ROOT word, loop up to 3 times to get enough words\n# all_dep_ids = []\n\n# for i in range(3): # at most 3 loops\n#     cur_dep_ids = []\n#     for word in sent_dict:\n#         if ((word['head'] in all_dep_ids + [root_id]) & (word['deprel'] in ['obl', 'compound','amod','nmod','conj','appos'])):\n#             cur_dep_ids.append(word['id'])\n\n#     all_dep_ids.extend(cur_dep_ids)\n#     print(i, all_dep_ids)\n#     if len(all_dep_ids) > 3: # bring back at least 3 words, if we have more, then they're too far away\n#         break\n# print(all_dep_ids)\n# all_dep_ids.append(root_id)\n\n# category_phrase_dict = dict()\n# for word in sent_dict:\n#     if (word['id'] in all_dep_ids):\n#         category_phrase_dict[word['id']] = (word['text'], word['deprel'])\n\n# print(category_phrase_dict)\n\n# category_phrase = []\n# for k,v in category_phrase_dict.items():\n#     category_phrase.append(v[0])\n\n# print('category_phrase:', category_phrase)\n# index_root_word = category_phrase.index(root_word)\n# category_phrase = category_phrase[:index_root_word+1]\n\n# category_phrase\n\n# # lop off everything after the root word - didn't work well, pros and cons\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c5510fec-e739-4988-bd2d-9a657a850dbe",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "92df5486",
    "execution_start": 1650592267397,
    "execution_millis": 45,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "# sent_dict",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "b4512ea6-42e0-4115-ba46-2db8488bcb92",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ee489451",
    "execution_start": 1650592267442,
    "execution_millis": 42,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 567
   },
   "source": "# id_word = {}\n# root_id = 0\n# for word in sent_dict:\n#     if word['upos'] != 'PUNCT':\n#         if word['head'] == 0:\n#             root_id = str(word['id']) \n#         id_word[str(word['id'])] = str(word['id']) + ':' + word['text'] + ':' + word['upos'] + ':' + word['xpos']\n\n# # Create Digraph object\n# sent_tree = graphviz.Digraph()\n\n# # Add just the nodes from first traversal of dict\n# for k, v in id_word.items(): \n#     # Add nodes\n#     # https://graphviz.org/doc/info/shapes.html\n#     if k == root_id:\n#         sent_tree.node(k, v, shape='star')    \n#     else:\n#         sent_tree.node(k, v, shape='egg')\n\n\n# # Traverse dict again to add all the relationships\n# for word in sent_dict:\n#     if (word['upos'] != 'PUNCT') & (str(word['head']) != '0'):        \n#         sent_tree.edge(str(word['id']), str(word['head']), label=word['deprel'])\n\n# # Visualize the graph\n# sent_tree.unflatten(stagger=2)  ",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0e9e10a5-7014-4c4e-8c8b-a2f962588f24",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "58d38476",
    "execution_start": 1650592267485,
    "execution_millis": 53,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 334,
    "deepnote_output_heights": [
     21.1875
    ]
   },
   "source": "def get_categories_from_wiki_article(article):\n    # use pywikibot because it can filter hidden 'meta' categories that aren't needed\n    # https://stackoverflow.com/questions/54526821/how-to-identify-wikipedia-categories-in-python\n    site = pw.Site(\"en\", \"wikipedia\")\n    non_hidden = [\n        cat.title()[:]\n        for cat in pw.Page(site, article).categories()\n        if 'hidden' not in cat.categoryinfo\n    ]\n    \n    return(non_hidden)\n\n#get_categories_from_wiki_article('The Avengers')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0345c1b6-4d3a-496a-888f-7d7868396dfe",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a05c856a",
    "execution_start": 1650592267545,
    "execution_millis": 60,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1089
   },
   "source": "def get_best_categories_for_term(wiki_term, wiki_cats, nlp_cat_phrase):\n    # remove parens and anyting inside them\n    wiki_term = re.sub(r'\\([^)]*\\)', '', wiki_term)\n    print('wiki_cats:', wiki_cats)\n    print('')\n\n    # https://stackoverflow.com/questions/65199011/is-there-a-way-to-check-similarity-between-two-full-sentences-in-python\n    rem_term_cats = [' '.join([wiki_term])] + wiki_cats\n    rem_first = model.encode(rem_term_cats)\n\n    print(\"Remove wiki-cats too close to actual term(first term)\")\n    print(rem_term_cats)\n    cos_remove = util.pytorch_cos_sim(rem_first, rem_first)[0].numpy()\n    print(cos_remove)\n    rem_idx = np.where(cos_remove > .7)[0]\n    #because first item is our search term here, but we need to remove items from wiki_cats, so subtract 1\n    rem_idx_from_wiki_cats = rem_idx[1:]-1\n    print(rem_idx_from_wiki_cats)\n    rem_too_close_wiki_cats = [j for i, j in enumerate(wiki_cats) if i not in rem_idx_from_wiki_cats]\n    \n    if len(rem_too_close_wiki_cats) != 0:\n        print('could remove categories too close and have something left.')\n        wiki_cats = rem_too_close_wiki_cats\n    else:\n        print('left removed cats or else none would be left')\n    print('after rem:', wiki_cats)\n\n\n    keep_cat_cats = [' '.join(nlp_cat_phrase)] + wiki_cats\n    keep_first = model.encode(keep_cat_cats)\n\n    print(\"Keep wiki-cats not too far to found category(first term)\")\n    print(keep_cat_cats)\n    keep_cos = util.pytorch_cos_sim(keep_first, keep_first)[0].numpy()\n    print(keep_cos)\n    keep_idx = np.where(keep_cos > .7)[0]\n\n    keep_idx_from_wiki_cats = keep_idx[1:]-1\n\n    print('keep idx:', keep_idx_from_wiki_cats)\n    print('pre-last-filt:', wiki_cats)\n\n    # if we got anything with a decent score, keep those, otherwise everything\n    \n    if len(keep_idx_from_wiki_cats) >= 2:\n        wiki_cats = [j for i, j in enumerate(wiki_cats) if i in keep_idx_from_wiki_cats]\n    else:\n        # keep Top 5 categories if filtering doesn't return much\n        cats_cos = list(zip(keep_cat_cats, keep_cos))\n        top_cats_cos = sorted(cats_cos, key=lambda x: x[1], reverse=True)[1:6]\n        wiki_cats = [i[0] for i in top_cats_cos]\n\n\n\n    print('post-last-filt:', wiki_cats)\n\n    return wiki_cats",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "506c43c6944a4e21aca5fd8fed4dbffb",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a880090d",
    "execution_start": 1650592267652,
    "execution_millis": 44,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 315
   },
   "source": "def get_first_unambiguous_wiki_term_and_page(search_term):\n    search_results = wikipedia.search(search_term)\n\n    first_wiki_term = search_results[0]    \n    #https://github.com/goldsmith/Wikipedia/issues/295\n    try:\n        page = wikipedia.page(first_wiki_term, auto_suggest=False)    \n    except wikipedia.DisambiguationError:\n        print(\"Oops! DisambiguationError, trying next result\")\n        first_wiki_term = search_results[1]\n        page = wikipedia.page(first_wiki_term, auto_suggest=False)\n\n    print('first_wiki_term:', first_wiki_term)\n    return first_wiki_term, page",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "79157f5e58544eccb0e3beffdf0c921b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b5e4c19b",
    "execution_start": 1650592267697,
    "execution_millis": 43,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 153
   },
   "source": "def get_stanza_dict_of_first_sentence(wiki_summary_text):\n    # Using stanza instead of nltk to save memory\n    doc = nlp(wiki_summary_text)\n    return doc.sentences[0].to_dict()\n    #first_sentence.text",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a95664ab78ad42149f1a97d86eaad459",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "be0c4ea5",
    "execution_start": 1650592267741,
    "execution_millis": 35,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "#first_wiki_term, wiki_page = get_first_unambiguous_wiki_term_and_page('Gamestop short squeeze')\n#graph_sent(get_stanza_dict_of_first_sentence(wiki_page.summary))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ca7f6bde2617453a883dfdf42d3bfe6c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f33b6a45",
    "execution_start": 1650592267783,
    "execution_millis": 55,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1341
   },
   "source": "def get_nlp_category_phrase(wiki_page):\n    \n    wiki_page_text = wiki_page.summary\n    # # Using stanza instead of nltk to save memory\n    # doc = nlp(wiki_page_text)\n    # first_sentence = doc.sentences[0]\n    # #first_sentence.text\n    sent_dict = get_stanza_dict_of_first_sentence(wiki_page_text)\n\n    # Look for the ROOT word of the dependency tree\n    # hopefully not the first word\n    root_id = 0\n    root_word = ''\n    for word in sent_dict:\n        if word['head'] == 0:\n            root_id = word['id']\n            root_word = word['text']\n            break    \n\n    # print(\"first root id:\", root_id)\n    # print(\"first root word:\", root_word)            \n\n    # Lost TV series for some reason has ROOT as first word\n    if root_id in [1]:\n        for word in sent_dict:\n            if (word['head'] == 1) & (word['deprel'] in ['nsubj:pass', 'parataxis']):\n                root_id = word['id']                \n                root_word = word['text']\n                break\n\n    # print(\"new root id:\", root_id)\n    # print(\"new root word:\", root_word)\n\n    # Get all modifiers of ROOT word, loop up to 3 times to get enough words\n    all_dep_ids = []\n\n    for i in range(3):  # at most 3 loops\n        cur_dep_ids = []\n        for word in sent_dict:\n            if ((word['head'] in all_dep_ids + [root_id]) & (word['deprel'] in ['obl', 'compound','amod','nmod','conj','appos'])):\n                cur_dep_ids.append(word['id'])\n\n        all_dep_ids.extend(cur_dep_ids)\n        #print(i, all_dep_ids)\n        if len(all_dep_ids) > 2:    # bring back at least 4 words, if we have more, then they're too far away\n            break\n    #print(all_dep_ids)\n    all_dep_ids.append(root_id)\n\n    category_phrase_dict = dict()\n    for word in sent_dict:\n        if (word['id'] in all_dep_ids):\n            category_phrase_dict[word['id']] = word['text']\n\n    #print(category_phrase_dict)\n\n    nlp_category_phrase = []\n    for k,v in category_phrase_dict.items():\n        nlp_category_phrase.append(v)\n    #print('**', search_term, '**', first_wiki_term, '**', nlp_category_phrase)\n    #print(wiki_page.categories)\n\n    # print(nlp_category_phrase)\n    index_root_word = nlp_category_phrase.index(root_word)\n    \n    # this was an attempt to cut off the phrases at the root word, but some continue past it, iPhone for example\n    #nlp_category_phrase = nlp_category_phrase[:index_root_word+1]\n\n    print('nlp phrase:', nlp_category_phrase)\n    return nlp_category_phrase\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "f8d4a36d-74c3-4c6a-b98a-4fc5f90944dc",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a6156eb2",
    "execution_start": 1650592267840,
    "execution_millis": 53,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 333
   },
   "source": "def get_category_from_search_term(search_term, mvp_flag):   \n    \n    first_wiki_term, wiki_page = get_first_unambiguous_wiki_term_and_page(search_term)\n\n    nlp_category_phrase = get_nlp_category_phrase(wiki_page)\n\n    raw_wiki_cats = get_categories_from_wiki_article(first_wiki_term)\n\n    best_wiki_cats = get_best_categories_for_term(first_wiki_term, raw_wiki_cats, nlp_category_phrase)\n\n    expanded_year_wiki_cats = get_all_combined_wiki_cats(best_wiki_cats)\n\n    make_white_list_cat_content_files(expanded_year_wiki_cats, first_wiki_term, mvp_flag)\n\n    return nlp_category_phrase, expanded_year_wiki_cats, best_wiki_cats, first_wiki_term",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0556ab71-2a7f-4b7e-a29c-9f0025b162b5",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650592267897,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3e50ee7188af4e538211787b2021583e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650592267910,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e9e03895-64b9-4cbe-8a35-933cbb247596",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650592267910,
    "execution_millis": 42,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "970109d2-d932-4b3d-a78b-0617a6f8722d",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f49348e0",
    "execution_start": 1650592267953,
    "execution_millis": 7,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 225
   },
   "source": "def get_wiki_wiki_pages_for_cat_members(category):\n    wiki_cat = ''\n    try:\n        wiki_cat = wiki_wiki.page(category)        \n    except requests.exceptions.SSLError:\n        print(\"SSLError exception caught!!!!!!!!!!!!!!!\")\n        time.sleep(5)\n        wiki_cat = wiki_wiki.page(category)\n    return wiki_cat",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "183082e5-f014-423e-9e88-aae9818bbbb8",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "84893082",
    "execution_start": 1650592268008,
    "execution_millis": 772,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 886.75,
    "deepnote_output_heights": [
     null,
     59.5625
    ]
   },
   "source": "test_wiki_cat = \"Category:Board games introduced in 1995\"\ndef return_new_year_cats(wiki_cat):\n    year_pattern = re.compile('(19|20)\\d{2}s?')\n    check_for_year = re.search(year_pattern, wiki_cat)\n\n    curr_year = dt.datetime.now().year\n\n    new_cat_list = []\n\n    if check_for_year != None:\n        print('found', check_for_year.group(0), 'in category:', wiki_cat)        \n        found_year = check_for_year.group(0)\n\n        if 's' in found_year:\n            new_category = wiki_cat.replace(found_year, '2020s')\n            print(new_category)\n            #check category exists\n            pages_in_new_cat = len(get_wiki_wiki_pages_for_cat_members(new_category).categorymembers.keys())\n            if pages_in_new_cat > 0:\n                # add cat to existing list\n                print(pages_in_new_cat, 'exists!')\n                new_cat_list.append(new_category)\n        else:\n            for pot_year in [str(x) for x in [curr_year - i for i in range(5)]]:\n                \n                #print(pot_year)\n                new_category = wiki_cat.replace(found_year, pot_year)\n                #check category exists\n                pages_in_new_cat = len(get_wiki_wiki_pages_for_cat_members(new_category).categorymembers.keys())\n                if pages_in_new_cat > 0:\n                    # add cat to existing list\n                    #print(pages_in_new_cat, 'exists!')\n                    new_cat_list.append(new_category)\n                \n\n    return new_cat_list\n\nreturn_new_year_cats(test_wiki_cat)\n",
   "outputs": [
    {
     "name": "stdout",
     "text": "found 1995 in category: Category:Board games introduced in 1995\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 19,
     "data": {
      "text/plain": "['Category:Board games introduced in 2020',\n 'Category:Board games introduced in 2019',\n 'Category:Board games introduced in 2018']"
     },
     "metadata": {}
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c2c0aad1-6879-496e-bb71-e2241f487e3e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a60b50ed",
    "execution_start": 1650592268469,
    "execution_millis": 52,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 369
   },
   "source": "def expand_years_in_cats_to_modern(list_wiki_cats):\n    new_year_cats_to_add = []\n    # SSLError: HTTPSConnectionPool(host='en.wikipedia.org', port=443)\n\n    for cat in list_wiki_cats:    \n        #print(\"****\", cat)\n        try:\n            new_year_cats_to_add.append(return_new_year_cats(cat))\n\n        except requests.exceptions.SSLError:\n            print(\"SSLError exception caught!!!!!!!!!!!!!!!\")\n            time.sleep(5)\n            new_year_cats_to_add.append(return_new_year_cats(cat))   \n\n    return new_year_cats_to_add\n\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "bab81a13-9a7d-4201-98e5-eaf506d70108",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6c692466",
    "execution_start": 1650592268523,
    "execution_millis": 51,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 189
   },
   "source": "def get_all_combined_wiki_cats(list_wiki_cats):\n    combined_wiki_cats = list_wiki_cats\n    for year_cats in expand_years_in_cats_to_modern(list_wiki_cats):\n        combined_wiki_cats = list(set(combined_wiki_cats + year_cats))\n    return combined_wiki_cats\n\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3c0d01a7-48b8-4f70-b0c1-29d19a6ae0c7",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e472789e",
    "execution_start": 1650592268585,
    "execution_millis": 45,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 927
   },
   "source": "def make_cat_content_dict_from_cats(wiki_cats, wiki_term, mvp_flag, max_num_wiki_articles_per_cat = 200):\n    # given the filtered cats, try getting the summaries for all the pages mentioned into a dict, to pass to Kim\n       \n    page_cat_content_dict = dict()\n    \n    pkl_file_name = './output_step1/wiki_200_cat_content_' + wiki_term.replace(' ', '_') + '.pickle'\n\n    if mvp_flag and file_exists(pkl_file_name):\n        print('mvp! wikipedia cat content already exists.')\n        with open(pkl_file_name, 'rb') as handle:\n            page_cat_content_dict = pickle.load(handle)\n    else:\n\n        for idx, cat in enumerate(wiki_cats):\n                \n            page_content_dict = dict()\n\n            try:       \n                cat_page_member_keys = get_wiki_wiki_pages_for_cat_members(cat).categorymembers.keys()\n            except requests.exceptions.SSLError:\n                print(\"SSLError exception caught!!!!!!!!!!!!!!!\")\n                time.sleep(5)\n                cat_page_member_keys = get_wiki_wiki_pages_for_cat_members(cat).categorymembers.keys()\n\n            print(cat, idx+1, 'of', len(wiki_cats), ':', len(cat_page_member_keys))\n            remove_from_pages = ['Category:', 'List of', 'Comparison of'] + [wiki_term]\n\n            # list_of_filt_pages = [page for page in list(cat_page_member_keys)[:100] if not any(x.lower() in page.lower() for x in remove_from_pages)]\n            list_of_filt_pages = [page for page in list(cat_page_member_keys) if not any(x.lower() in page.lower() for x in remove_from_pages)][:max_num_wiki_articles_per_cat]\n\n            for filt_page in list_of_filt_pages:            \n                try:                \n                    page_content_dict[filt_page] = wikipedia.page(filt_page, auto_suggest=False).content                \n                except wikipedia.DisambiguationError:\n                    print(\"Oops! DisambiguationError, trying next result\")                \n                    continue\n            \n            page_cat_content_dict[cat] = page_content_dict\n\n        print('Dumping wikipedia cat content file')\n        with open(pkl_file_name, 'wb') as handle:\n            pickle.dump(page_cat_content_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n    return page_cat_content_dict\n\n    # need to catch timeout here, for hitting API too often\n\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "63d5a7739ea24ba1bac72020535c458c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650592268681,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "01f7352ce1474621b61b799754ee7e93",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "77f66f77",
    "execution_start": 1650592268683,
    "execution_millis": 42,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "#wiki_cats",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "faa5e9a1-4b87-4872-84ed-a4a4901eaa58",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "faef2f6",
    "execution_start": 1650592268725,
    "execution_millis": 48,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 549
   },
   "source": "def get_whitelist_dicts_by_cat(wiki_cats, wiki_term):\n    #print(wiki_cats)\n    # to get whitelist of all terms to search for in sentences from Kim deemed to be related to our category\n    white_list_dict = dict()\n    for cat in wiki_cats:\n        cat_page = get_wiki_wiki_pages_for_cat_members(cat)\n        print(cat)#, len(cat_page.categorymembers.keys()))\n        remove_from_pages = ['Category:', 'List of', 'Comparison of'] + [wiki_term]\n\n        list_of_filt_pages = [page for page in cat_page.categorymembers.keys() if not any(x.lower() in page.lower() for x in remove_from_pages)]\n\n        # get rid of everything between parens like this: 'Chimera (South Korean TV series)',\n        # https://stackoverflow.com/questions/29570771/re-sub-on-lists-python-3\n        # https://stackoverflow.com/questions/640001/how-can-i-remove-text-within-parentheses-with-a-regex\n        # https://stackoverflow.com/questions/71023854/how-to-find-subcategories-and-subpages-on-wikipedia-using-pywikibot\n        \n        list_of_filt_pages_nothing_in_parens = [re.sub(r'\\([^)]*\\)', '', i) for i in list_of_filt_pages]\n\n        list_of_filt_pages_no_parens = [i.replace('(', '').replace(')', '') for i in list_of_filt_pages]\n\n        list_of_filt_pages_no_parens_strip = list(set([i.strip() for i in list_of_filt_pages_no_parens]))\n\n        white_list_dict[cat] = list_of_filt_pages_no_parens_strip\n\n    return white_list_dict\n\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e4eb500305a542b4ae842b07fd0f971c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3f1b10ba",
    "execution_start": 1650592268774,
    "execution_millis": 47,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 531
   },
   "source": "def filter_whitelist_dict_for_common_words(white_list_dict, wiki_term):\n    filt_white_list = dict()\n    # remove words that are too common in each category\n    # rule: appear at least 10 times, and be in greater than 20% of all entries\n\n    for k, v in white_list_dict.items(): # {category:list of titles}\n        print(len(v), k, v)\n        words = Counter()\n        for phrase in v:\n            words.update(phrase.split())\n        print(words)\n        \n        words_to_remove = []\n        for word, count in words.most_common():\n            if (count < 10):\n                break\n            if (count/len(v) > .2):\n                words_to_remove.append(word)\n\n        print(\"remove:\", words_to_remove)\n\n        p = re.compile('|'.join(map(re.escape, words_to_remove))) # escape to handle metachars\n        filt_white_list[k] = [' '.join(p.sub('', s).split()) for s in v]\n\n    return filt_white_list\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d7e6d2c2a0ec4a9b874b481b34ab6239",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fb62b46c",
    "execution_start": 1650592268822,
    "execution_millis": 43,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 351
   },
   "source": "def make_white_list_cat_content_files(wiki_cats, wiki_term, mvp_flag):\n    make_cat_content_dict_from_cats(wiki_cats, wiki_term, mvp_flag)\n    \n    white_list_dict = get_whitelist_dicts_by_cat(wiki_cats, wiki_term)\n    white_list_dict\n\n    filt_white_list = filter_whitelist_dict_for_common_words(white_list_dict, wiki_term)\n\n    filt_white_list\n\n    \n\n    with open('./output_step1/white_list_200_' + wiki_term.replace(' ', '_') + '.pickle', 'wb') as handle:\n        pickle.dump(filt_white_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n    print(wiki_term)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "7f6a58c7074845b9b135223e84c4fc04",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3b3dae2b",
    "execution_start": 1650592292036,
    "execution_millis": 73930,
    "owner_user_id": "9187ece3-39a6-4bea-aed6-9a68f93aaf4f",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 991.4375
   },
   "source": "test_terms = ['model t', 'squid game', 'lost tv', 'apple iphone', \\\n                'obama', 'trump', 'subway tile', 'sport', 'mandalorian',\\\n                'gamestop', 'ps5', 'bitcoin', 'ketogenic diet',\\\n                'dogecoin', 'ford bronco', 'ted lasso', 'pickle ball',\\\n                'data science', 'crocs', 'terraforming mars', 'final fantasy',\\\n                'catan', 'steve jobs', 'batman', 'tesla motors', 'cronut', 'cryptocurrency',\\\n                'elon musk', 'beastie boys', 'birkin bag', 'Game Stop Short Squeeze', 'oxycontin', 'Virtual reality']\n\n#term = 'Mother 3'\n#mvp_flag = True\n\n#nlp_cat_phrase, wiki_cats, init_wiki_cats, wiki_term = get_category_from_search_term(term, mvp_flag)\n# # # Try search for all categories that are cosine close to search category\n\n# make_white_list_cat_content_files(wiki_cats, wiki_term, mvp_flag)",
   "outputs": [
    {
     "name": "stdout",
     "text": "first_wiki_term: Mother 3\nnlp phrase: ['2006', 'role-playing', 'video', 'game']\nwiki_cats: ['Category:2006 video games', 'Category:Brownie Brown games', 'Category:Cancelled 64DD games', 'Category:Cancelled Nintendo 64 games', 'Category:Cancelled Super Nintendo Entertainment System games', 'Category:Dinosaurs in video games', 'Category:Fiction about mind control', 'Category:Game Boy Advance games', 'Category:HAL Laboratory games', 'Category:Japan-exclusive video games', 'Category:Japanese role-playing video games', 'Category:Media containing Gymnopedies', 'Category:Mother (video game series)', 'Category:Nintendo games', 'Category:Science fantasy video games', 'Category:Science fiction comedy', 'Category:Single-player video games', 'Category:Suicide in fiction', 'Category:Vaporware video games', 'Category:Video game sequels', 'Category:Video games about psychic powers', 'Category:Video games developed in Japan', 'Category:Video games scored by Shogo Sakai', 'Category:Video games set on fictional islands', 'Category:Video games with oblique graphics', 'Category:Virtual Console games for Wii U', 'Category:Works about twin brothers']\n\nRemove wiki-cats too close to actual term(first term)\n['Mother 3', 'Category:2006 video games', 'Category:Brownie Brown games', 'Category:Cancelled 64DD games', 'Category:Cancelled Nintendo 64 games', 'Category:Cancelled Super Nintendo Entertainment System games', 'Category:Dinosaurs in video games', 'Category:Fiction about mind control', 'Category:Game Boy Advance games', 'Category:HAL Laboratory games', 'Category:Japan-exclusive video games', 'Category:Japanese role-playing video games', 'Category:Media containing Gymnopedies', 'Category:Mother (video game series)', 'Category:Nintendo games', 'Category:Science fantasy video games', 'Category:Science fiction comedy', 'Category:Single-player video games', 'Category:Suicide in fiction', 'Category:Vaporware video games', 'Category:Video game sequels', 'Category:Video games about psychic powers', 'Category:Video games developed in Japan', 'Category:Video games scored by Shogo Sakai', 'Category:Video games set on fictional islands', 'Category:Video games with oblique graphics', 'Category:Virtual Console games for Wii U', 'Category:Works about twin brothers']\n[1.0000001  0.17153281 0.16137926 0.1626468  0.1854989  0.1815439\n 0.14026144 0.09337018 0.24522595 0.08325648 0.11740737 0.12213674\n 0.09579008 0.5926278  0.22187085 0.1362668  0.12012753 0.13617006\n 0.1384219  0.14150462 0.22271976 0.17854764 0.06603663 0.09719418\n 0.08955178 0.13065107 0.11617889 0.23232335]\n[]\ncould remove categories too close and have something left.\nafter rem: ['Category:2006 video games', 'Category:Brownie Brown games', 'Category:Cancelled 64DD games', 'Category:Cancelled Nintendo 64 games', 'Category:Cancelled Super Nintendo Entertainment System games', 'Category:Dinosaurs in video games', 'Category:Fiction about mind control', 'Category:Game Boy Advance games', 'Category:HAL Laboratory games', 'Category:Japan-exclusive video games', 'Category:Japanese role-playing video games', 'Category:Media containing Gymnopedies', 'Category:Mother (video game series)', 'Category:Nintendo games', 'Category:Science fantasy video games', 'Category:Science fiction comedy', 'Category:Single-player video games', 'Category:Suicide in fiction', 'Category:Vaporware video games', 'Category:Video game sequels', 'Category:Video games about psychic powers', 'Category:Video games developed in Japan', 'Category:Video games scored by Shogo Sakai', 'Category:Video games set on fictional islands', 'Category:Video games with oblique graphics', 'Category:Virtual Console games for Wii U', 'Category:Works about twin brothers']\nKeep wiki-cats not too far to found category(first term)\n['2006 role-playing video game', 'Category:2006 video games', 'Category:Brownie Brown games', 'Category:Cancelled 64DD games', 'Category:Cancelled Nintendo 64 games', 'Category:Cancelled Super Nintendo Entertainment System games', 'Category:Dinosaurs in video games', 'Category:Fiction about mind control', 'Category:Game Boy Advance games', 'Category:HAL Laboratory games', 'Category:Japan-exclusive video games', 'Category:Japanese role-playing video games', 'Category:Media containing Gymnopedies', 'Category:Mother (video game series)', 'Category:Nintendo games', 'Category:Science fantasy video games', 'Category:Science fiction comedy', 'Category:Single-player video games', 'Category:Suicide in fiction', 'Category:Vaporware video games', 'Category:Video game sequels', 'Category:Video games about psychic powers', 'Category:Video games developed in Japan', 'Category:Video games scored by Shogo Sakai', 'Category:Video games set on fictional islands', 'Category:Video games with oblique graphics', 'Category:Virtual Console games for Wii U', 'Category:Works about twin brothers']\n[1.         0.69070894 0.26865914 0.2826099  0.27937022 0.3146136\n 0.44859776 0.15849654 0.48202366 0.38197666 0.46327206 0.6764268\n 0.11965141 0.42000872 0.4163032  0.4810283  0.20195659 0.54589117\n 0.13659339 0.4862818  0.478755   0.45040393 0.4557313  0.41551337\n 0.44263518 0.49431378 0.41792762 0.06421155]\nkeep idx: []\npre-last-filt: ['Category:2006 video games', 'Category:Brownie Brown games', 'Category:Cancelled 64DD games', 'Category:Cancelled Nintendo 64 games', 'Category:Cancelled Super Nintendo Entertainment System games', 'Category:Dinosaurs in video games', 'Category:Fiction about mind control', 'Category:Game Boy Advance games', 'Category:HAL Laboratory games', 'Category:Japan-exclusive video games', 'Category:Japanese role-playing video games', 'Category:Media containing Gymnopedies', 'Category:Mother (video game series)', 'Category:Nintendo games', 'Category:Science fantasy video games', 'Category:Science fiction comedy', 'Category:Single-player video games', 'Category:Suicide in fiction', 'Category:Vaporware video games', 'Category:Video game sequels', 'Category:Video games about psychic powers', 'Category:Video games developed in Japan', 'Category:Video games scored by Shogo Sakai', 'Category:Video games set on fictional islands', 'Category:Video games with oblique graphics', 'Category:Virtual Console games for Wii U', 'Category:Works about twin brothers']\npost-last-filt: ['Category:2006 video games', 'Category:Japanese role-playing video games', 'Category:Single-player video games', 'Category:Video games with oblique graphics', 'Category:Vaporware video games']\nfound 2006 in category: Category:2006 video games\nCategory:Vaporware video games 1 of 10 : 56\nCategory:2022 video games 2 of 10 : 60\nCategory:2006 video games 3 of 10 : 876\n",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2f716ee5e879>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmvp_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnlp_cat_phrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_wiki_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_category_from_search_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmvp_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# # # Try search for all categories that are cosine close to search category\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-d142bf46cd86>\u001b[0m in \u001b[0;36mget_category_from_search_term\u001b[0;34m(search_term, mvp_flag)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mexpanded_year_wiki_cats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_combined_wiki_cats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_wiki_cats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmake_white_list_cat_content_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_year_wiki_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_wiki_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmvp_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp_category_phrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_year_wiki_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_wiki_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_wiki_term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-b7a020fe7a09>\u001b[0m in \u001b[0;36mmake_white_list_cat_content_files\u001b[0;34m(wiki_cats, wiki_term, mvp_flag)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_white_list_cat_content_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmvp_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmake_cat_content_dict_from_cats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmvp_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwhite_list_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_whitelist_dicts_by_cat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_cats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki_term\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwhite_list_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-21b6620e9d4d>\u001b[0m in \u001b[0;36mmake_cat_content_dict_from_cats\u001b[0;34m(wiki_cats, wiki_term, mvp_flag, max_num_wiki_articles_per_cat)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilt_page\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_filt_pages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0mpage_content_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilt_page\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilt_page\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_suggest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisambiguationError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Oops! DisambiguationError, trying next result\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/wikipedia/wikipedia.py\u001b[0m in \u001b[0;36mpage\u001b[0;34m(title, pageid, auto_suggest, redirect, preload)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;31m# if there is no suggestion or search results, the page doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mPageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mpageid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mWikipediaPage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpageid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpageid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/wikipedia/wikipedia.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, title, pageid, redirect, preload, original_title)\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Either a title or a pageid must be specified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mredirect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpreload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/wikipedia/wikipedia.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self, redirect, preload)\u001b[0m\n\u001b[1;32m    334\u001b[0m       \u001b[0mquery_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pageids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpageid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wiki_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/wikipedia/wikipedia.py\u001b[0m in \u001b[0;36m_wiki_request\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m   \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPI_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mRATE_LIMIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \"\"\"\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    527\u001b[0m         }\n\u001b[1;32m    528\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m                 )\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m             )\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                 \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;31m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"user-agent\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_default_user_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTTPConnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequest_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1279\u001b[0m                 encode_chunked=False):\n\u001b[1;32m   1280\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0;31m# default charset of iso-8859-1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\\r\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotConnected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m             )\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "686bea5b231640a9bad70dd351059bd4",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "9294afce",
    "execution_start": 1650577226062,
    "execution_millis": 51,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 117
   },
   "source": "# for term in test_terms:\n#     first_wiki_term, wiki_page = get_first_unambiguous_wiki_term_and_page(term)\n#     print(term, ':', first_wiki_term, ':', get_nlp_category_phrase(wiki_page))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e8dc11e71c34452fb9367d4499281662",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "8c12abcd",
    "execution_start": 1650577226116,
    "execution_millis": 49,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "# expanded_wiki_cats = \n# print(wiki_cats)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "01951f9cdd56477abe7ef2de1f899326",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "356eaf3f",
    "execution_start": 1650577226212,
    "execution_millis": 44,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "##expand_years_in_cats_to_modern(wiki_cats)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "48311d4657b641b28a00b71e6cd089c2",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b5aa4f56",
    "execution_start": 1650577226257,
    "execution_millis": 425,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 222.375
   },
   "source": "%%time\n\n\n\n\n",
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 4 Âµs, sys: 0 ns, total: 4 Âµs\nWall time: 8.11 Âµs\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6094baef766a487b9262b8e4f7792821",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "cb4ea6e1",
    "execution_start": 1650577226306,
    "execution_millis": 38,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "# for k,v in page_cat_content_dict.items():\n#     print(k, len(v))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a866d8ab593844b68118e482eb604d6a",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "77f66f77",
    "execution_start": 1650577226345,
    "execution_millis": 46,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "#wiki_cats",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "48e9de0c51214766acb9dd0d01b96295",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b31fd813",
    "execution_start": 1650577226436,
    "execution_millis": 16931472,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "790cf476a4c94d7d908965f86167d627",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "681f4679",
    "execution_start": 1650577226437,
    "execution_millis": 44,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "#white_list_dict.keys()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a20242ab9c734fd78c97cc1f9b330180",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "execution_start": 1650577226481,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ae299f11-76fe-443e-94af-94fedcbe782e",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "9c1555b4",
    "execution_start": 1650577226482,
    "execution_millis": 42,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "# with open('./output_step1/wiki_100summaries_' + wiki_term.replace(' ', '') + '.pickle', 'wb') as handle:\n#     pickle.dump(page_summary_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1370b674-a678-455f-9a73-805949b4cd88",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "execution_start": 1650577226525,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "8eda2ea9-9ace-4be2-8098-a66d83257e8e",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "9b9dbcce",
    "execution_start": 1650577226526,
    "execution_millis": 54,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 153
   },
   "source": "# test_model = model.encode(['catan', 'wingspan', 'Bananagrams', 'missle'])\n\n# cos_test = util.pytorch_cos_sim(test_model, test_model)[0].numpy()\n# print(cos_test)\n# #rem_idx = np.where(cos_remove > .7)[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "fb254207-ec77-41f1-9e37-65616c87bd3e",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "eea86363",
    "execution_start": 1650577226624,
    "execution_millis": 46,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 117
   },
   "source": "# # attempt at searching for more categories, if what we found above isn't enough\n# search_results = wikipedia.search('Category:dietary therapy', results=100, suggestion=False)\n# print(search_results)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "8defb22e-7e7f-49dd-9cc3-3c6d9bd59280",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "36b441b2",
    "execution_start": 1650577226671,
    "execution_millis": 46,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "# wikipedia.page('List of board games', auto_suggest=False).summary",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6de77bd0-d457-44fe-a236-93a471448d74",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "bbc4525",
    "execution_start": 1650577226718,
    "execution_millis": 43,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 369
   },
   "source": "\n\n# site = pw.Site(\"en\", \"wikipedia\")\n# print([\n#     cat.title()\n#     for cat in pw.Page(site, 'support-vector machine').categories()\n#     if 'hidden' not in cat.categoryinfo\n# ])\n\n# p = pw.Page(site, 'support-vector machine')\n# list(p.categories())\n\n# for cat in p.categories():\n#     print(cat)\n#     print(cat.categoryinfo)\n#     print()\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "7c8c1d84-5076-400b-a753-408e8bacf748",
    "tags": [],
    "deepnote_to_be_reexecuted": true,
    "source_hash": "b623e53d",
    "execution_start": 1650577226762,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=90b052a7-f47d-474e-888f-9345355cfd9a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "66542857-f646-490d-b341-dadf1760bc19",
  "deepnote_execution_queue": []
 }
}