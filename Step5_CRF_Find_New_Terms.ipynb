{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d65a962864e949f3be65bb8e64e21a1e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650851744935,
    "execution_millis": 2,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": null,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "fff9c202-f3eb-414b-8e09-6e41c40bc822",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a519c074",
    "execution_start": 1650851744948,
    "execution_millis": 23960,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 6,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1215.625,
    "deepnote_output_heights": [
     607
    ]
   },
   "source": "import pickle\nimport re\n\nimport stanza\nstanza.download('en') # download English model\nnlp = stanza.Pipeline(lang='en', processors='tokenize,pos')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom itertools import chain, groupby\n\nimport pycrfsuite\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\n#import itertools\nfrom collections import Counter, OrderedDict\nfrom os.path import exists as file_exists\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6972129a9d83495aa32db15081859c6a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "2022-04-25 01:55:56 INFO: Downloading default packages for language: en (English)...\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.4.0/models/default.zip:   0%|          | 0â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33be2140bd8f491c9b84c3934405b965"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "2022-04-25 01:56:05 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9b6566f36834fd9b27c08f3ad48da6c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "2022-04-25 01:56:06 INFO: Loading these models for language: en (English):\n========================\n| Processor | Package  |\n------------------------\n| tokenize  | combined |\n| pos       | combined |\n========================\n\n2022-04-25 01:56:06 INFO: Use device: cpu\n2022-04-25 01:56:06 INFO: Loading: tokenize\n2022-04-25 01:56:06 INFO: Loading: pos\n2022-04-25 01:56:06 INFO: Done loading processors!\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "60b61936db604bf188c3926fc7e308d3",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4cc3f422",
    "execution_start": 1650852473679,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 210,
     "w": 12,
     "h": 5
    },
    "owner_user_id": "9187ece3-39a6-4bea-aed6-9a68f93aaf4f",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 153
   },
   "source": "def remove_spaces_from_term(term_with_spaces):\n    return term_with_spaces.replace(' ', '')\n\ndef swap_spaces_for_underscore(term_with_spaces):\n    return term_with_spaces.replace(' ', '_')",
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1cb19d3162b04d56bd28bf00c6e35098",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "831f92f0",
    "execution_start": 1650852473680,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 12,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 189
   },
   "source": "# term_content = dict(itertools.islice(term_content.items(), 2))\n# temp_dict = dict()\n# for k, v in term_content.items():\n#     temp_dict[k] = dict(itertools.islice(v.items(), 100))\n\n# term_content = temp_dict\n# term_content",
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "f926382c2d8e426cb642fb2ed3fd7db3",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1bb8d57e",
    "execution_start": 1650852473681,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 18,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 171
   },
   "source": "# key = \"MicroMacro: Crime City\" #'The Castles of Burgundy'\n\n# key_nothing_in_parens = re.sub(r'\\([^)]*\\)', '', key).strip()\n# key_nothing_in_parens = re.sub(r'[^\\w\\s]', '', key_nothing_in_parens).split()\n\n# key_nothing_in_parens",
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ff14bdf7a1294f0cb4280663f3a48b4e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4c42be11",
    "execution_start": 1650852473682,
    "execution_millis": 30,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 24,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 675,
    "deepnote_output_heights": [
     21.1875
    ]
   },
   "source": "def get_clean_parsed_key_from_parsed_sentence(nlp_sent):\n    \"\"\"Get just the key/thing from a constructed sentence that has been parsed by stanza\n    It has the form: '\"Puerto Rico\" is a thing.'\n    We want to return: \n    ['Puerto', 'Rico']\n\n    ' is a thing.' parses into 4 items, so we take everythign from the beginning, except the last 4\n\n\n    Parameters\n    ----------\n    nlp_sent : nlp object\n        From Stanza\n    \n            \n    Returns\n    -------\n    parsed_key : List\n        List of words to be used as the Entity to look for in the rest of the text\n    \n    \"\"\"\n    \n    parens_idx = -4        \n    \n    #print('parens:', parens_idx)\n    parsed_key = [term.text for term in nlp_sent.tokens[:parens_idx]] # length of seperator above\n    parsed_key = parsed_key[1:-1] # length of seperator above\n    #print(\"new:\", parsed_key)\n\n    return parsed_key   \n\n# test_key = nlp('\"Puerto Rico\" is a thing.').sentences[0]\n# test_key.tokens\n# get_clean_parsed_key_from_parsed_sentence(test_key)",
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "524598779fe848298a909dfc587a9e77",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "3e4cc83d",
    "execution_start": 1650852473712,
    "execution_millis": 704774,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 30,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 585
   },
   "source": "def get_idxs_of_sublist_matches(big_list, substring_to_find):\n    \"\"\"Given a big list and small one, find the indexes where the small exists in the big one\n\n   \n    Parameters\n    ----------\n    big_list : list\n        Large list of words\n\n    substring_to_find: list\n        Small list of words, the ones we're looking for in the large list\n    \n            \n    Returns\n    -------\n    indexes : List\n        List of indexes with matches as tuples, from first match to last match \n        with length of substring to find\n    \n    \"\"\"\n\n    indexes = []\n\n    # https://stackoverflow.com/questions/10459493/find-indexes-of-sequence-in-list-in-python\n    for i in range(len(big_list)):\n        if big_list[i:i+len(substring_to_find)] == substring_to_find:\n            indexes.append((i, i+len(substring_to_find)))\n\n    return indexes",
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c3efc8e6fb314a93b7df0277834635fd",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fe66fcde",
    "execution_start": 1650852473713,
    "execution_millis": 704769,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 36,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "#get_idxs_of_substring_matches(term_content[test_key], test_key)",
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ed17b75e1669479791d43e2094af0b3b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b31dc1e4",
    "execution_start": 1650852473733,
    "execution_millis": 704768,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 42,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 189
   },
   "source": "def expand_all_tups_to_list(tup_list):\n    # Takes the list of tuples and expands them to include all indexes between the start and end index\n    \n    exp_list = []\n    for tup in tup_list:\n        exp_list.extend(list(range(tup[0], tup[1])))\n    return exp_list",
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "9f862030b69248249f4c8aef1804999d",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "67ca1d15",
    "execution_start": 1650852473733,
    "execution_millis": 704780,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 48,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 225
   },
   "source": "def clean_text_for_crf_parsing(text):\n    # simple cleaning of text before NLP processing\n    \n    text = text.replace('\\n', '')\n    text = text.replace('+/u/sodogetip', '')\n    sent_remove_between_equals = re.sub(r'==[^=]*==', '', text).strip() \n\n    return sent_remove_between_equals\n",
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c79c82912a9d46a2a3e4c74c8a0c6d7a",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b376acd3",
    "execution_start": 1650852473734,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 54,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 783
   },
   "source": "def return_parsed_words_and_pos_tuples(stanza_nlp_doc, first_is_key=False):\n    \"\"\"We're starting to make labeled training data, so here we're making tuples from parsed text in a list\n    Each list is a sentence, and each tuple is the word, followed by the part of speech\n\n   \n    Parameters\n    ----------\n    stanza_nlp_doc : dict\n        Result of text parsed by stanza\n\n    first_is_key: bool\n        Whether or not the first sentence is a made up sentence make sure the key is parsed correctly\n    \n            \n    Returns\n    -------\n    parsed_key : List\n        Just the parsed keyt, if it's asked for\n    all_sents : List\n        List of tuples, (word, part-of-speech)\n    \"\"\"\n    \n    all_sents = []\n    parsed_key = ''\n    for idx, sent in enumerate(stanza_nlp_doc.sentences):\n        if (first_is_key & (idx == 0)): # first sentence(\"xxx\" is a thing.), because we concatenated the 'key' to the text\n\n            #print('first sent:', [term.text for term in sent.tokens])\n            parsed_key = get_clean_parsed_key_from_parsed_sentence(sent)\n        else:            \n            # real wiki text starts now\n            nlp_parsed_sent = [(word.text, word.upos) for token in sent.tokens for word in token.words]\n            # all_sents is a list of tuples, (the_word, part_of_speech) for each word in a sentence\n            all_sents.append(nlp_parsed_sent)\n\n            # if (idx == 1):\n            #     print('nlp_parsed:', nlp_parsed_sent)\n        # add 'word.xpos,' later\n\n    return parsed_key, all_sents ",
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "7edb2cb879c84de5b1398a5ac33fdc98",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7b0f7962",
    "execution_start": 1650852473744,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 60,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1035
   },
   "source": "def label_traing_data_for_crf(key_words, stanza_word_pos_tuples):\n    \"\"\"Given key-words and a bunch of tuples, we need to find mentions of them to tag them with a 'Y'\n    If the word isn't a key, then it gets a 'N'\n\n   \n    Parameters\n    ----------\n    key_words : list\n        Key words in a list\n\n    stanza_word_pos_tuples: list\n        list of tuples, in which we're looking for mentions of the key\n    \n            \n    Returns\n    -------\n    list_of_labeled_sentences_in_cat : List\n        List of tuples, 3 units long, (word, part-of-speech, 'Y or N')\n    \n    \n    \"\"\"\n    \n\n    \n    list_of_labeled_sentences_in_cat = []\n    \n    for idx_all, sent in enumerate(stanza_word_pos_tuples):\n        #print(sent) \n        list_of_single_sentence = []   \n        list_of_only_words_from_parsed_tuple = [st[0].upper() for st in sent]\n\n        key_words = [kw.upper() for kw in key_words]    \n\n        indexes = get_idxs_of_sublist_matches(list_of_only_words_from_parsed_tuple, key_words)\n\n        # This is to expand the indexes tuple above (3,6), into a list [3,4,5,6]\n        # to know all the indexes in which to put 'Y's, the rest being 'N's\n        for idx, tup in enumerate(sent):\n            if idx in expand_all_tups_to_list(indexes):\n                tup = tup+('Y',)\n            else:\n                tup = tup+('N',)\n\n            # (',', 'PUNCT', 'N'),\n            if (tup[0] == ',') & (tup[1] == 'PUNCT') & (tup[2] == 'Y'):\n                tup = (',', 'PUNCT', 'N')\n            list_of_single_sentence.append(tup)\n\n        list_of_labeled_sentences_in_cat.append(list_of_single_sentence)\n\n        # if idx_all == 0:\n        #     print('labeled:', list_of_single_sentence)\n\n    return list_of_labeled_sentences_in_cat",
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "43c13debf5fa4c459997294e844ec686",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e6308d10",
    "execution_start": 1650852473745,
    "execution_millis": 2,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 66,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1518.375
   },
   "source": "%%time\n# New version with single call to stanza nlp library per term\n\ndef get_parsed_labeled_sentences_from_cat_dict(term_content, mvp_flag, wiki_term):\n    \"\"\"We're putting things together here to label Wikipedia text for mentions of the the Wikipedia title\n\n   \n    Parameters\n    ----------\n    term_content : list of dicts\n        by category, all the Wikipedia text for articles in the same category\n\n    mvp_flag: bool\n        whether or not to read existing pickle files, if they exist\n\n    wiki_term: str\n        The Wikipedia title\n    \n            \n    Returns\n    -------\n    list_of_labeled_sentences_in_cat : List\n        List of tuples, 3 units long, (word, part-of-speech, 'Y or N')\n    \n    \n    \"\"\"\n\n    labeled_sentences_per_cat = dict()\n    pkl_file_name = './output_step5/dict200_labeled_cat_sentences_' + swap_spaces_for_underscore(wiki_term) + '.pickle'\n\n    if mvp_flag and file_exists(pkl_file_name):\n        print('mvp! wikipedia article already parsed and labeled.')\n        with open(pkl_file_name, 'rb') as handle:\n            labeled_sentences_per_cat = pickle.load(handle)\n    else:\n\n\n        for k_cat,v_cat_dicts in term_content.items():\n            list_of_labeled_sentences_in_cat = []\n            for wiki_key,v_wiki_text in v_cat_dicts.items():\n                \n                clean_key = re.sub(r'\\([^)]*\\)', '', wiki_key).strip()\n                #parsed_key = get_clean_parsed_key(wiki_key)\n\n                clean_text_to_parse = clean_text_for_crf_parsing(v_wiki_text)\n                seperator = '\" is a thing. ' # 4 items parsed from here: 3 words and period\n\n                #print('seperator:', seperator)\n                # added double-ticks into here , to seperate out the 'key' for stanza\n                doc = nlp('\"' + clean_key + seperator + clean_text_to_parse)\n\n                \n                key_words, parsed_word_pos_sentences = return_parsed_words_and_pos_tuples(doc, True)\n                #print(key_words)\n                \n                list_of_labeled_sentences_in_cat.append(label_traing_data_for_crf(key_words, parsed_word_pos_sentences))\n\n\n            # result is a dictionary, with wiki categories as keys, and values being a list of lists of sentences\n            # 'Category:Territorial acquisition and development games': \n            #    [[('Space', 'PROPN', 'Y'),\n            #    ('Empires', 'PROPN', 'Y'),\n            #    ('4X', 'PROPN', 'Y'),\n            #    ('is', 'AUX', 'N'),\n            #    ('a', 'DET', 'N'),\n            #    ('4X', 'ADJ', 'N'),.......\n\n            labeled_sentences_per_cat[k_cat] = list_of_labeled_sentences_in_cat\n\n        print('Dumping:', pkl_file_name)\n        with open(pkl_file_name, 'wb') as handle:\n            pickle.dump(labeled_sentences_per_cat, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n        # This is Conditional Random Field training data for each category\n    return labeled_sentences_per_cat\n\n# dict_labeled_cat_sentences = get_parsed_labeled_sentences_from_cat_dict(term_content, mvp_flag, wiki_term)\n",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 3 Âµs, sys: 0 ns, total: 3 Âµs\nWall time: 6.68 Âµs\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "4a6380cbb31242a4bc39ec3c5491db30",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d904828b",
    "execution_start": 1650852473762,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 72,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "# https://notebook.community/thammegowda/notes/nlproc/hw3/notes/pycrf_tutorial\n",
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "cea2ba7f14f8437daa38ff68fdd0b5f1",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650852473763,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 78,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "9b441a1d5eb64ff18151df930f92f79d",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "65d96bd8",
    "execution_start": 1650852473763,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 84,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "#list_combined_cats_labeled_sentences[0]",
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c09f8b3ea9984a6aabe8d02c06962838",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650852473794,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 90,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ff3edac27b604ec28122686fd21e8c4b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "aa69ea23",
    "execution_start": 1650852473795,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 204,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 729
   },
   "source": "def create_labeled_training_data_from_wikipedia(wiki_term, mvp_flag):\n\n    # Combine all the previously labeled data into a single category, because some categpries don't\n    # have enough articles to train on\n\n    print(\"Wikipedia term:\", wiki_term)\n    \n    # Load Wikipedia articles created in Step 1\n    with open('./output_step1/wiki_200_cat_content_' + swap_spaces_for_underscore(wiki_term) + '.pickle', 'rb') as handle:\n        term_content = pickle.load(handle)\n\n    # See how many wikipedia article we're workign with\n    print(\"See wikipedia categories and number of article saved as training data to be labeled.\")\n    cume_number_of_items = 0\n    for k,v in term_content.items():\n        cume_number_of_items = cume_number_of_items + len(v)\n        print(k, cume_number_of_items, len(v))\n\n    dict_labeled_cat_sentences = get_parsed_labeled_sentences_from_cat_dict(term_content, mvp_flag, wiki_term)\n\n    list_combined_cats_labeled_sentences = []\n\n    for k_category,v_cat_items in dict_labeled_cat_sentences.items():\n        for sent in v_cat_items:\n            list_combined_cats_labeled_sentences.extend(sent)\n        \n    print(len(list_combined_cats_labeled_sentences), 'sentences from wikipedia available to train on')\n\n    labels = Counter()\n    for sentence in list_combined_cats_labeled_sentences:\n        for word_pos_tup in sentence:\n            labels.update(word_pos_tup[2])\n    print(labels, 'numbers of each label from wikipedia')\n\n    return list_combined_cats_labeled_sentences\n\n",
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0e0f6495746b496db2e82f1188e7a60a",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9648a8ed",
    "execution_start": 1650852473796,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 96,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 369
   },
   "source": "def train_test_split_labeled_sentences(docs):\n    # 80/20% split for train/test data\n    \n    random.seed(42)\n    random.shuffle(docs)\n\n    tot_sentences = len(docs)\n\n\n    len_train_sents = math.floor((tot_sentences * .8))\n    len_test_sents = tot_sentences - len_train_sents\n    assert len_train_sents + len_test_sents == tot_sentences, \"boo\"\n\n    train_sents = docs[:len_train_sents]\n    test_sents = docs[len_train_sents:]\n\n    return train_sents, test_sents",
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3eb8fc3f6f6a4aa69612843820db931c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "341562f",
    "execution_start": 1650852473829,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 102,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1035
   },
   "source": "# from the documentation: https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html\n# we're creating features from each tuple\n\ndef word2features(sent, i):\n    word = sent[i][0]\n    postag = sent[i][1]\n    features = [\n        'bias',\n        'word.lower=' + word.lower(),\n        'word[-3:]=' + word[-3:],\n        'word[-2:]=' + word[-2:],\n        'word.isupper=%s' % word.isupper(),\n        'word.istitle=%s' % word.istitle(),\n        'word.isdigit=%s' % word.isdigit(),\n        'postag=' + postag,\n        'postag[:2]=' + postag[:2],\n    ]\n    if i > 0:\n        word1 = sent[i-1][0]\n        postag1 = sent[i-1][1]\n        features.extend([\n            '-1:word.lower=' + word1.lower(),\n            '-1:word.istitle=%s' % word1.istitle(),\n            '-1:word.isupper=%s' % word1.isupper(),\n            '-1:postag=' + postag1,\n            '-1:postag[:2]=' + postag1[:2],\n        ])\n    else:\n        features.append('BOS')\n        \n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        postag1 = sent[i+1][1]\n        features.extend([\n            '+1:word.lower=' + word1.lower(),\n            '+1:word.istitle=%s' % word1.istitle(),\n            '+1:word.isupper=%s' % word1.isupper(),\n            '+1:postag=' + postag1,\n            '+1:postag[:2]=' + postag1[:2],\n        ])\n    else:\n        features.append('EOS')\n                \n    return features\n\n\ndef sent2features(sent):\n    return [word2features(sent, i) for i in range(len(sent))]\n\ndef sent2labels(sent):\n    return [label for token, postag, label in sent]\n\ndef sent2tokens(sent):\n    return [token for token, postag, label in sent]",
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "da8780cb7ac84b02bf26c39c4344d023",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "41a0977",
    "execution_start": 1650852473829,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 108,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 225
   },
   "source": "def get_x_y_features_labels(train_sents, test_sents):\n    \n    X_train = [sent2features(s) for s in train_sents]\n    y_train = [sent2labels(s) for s in train_sents]\n\n    X_test = [sent2features(s) for s in test_sents]\n    y_test = [sent2labels(s) for s in test_sents]\n\n    return X_train, y_train, X_test, y_test",
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "22279c9952654052aa9cfcb8739fdd99",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650852473830,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 216,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "8ee3636776f84930853655087c686079",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "696a34d4",
    "execution_start": 1650852473846,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 114,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 729
   },
   "source": "def return_trained_tagger(X_train, y_train, wiki_term, mvp_flag):\n    #This returns the trained model\n    \n    crf_model_file_name = './output_step5/tagger_200_' + swap_spaces_for_underscore(wiki_term) + '.crfsuite'\n    #crf_model_file_name = u''.join(('./output_step5/tagger_200_' , swap_spaces_for_underscore(wiki_term) , '.crfsuite')).encode('utf-8').strip()\n\n    tagger = pycrfsuite.Tagger()\n\n    if not (mvp_flag and file_exists(crf_model_file_name)):\n        trainer = pycrfsuite.Trainer(verbose=False)\n        max_sentences = 10000\n        print('Trainer created, will use max of', max_sentences, 'of', len(X_train), 'items')\n        for xseq, yseq in zip(X_train[:max_sentences], y_train[:max_sentences]):\n            trainer.append(xseq, yseq)\n        print('Training data joined')\n        trainer.set_params({\n        'c1': 1.0,   # coefficient for L1 penalty\n        'c2': 1e-3,  # coefficient for L2 penalty\n        'max_iterations': 200,  # stop earlier\n\n        # include transitions that are possible, but not observed\n        'feature.possible_transitions': True\n        })\n\n        print('training with params:', trainer.params())\n\n        # Train model\n        trainer.train(crf_model_file_name) # 50- 10secs, 200-38secs\n\n        print('trainer_features:', trainer.logparser.last_iteration)\n    else:\n        print('mvp! using existing tagger file')\n    \n    tagger.open(crf_model_file_name)\n\n    return tagger\n",
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "49db52fb615a4b2ca8a5742897904896",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9ae3a531",
    "execution_start": 1650852473847,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 120,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 243
   },
   "source": "# a test to see that it's working\n\ndef print_example_sentence(test_sents, tagger):\n    print(\"Sample test sentence and predicted/actual tags\")\n    example_sent = test_sents[0]\n    print(' '.join(sent2tokens(example_sent)), end='\\n\\n')\n\n    print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n    print(\"Actual:  \", ' '.join(sent2labels(example_sent)))\n",
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d2974890422e48c4ba68eebe53370c17",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2d434bda",
    "execution_start": 1650852473848,
    "execution_millis": 28,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 126,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 477
   },
   "source": "def bio_classification_report(y_true, y_pred):\n    \"\"\"\n    Classification report for a list of BIO-encoded sequences.\n    It computes token-level metrics and discards \"O\" labels.\n    \n    Note that it requires scikit-learn 0.15+ (or a version from github master)\n    to calculate averages properly!\n    \"\"\"\n    lb = LabelBinarizer()\n    \n    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n        \n    tagset = set(lb.classes_) - {'O'}\n    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n    \n    return classification_report(\n        y_true_combined,\n        y_pred_combined,\n        labels = [class_indices[cls] for cls in tagset],\n        target_names = tagset,\n    )",
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e7f5d99b29ac42ee96cf2c90a7e8b2bc",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "308c29f5",
    "execution_start": 1650852473876,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 132,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 603
   },
   "source": "# We want the classification report in a dataframe to print it easier\n\ndef classification_report_df(report):\n    report_data = []\n    lines = report.split('\\n')\n    del lines[-5]\n    del lines[-1]\n    del lines[1]\n    for line in lines[1:3]:\n        row = OrderedDict()\n        row_data = line.split()\n        #print(row_data)\n        row_data = list(filter(None, row_data))\n        row['class'] = row_data[0]\n        row['precision'] = float(row_data[1])\n        row['recall'] = float(row_data[2])\n        row['f1_score'] = float(row_data[3])\n        row['support'] = int(row_data[4])\n        report_data.append(row)\n    df = pd.DataFrame.from_dict(report_data)\n    df.set_index('class', inplace=True)\n    return df\n\ndef get_model_accuracy_df(tagger, X_test, y_test):\n    y_pred = [tagger.tag(xseq) for xseq in X_test]\n\n    report = bio_classification_report(y_test, y_pred)\n    return classification_report_df(report).reset_index()\n\n",
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "16f589a45b3d426d9d55065a0a713300",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1f8102cd",
    "execution_start": 1650852473877,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 138,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 369
   },
   "source": "\ndef get_clean_unseen_text_to_predict(wiki_term):\n\n    pkl_file_name = './output_step4/posts_' + swap_spaces_for_underscore(wiki_term) + '.pickle'\n    with open(pkl_file_name, 'rb') as handle:\n        related_posts = pickle.load(handle)\n\n    unseen_text = [clean_text_for_crf_parsing(i) for i in related_posts[:]]\n\n    print(\"Number of Sentences in Reddit text:\", len(unseen_text))\n    print(\"Some example sentences:\")\n    for i in unseen_text[:10]:\n        print(i) \n\n    return unseen_text\n\n",
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "da64c912112d47eabfa62a8c9e383553",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5b9d29bf",
    "execution_start": 1650852473915,
    "execution_millis": 704755,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 144,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 225
   },
   "source": "# related_posts = ['Good shonen anime similar to Demon Slayer or Hunter X Hunter?',\n#  'Looking for shows like Ratched, Undercover, Collateral, Brand New Cherry Flavor, Zero Zero Zero, Mr. Robot, The Haunting Of The Hill House, Bordertown, Better Call Saul, or Bob?',\n#  'Korean wave be like :. well the drama is fun but its nowhere near to best writing. Seem like you are just new with kdramas. Because best writing being associated more with kdramas like signal, kingdom, mr sunshine, misaeng,etc.',\n#  \"[REQUEST] tv shows similar to Mcmafia. I watched it recently on AMC+.  Avoided it for so long because of the name, surprisingly ended up really enjoying it.  I loved the international feeling of it.  Can't wait for season 2.\",\n#  'Korean wave be like :. WcðŸ‘Œ',\n#  'Korean wave be like :. Even squid game is much better drama than CLOY. I watched both dramas. Both are fun of course. Personally, i found squid game much better drama than CLOY too. I am pretty sure you not yet to watch dramas that i suggested.',\n#  \"My flat mate has taken the security of the milk in next level. He cut one milk bottle into half and put on top of the new one and lock it so nobody can try to steal milk from him.. That's nothing a little steak knife can't fix.\"] + \\\n#  related_posts\n",
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c9e83ca5b9154986a4d1f4b3aef3368b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7ad2d8db",
    "execution_start": 1650852473916,
    "execution_millis": 21,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 150,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1593
   },
   "source": "\n\ndef get_new_crf_phrases_counts_from_clean_unseen_posts(unseen_text, tagger, mvp_flag, wiki_term):\n    \n    crf_tagged_pkl_file = './output_step5/counter_crf_tagged_phrases_' + swap_spaces_for_underscore(wiki_term) + '.pickle'\n    crf_exact_pkl_file = './output_step5/counter_crf_exact_phrases_' + swap_spaces_for_underscore(wiki_term) + '.pickle'\n    \n    #crf_tagged_pkl_file = u''.join(('./output_step5/counter_crf_tagged_phrases_' , swap_spaces_for_underscore(wiki_term) , '.pickle')).encode('utf-8').strip()\n    \n    \n    \n    crf_tagged_phrases_in_text = Counter()\n    crf_exact_phrases_in_text = Counter()\n\n    if mvp_flag and file_exists(crf_tagged_pkl_file) and file_exists(crf_exact_pkl_file):\n        print('mvp!')\n        with open(crf_tagged_pkl_file, 'rb') as handle:\n            crf_tagged_phrases_in_text = pickle.load(handle)\n        with open(crf_exact_pkl_file, 'rb') as handle:\n            crf_exact_phrases_in_text = pickle.load(handle)\n    else:            \n\n        for sentence in unseen_text[:]:\n            doc = nlp(sentence)\n            _, unseen_parsed_word_pos_sentences = return_parsed_words_and_pos_tuples(doc, False)\n            #print('uns:', unseen_parsed_word_pos_sentences)\n            X_unseen = [sent2features(s) for s in unseen_parsed_word_pos_sentences]\n            y_unseen_pred = [tagger.tag(xseq) for xseq in X_unseen]\n\n            #print('yun:', y_unseen_pred)\n\n            #new_big_list_sentences = []\n            for idx_big, parsed_sent in enumerate(unseen_parsed_word_pos_sentences):\n                new_small_sentence = []\n                \n                # new_phrases_found = []\n                # idx_seen = []\n                #'n,n,Y,Y,n,n,n,n,Y,Y,Y,n,n'\n                for idx_small, tup in enumerate(parsed_sent[:]):\n                    #unseen_pred = y_unseen_pred[idx_big][idx_small]\n                    \n                    new_small_sentence.append((tup[0], y_unseen_pred[idx_big][idx_small] ))\n\n                    # https://stackoverflow.com/questions/71746563/finding-consecutive-groupings-of-items-in-list-of-lists-in-python\n                found_phrases_in_sent = [ ' '.join(i[0] for i in b)for a, b in groupby(new_small_sentence, key=lambda x:x[1]) if a=='Y']\n                \n                if len(found_phrases_in_sent)> 0:\n                    #print('Found:', found_phrases_in_sent)\n                    #print(\"in:\", parsed_sent)\n\n                    for found_phrase in found_phrases_in_sent:\n\n                        # for each item in found_phrases_in_sent, only once\n                        # if it's in the counter, we've seen it\n                        # for each found entity, we're going to spin through all the posts\n                        # and look for exact matches\n                        if found_phrase.upper() not in crf_tagged_phrases_in_text:\n                            for post in unseen_text: # checking for exact matches of CRF terms in posts\n                                \n                                remove_spaces = [' .', ' ?', ' !', ' :', \" N'T\", ' -', '- ', \" 'S\", \" 'D\", \" 'VE\", \" 'RE\", \" 'M\", \" 'LL\"]\n                                found_phrase_no_spaces = str(found_phrase)\n                                for rem_sp in remove_spaces:\n                                    found_phrase_no_spaces = found_phrase_no_spaces.replace(rem_sp, rem_sp.replace(' ', ''))\n                                                                \n                                crf_exact_phrases_in_text.update({found_phrase.upper(): post.upper().count(found_phrase_no_spaces.upper()) })\n\n\n                    crf_tagged_phrases_in_text.update([i.upper() for i in found_phrases_in_sent])\n\n                    #new_big_list_sentences.append(new_small_sentence)\n                    #print('prs:', parsed_sent)\n\n            #print([tup for sent in new_big_list_sentences for tup in sent])\n            #print(new_big_list_sentences)\n\n        print('Dumping:', crf_tagged_pkl_file, crf_exact_pkl_file)\n        with open(crf_tagged_pkl_file, 'wb') as handle:\n            pickle.dump(crf_tagged_phrases_in_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n        with open(crf_exact_pkl_file, 'wb') as handle:\n            pickle.dump(crf_exact_phrases_in_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    return crf_tagged_phrases_in_text, crf_exact_phrases_in_text\n\n",
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ffaa6905ebe145f09216ebe84a5f6301",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f101446b",
    "execution_start": 1650852473937,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 156,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "#new_phrases_found_from_crf.most_common()",
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c07bd672704f4b7bb984ca5e191c85ed",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cb6a0593",
    "execution_start": 1650852473938,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 162,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 117
   },
   "source": "# try to evaluate true counts from unseen data, based on exact match\n# should have identical keys as above\n#exact_crf_phrase_in_text.most_common()",
   "execution_count": 61,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "243ab1f3a44e4d158f964b6d6c4c9b96",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "8b503e93",
    "execution_start": 1650852473939,
    "execution_millis": 42,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 168,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 837
   },
   "source": "\ndef check_unseen_against_wiki_whitelist(wiki_term, unseen_text):\n\n    pkl_file_name = './output_step1/white_list_200_' + swap_spaces_for_underscore(wiki_term) + '.pickle'\n    with open(pkl_file_name, 'rb') as handle:\n        wiki_white_list = pickle.load(handle)\n\n    exact_wiki_phrase_in_text = Counter()\n    all_wiki_articles = set()\n\n    for key_cat, val_list_articles in wiki_white_list.items():\n            all_wiki_articles.update(val_list_articles)\n\n    for sentence in unseen_text:\n        sentence = sentence.upper()  \n\n        for article in all_wiki_articles:\n            if article.upper() in sentence:\n                exact_wiki_phrase_in_text.update([article.upper()])\n    \n    return exact_wiki_phrase_in_text\n    \ndef return_final_df(wiki_term, exact_wiki_phrase_in_text, new_phrases_found_from_crf, exact_crf_phrase_in_text):    \n    df_wiki_white = pd.DataFrame.from_dict(exact_wiki_phrase_in_text.most_common())\n    df_wiki_white.columns = ['Word', 'Exact_Whitelist']\n    df_wiki_white = df_wiki_white.set_index('Word')\n\n    df_crf_results = pd.DataFrame({'CRF_Model_Found': new_phrases_found_from_crf, 'Exact_CRF': exact_crf_phrase_in_text})\n    df_crf_results = df_crf_results.join(df_wiki_white , how='outer')\n\n    df_crf_results['CRF_Recall'] = df_crf_results['CRF_Model_Found']/df_crf_results['Exact_CRF']\n\n    df_crf_results = df_crf_results.reset_index()\n    df_crf_results = df_crf_results.rename(columns={'index': 'Entity'})\n\n    df_crf_results = df_crf_results.sort_values(by=['CRF_Model_Found', 'Exact_CRF', 'CRF_Recall', 'Entity'], ascending=[False, False, False, True], na_position='last')\n    df_crf_results.to_csv('./output_step5/crf_results_' + swap_spaces_for_underscore(wiki_term) + '.csv', encoding='utf-8', index=False)\n    df_crf_results.to_csv('./streamlit/example_output/crf_results_' + swap_spaces_for_underscore(wiki_term) + '.csv', encoding='utf-8', index=False)\n\n    return df_crf_results\n\n\n",
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "59a78861ab7a4542856c5c33e723b83e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "80fca27f",
    "execution_start": 1650852473981,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 192,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 225
   },
   "source": "# see if any are not in white list\n# check for upper and letter upper/lower case\n# eval accuracy at each step\n# github readme\n# Context of wikipedia is different from reddit for CRF model\n# try getting rid of commas and 'Y'\n# try exact match against whitelist\n# try Spacy near\n# intro and motivation, novel item, from scikit",
   "execution_count": 63,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d2d86589bedf4c189d9433c14c3c73d6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "bcf5ebb1",
    "execution_start": 1650852473982,
    "execution_millis": 17545,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 222,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1887,
    "deepnote_output_heights": [
     136.3125,
     607
    ]
   },
   "source": "%%time\n\n\ndef calculate_final_results_for_wiki_term(wiki_term, mvp_flag):\n    docs = create_labeled_training_data_from_wikipedia(wiki_term, mvp_flag)\n\n    train_sents, test_sents = train_test_split_labeled_sentences(docs)\n    X_train, y_train, X_test, y_test = get_x_y_features_labels(train_sents, test_sents)\n    print('Ready to train.')\n    tagger = return_trained_tagger(X_train, y_train, wiki_term, mvp_flag)\n\n    print_example_sentence(test_sents, tagger)\n\n    print(swap_spaces_for_underscore(wiki_term))\n    print(get_model_accuracy_df(tagger, X_test, y_test ))\n\n    unseen_text = get_clean_unseen_text_to_predict(wiki_term)\n\n    new_phrases_found_from_crf, exact_crf_phrase_in_text = get_new_crf_phrases_counts_from_clean_unseen_posts(unseen_text, tagger, mvp_flag, wiki_term)\n\n    exact_wiki_phrase_in_text = check_unseen_against_wiki_whitelist(wiki_term, unseen_text)\n    df_final = return_final_df(wiki_term, exact_wiki_phrase_in_text, new_phrases_found_from_crf, exact_crf_phrase_in_text)\n    print(wiki_term, 'finished')\n    return df_final\n\ncalculate_final_results_for_wiki_term('Squid Game', True)\n#calculate_final_results_for_wiki_term('PokÃ©mon', False)\n",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "text": "Wikipedia term: Squid Game\nSee wikipedia categories and number of article saved as training data to be labeled.\nCategory:2021 South Korean television series debuts 114 114\nCategory:South Korean thriller television series 226 112\nCategory:2018 South Korean television series debuts 379 153\nCategory:2019 South Korean television series debuts 523 144\nCategory:South Korean horror fiction television series 534 11\nCategory:South Korean action television series 627 93\nCategory:Television shows set in Seoul 653 26\nCategory:2022 South Korean television series debuts 729 76\nCategory:2020 South Korean television series debuts 852 123\nmvp! wikipedia article already parsed and labeled.\n25972 sentences from wikipedia available to train on\nCounter({'N': 626828, 'Y': 7090}) numbers of each label from wikipedia\nReady to train.\nmvp! using existing tagger file\nSample test sentence and predicted/actual tags\nEpisode 12 Eun Jae is about to tell Yi Joon that Jin Sol is his son .\n\nPredicted: N N N N N N N N N N N N N N N N N\nActual:   N N N N N N N N N N N N N N N N N\nSquid_Game\n  class  precision  recall  f1_score  support\n0     N        1.0    1.00      1.00   124866\n1     Y        0.8    0.61      0.69     1436\nNumber of Sentences in Reddit text: 10000\nSome example sentences:\nTwenty-Five, Twenty-One [Episode 12].* **Drama**: [Twenty-Five, Twenty-One](https://asianwiki.com/Twenty-Five,_Twenty-One)   * Korean Title: ìŠ¤ë¬¼ë‹¤ì„¯ ìŠ¤ë¬¼í•˜ë‚˜* **Network**: tvN* **Premiere Date**: February 12, 2022* **Airing Schedule**: Saturday &amp; Sunday, 21:10 KST* **Episodes**: 16* **Director**: [Jung Ji Hyun](https://asianwiki.com/Jung_Ji-Hyun_(director)) (Mr. Sunshine, The King: Eternal Monarch, Search: WWW)* **Writer**: [Kwon Do Eun](https://mydramalist.com/people/21763-kwon-eun-sol) (Search: WWW)* **Cast**: [Kim Tae Ri](https://asianwiki.com/Kim_Tae-Ri) as Na Hee Do, [Nam Joo Hyuk](https://asianwiki.com/Nam_Joo-Hyuk) as Baek Yi Jin, [Bona](https://asianwiki.com/Bona_(WJSN)) as Go Yoo Rim, [Choi Hyun Wook](https://asianwiki.com/Choi_Hyun-Wook_(2002)) as Moon Ji Woong, [Lee Joo Myoung](https://asianwiki.com/Lee_Joo-Myoung) as Ji Seung Wan* **Streaming Source**: [Netflix](https://www.netflix.com/title/81517168)* **Plot** **Synopsis**: In a time when dreams seem out of reach, a teen fencer pursues big ambitions and meets a hardworking young man who seeks to rebuild his life. (Source: Netflix)* **Previous Discussions**: [\\[Episodes 1 &amp; 2\\]](https://www.reddit.com/r/KDRAMA/comments/sql71t/twentyfive_twentyone_episodes_1_2/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 3 &amp; 4\\]](https://www.reddit.com/r/KDRAMA/comments/sw5kqi/twentyfive_twentyone_episodes_3_4/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 5 &amp; 6\\]](https://www.reddit.com/r/KDRAMA/comments/t0yfys/twentyfive_twentyone_episodes_5_6/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 7\\]](https://www.reddit.com/r/KDRAMA/comments/t74moh/twentyfive_twentyone_episode_7/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 8\\]](https://www.reddit.com/r/KDRAMA/comments/t7rpbf/twentyfive_twentyone_episode_8/?utm_source=share&amp;utm_medium=web2x&amp;context=3)[\\[Episode 9\\]](https://www.reddit.com/r/KDRAMA/comments/tcckit/twentyfive_twentyone_episode_9/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 10\\]](https://www.reddit.com/r/KDRAMA/comments/td1cuy/twentyfive_twentyone_episode_10/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 11\\]](https://www.reddit.com/r/KDRAMA/comments/thopwa/twentyfive_twentyone_episode_11/?utm_source=share&amp;utm_medium=web2x&amp;context=3)* **Conduct Reminder**:&gt;We encourage our users to read the following before participating in any discussions on [/r/KDRAMA](https://www.reddit.com/r/KDRAMA): (1) [Reddiquette](https://www.reddithelp.com/hc/en-us/articles/205926439), (2) our [Conduct Rules](https://www.reddit.com/r/KDRAMA/wiki/rules#wiki_1._conduct) (3) our [Policies](https://www.reddit.com/r/KDRAMA/wiki/policies), and (4) the [When Discussions Get Personal Post](https://www.reddit.com/r/KDRAMA/comments/jqrjx6/meta_when_discussions_get_personal/).  Any users who are displaying negative conduct (including but not limited to bullying, harassment, or personal attacks) will be given a warning, repeated behaviour will lead to increasing exclusions from our community. Any extreme cases of misconduct (such as racism or hate speech) will result in an immediate permanent ban from our community and a report to Reddit admin.  Additionally, mentions of down-voting, unpopular opinions, and the use of profanity may see your comments locked or removed without notice.* **Spoiler Tag Reminder**:&gt;Be mindful of others who may not have yet seen this drama, and use spoiler tags when discussing key plot developments or other important information. You can create a spoiler tag by writing &gt; ! this! &lt; without the spaces in between to get this &gt;!spoiler!&lt;. For more information about when and how to use spoiler tags see our [Spoiler Tag Wiki](https://www.reddit.com/r/KDRAMA/wiki/spoilertags).\nTwenty-Five, Twenty-One [Episode 13].* **Drama**: [Twenty-Five, Twenty-One](https://asianwiki.com/Twenty-Five,_Twenty-One)   * Korean Title: ìŠ¤ë¬¼ë‹¤ì„¯ ìŠ¤ë¬¼í•˜ë‚˜* **Network**: tvN* **Premiere Date**: February 12, 2022* **Airing Schedule**: Saturday &amp; Sunday, 21:10 KST* **Episodes**: 16* **Director**: [Jung Ji Hyun](https://asianwiki.com/Jung_Ji-Hyun_(director)) (Mr. Sunshine, The King: Eternal Monarch, Search: WWW)* **Writer**: [Kwon Do Eun](https://mydramalist.com/people/21763-kwon-eun-sol) (Search: WWW)* **Cast**: [Kim Tae Ri](https://asianwiki.com/Kim_Tae-Ri) as Na Hee Do, [Nam Joo Hyuk](https://asianwiki.com/Nam_Joo-Hyuk) as Baek Yi Jin, [Bona](https://asianwiki.com/Bona_(WJSN)) as Go Yoo Rim, [Choi Hyun Wook](https://asianwiki.com/Choi_Hyun-Wook_(2002)) as Moon Ji Woong, [Lee Joo Myoung](https://asianwiki.com/Lee_Joo-Myoung) as Ji Seung Wan* **Streaming Source**: [Netflix](https://www.netflix.com/title/81517168)* **Plot** **Synopsis**: In a time when dreams seem out of reach, a teen fencer pursues big ambitions and meets a hardworking young man who seeks to rebuild his life. (Source: Netflix)* **Previous Discussions**: [\\[Episodes 1 &amp; 2\\]](https://www.reddit.com/r/KDRAMA/comments/sql71t/twentyfive_twentyone_episodes_1_2/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 3 &amp; 4\\]](https://www.reddit.com/r/KDRAMA/comments/sw5kqi/twentyfive_twentyone_episodes_3_4/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 5 &amp; 6\\]](https://www.reddit.com/r/KDRAMA/comments/t0yfys/twentyfive_twentyone_episodes_5_6/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 7\\]](https://www.reddit.com/r/KDRAMA/comments/t74moh/twentyfive_twentyone_episode_7/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 8\\]](https://www.reddit.com/r/KDRAMA/comments/t7rpbf/twentyfive_twentyone_episode_8/?utm_source=share&amp;utm_medium=web2x&amp;context=3)[\\[Episode 9\\]](https://www.reddit.com/r/KDRAMA/comments/tcckit/twentyfive_twentyone_episode_9/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 10\\]](https://www.reddit.com/r/KDRAMA/comments/td1cuy/twentyfive_twentyone_episode_10/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 11\\]](https://www.reddit.com/r/KDRAMA/comments/thopwa/twentyfive_twentyone_episode_11/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 12\\]](https://www.reddit.com/r/KDRAMA/comments/tihdh7/twentyfive_twentyone_episode_12/?utm_source=share&amp;utm_medium=web2x&amp;context=3)* **Conduct Reminder**:&gt;We encourage our users to read the following before participating in any discussions on [/r/KDRAMA](https://www.reddit.com/r/KDRAMA): (1) [Reddiquette](https://www.reddithelp.com/hc/en-us/articles/205926439), (2) our [Conduct Rules](https://www.reddit.com/r/KDRAMA/wiki/rules#wiki_1._conduct) (3) our [Policies](https://www.reddit.com/r/KDRAMA/wiki/policies), and (4) the [When Discussions Get Personal Post](https://www.reddit.com/r/KDRAMA/comments/jqrjx6/meta_when_discussions_get_personal/).  Any users who are displaying negative conduct (including but not limited to bullying, harassment, or personal attacks) will be given a warning, repeated behaviour will lead to increasing exclusions from our community. Any extreme cases of misconduct (such as racism or hate speech) will result in an immediate permanent ban from our community and a report to Reddit admin.  Additionally, mentions of down-voting, unpopular opinions, and the use of profanity may see your comments locked or removed without notice.* **Spoiler Tag Reminder**:&gt;Be mindful of others who may not have yet seen this drama, and use spoiler tags when discussing key plot developments or other important information. You can create a spoiler tag by writing &gt; ! this! &lt; without the spaces in between to get this &gt;!spoiler!&lt;. For more information about when and how to use spoiler tags see our [Spoiler Tag Wiki](https://www.reddit.com/r/KDRAMA/wiki/spoilertags).\nTwenty-Five, Twenty-One [Episode 14].* **Drama**: [Twenty-Five, Twenty-One](https://asianwiki.com/Twenty-Five,_Twenty-One)   * Korean Title: ìŠ¤ë¬¼ë‹¤ì„¯ ìŠ¤ë¬¼í•˜ë‚˜* **Network**: tvN* **Premiere Date**: February 12, 2022* **Airing Schedule**: Saturday &amp; Sunday, 21:10 KST* **Episodes**: 16* **Director**: [Jung Ji Hyun](https://asianwiki.com/Jung_Ji-Hyun_(director)) (Mr. Sunshine, The King: Eternal Monarch, Search: WWW)* **Writer**: [Kwon Do Eun](https://mydramalist.com/people/21763-kwon-eun-sol) (Search: WWW)* **Cast**: [Kim Tae Ri](https://asianwiki.com/Kim_Tae-Ri) as Na Hee Do, [Nam Joo Hyuk](https://asianwiki.com/Nam_Joo-Hyuk) as Baek Yi Jin, [Bona](https://asianwiki.com/Bona_(WJSN)) as Go Yoo Rim, [Choi Hyun Wook](https://asianwiki.com/Choi_Hyun-Wook_(2002)) as Moon Ji Woong, [Lee Joo Myoung](https://asianwiki.com/Lee_Joo-Myoung) as Ji Seung Wan* **Streaming Source**: [Netflix](https://www.netflix.com/title/81517168)* **Plot** **Synopsis**: In a time when dreams seem out of reach, a teen fencer pursues big ambitions and meets a hardworking young man who seeks to rebuild his life. (Source: Netflix)* **Previous Discussions**: [\\[Episodes 1 &amp; 2\\]](https://www.reddit.com/r/KDRAMA/comments/sql71t/twentyfive_twentyone_episodes_1_2/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 3 &amp; 4\\]](https://www.reddit.com/r/KDRAMA/comments/sw5kqi/twentyfive_twentyone_episodes_3_4/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 5 &amp; 6\\]](https://www.reddit.com/r/KDRAMA/comments/t0yfys/twentyfive_twentyone_episodes_5_6/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 7\\]](https://www.reddit.com/r/KDRAMA/comments/t74moh/twentyfive_twentyone_episode_7/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 8\\]](https://www.reddit.com/r/KDRAMA/comments/t7rpbf/twentyfive_twentyone_episode_8/?utm_source=share&amp;utm_medium=web2x&amp;context=3)[\\[Episode 9\\]](https://www.reddit.com/r/KDRAMA/comments/tcckit/twentyfive_twentyone_episode_9/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 10\\]](https://www.reddit.com/r/KDRAMA/comments/td1cuy/twentyfive_twentyone_episode_10/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 11\\]](https://www.reddit.com/r/KDRAMA/comments/thopwa/twentyfive_twentyone_episode_11/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 12\\]](https://www.reddit.com/r/KDRAMA/comments/tihdh7/twentyfive_twentyone_episode_12/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 13\\]](https://www.reddit.com/r/KDRAMA/comments/tojaik/twentyfive_twentyone_episode_13/?utm_source=share&amp;utm_medium=web2x&amp;context=3)* **Conduct Reminder**:&gt;We encourage our users to read the following before participating in any discussions on [/r/KDRAMA](https://www.reddit.com/r/KDRAMA): (1) [Reddiquette](https://www.reddithelp.com/hc/en-us/articles/205926439), (2) our [Conduct Rules](https://www.reddit.com/r/KDRAMA/wiki/rules#wiki_1._conduct) (3) our [Policies](https://www.reddit.com/r/KDRAMA/wiki/policies), and (4) the [When Discussions Get Personal Post](https://www.reddit.com/r/KDRAMA/comments/jqrjx6/meta_when_discussions_get_personal/).  Any users who are displaying negative conduct (including but not limited to bullying, harassment, or personal attacks) will be given a warning, repeated behaviour will lead to increasing exclusions from our community. Any extreme cases of misconduct (such as racism or hate speech) will result in an immediate permanent ban from our community and a report to Reddit admin.  Additionally, mentions of down-voting, unpopular opinions, and the use of profanity may see your comments locked or removed without notice.* **Spoiler Tag Reminder**:&gt;Be mindful of others who may not have yet seen this drama, and use spoiler tags when discussing key plot developments or other important information. You can create a spoiler tag by writing &gt; ! this! &lt; without the spaces in between to get this &gt;!spoiler!&lt;. For more information about when and how to use spoiler tags see our [Spoiler Tag Wiki](https://www.reddit.com/r/KDRAMA/wiki/spoilertags).\nTwenty-Five, Twenty-One [Episode 16].* **Drama**: [Twenty-Five, Twenty-One](https://asianwiki.com/Twenty-Five,_Twenty-One)   * Korean Title: ìŠ¤ë¬¼ë‹¤ì„¯ ìŠ¤ë¬¼í•˜ë‚˜* **Network**: tvN* **Premiere Date**: February 12, 2022* **Airing Schedule**: Saturday &amp; Sunday, 21:10 KST* **Episodes**: 16* **Director**: [Jung Ji Hyun](https://asianwiki.com/Jung_Ji-Hyun_(director)) (Mr. Sunshine, The King: Eternal Monarch, Search: WWW)* **Writer**: [Kwon Do Eun](https://mydramalist.com/people/21763-kwon-eun-sol) (Search: WWW)* **Cast**: [Kim Tae Ri](https://asianwiki.com/Kim_Tae-Ri) as Na Hee Do, [Nam Joo Hyuk](https://asianwiki.com/Nam_Joo-Hyuk) as Baek Yi Jin, [Bona](https://asianwiki.com/Bona_(WJSN)) as Go Yoo Rim, [Choi Hyun Wook](https://asianwiki.com/Choi_Hyun-Wook_(2002)) as Moon Ji Woong, [Lee Joo Myoung](https://asianwiki.com/Lee_Joo-Myoung) as Ji Seung Wan* **Streaming Source**: [Netflix](https://www.netflix.com/title/81517168)* **Plot** **Synopsis**: In a time when dreams seem out of reach, a teen fencer pursues big ambitions and meets a hardworking young man who seeks to rebuild his life. (Source: Netflix)* **Previous Discussions**: [\\[Episodes 1 &amp; 2\\]](https://www.reddit.com/r/KDRAMA/comments/sql71t/twentyfive_twentyone_episodes_1_2/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 3 &amp; 4\\]](https://www.reddit.com/r/KDRAMA/comments/sw5kqi/twentyfive_twentyone_episodes_3_4/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 5 &amp; 6\\]](https://www.reddit.com/r/KDRAMA/comments/t0yfys/twentyfive_twentyone_episodes_5_6/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 7\\]](https://www.reddit.com/r/KDRAMA/comments/t74moh/twentyfive_twentyone_episode_7/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 8\\]](https://www.reddit.com/r/KDRAMA/comments/t7rpbf/twentyfive_twentyone_episode_8/?utm_source=share&amp;utm_medium=web2x&amp;context=3)[\\[Episode 9\\]](https://www.reddit.com/r/KDRAMA/comments/tcckit/twentyfive_twentyone_episode_9/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 10\\]](https://www.reddit.com/r/KDRAMA/comments/td1cuy/twentyfive_twentyone_episode_10/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 11\\]](https://www.reddit.com/r/KDRAMA/comments/thopwa/twentyfive_twentyone_episode_11/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 12\\]](https://www.reddit.com/r/KDRAMA/comments/tihdh7/twentyfive_twentyone_episode_12/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 13\\]](https://www.reddit.com/r/KDRAMA/comments/tojaik/twentyfive_twentyone_episode_13/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 14\\]](https://www.reddit.com/r/KDRAMA/comments/tpfvu1/twentyfive_twentyone_episode_14/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 15\\]](https://www.reddit.com/r/KDRAMA/comments/tuasg0/twentyfive_twentyone_episode_15/?utm_source=share&amp;utm_medium=web2x&amp;context=3)* **Conduct Reminder**:&gt;We encourage our users to read the following before participating in any discussions on [/r/KDRAMA](https://www.reddit.com/r/KDRAMA): (1) [Reddiquette](https://www.reddithelp.com/hc/en-us/articles/205926439), (2) our [Conduct Rules](https://www.reddit.com/r/KDRAMA/wiki/rules#wiki_1._conduct) (3) our [Policies](https://www.reddit.com/r/KDRAMA/wiki/policies), and (4) the [When Discussions Get Personal Post](https://www.reddit.com/r/KDRAMA/comments/jqrjx6/meta_when_discussions_get_personal/).  Any users who are displaying negative conduct (including but not limited to bullying, harassment, or personal attacks) will be given a warning, repeated behaviour will lead to increasing exclusions from our community. Any extreme cases of misconduct (such as racism or hate speech) will result in an immediate permanent ban from our community and a report to Reddit admin.  Additionally, mentions of down-voting, unpopular opinions, and the use of profanity may see your comments locked or removed without notice.* **Spoiler Tag Reminder**:&gt;Be mindful of others who may not have yet seen this drama, and use spoiler tags when discussing key plot developments or other important information. You can create a spoiler tag by writing &gt; ! this! &lt; without the spaces in between to get this &gt;!spoiler!&lt;. For more information about when and how to use spoiler tags see our [Spoiler Tag Wiki](https://www.reddit.com/r/KDRAMA/wiki/spoilertags).\nTwenty-Five, Twenty-One [Episode 11].* **Drama**: [Twenty-Five, Twenty-One](https://asianwiki.com/Twenty-Five,_Twenty-One)   * Korean Title: ìŠ¤ë¬¼ë‹¤ì„¯ ìŠ¤ë¬¼í•˜ë‚˜* **Network**: tvN* **Premiere Date**: February 12, 2022* **Airing Schedule**: Saturday &amp; Sunday, 21:10 KST* **Episodes**: 16* **Director**: [Jung Ji Hyun](https://asianwiki.com/Jung_Ji-Hyun_(director)) (Mr. Sunshine, The King: Eternal Monarch, Search: WWW)* **Writer**: [Kwon Do Eun](https://mydramalist.com/people/21763-kwon-eun-sol) (Search: WWW)* **Cast**: [Kim Tae Ri](https://asianwiki.com/Kim_Tae-Ri) as Na Hee Do, [Nam Joo Hyuk](https://asianwiki.com/Nam_Joo-Hyuk) as Baek Yi Jin, [Bona](https://asianwiki.com/Bona_(WJSN)) as Go Yoo Rim, [Choi Hyun Wook](https://asianwiki.com/Choi_Hyun-Wook_(2002)) as Moon Ji Woong, [Lee Joo Myoung](https://asianwiki.com/Lee_Joo-Myoung) as Ji Seung Wan* **Streaming Source**: [Netflix](https://www.netflix.com/title/81517168)* **Plot** **Synopsis**: In a time when dreams seem out of reach, a teen fencer pursues big ambitions and meets a hardworking young man who seeks to rebuild his life. (Source: Netflix)* **Previous Discussions**: [\\[Episodes 1 &amp; 2\\]](https://www.reddit.com/r/KDRAMA/comments/sql71t/twentyfive_twentyone_episodes_1_2/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 3 &amp; 4\\]](https://www.reddit.com/r/KDRAMA/comments/sw5kqi/twentyfive_twentyone_episodes_3_4/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 5 &amp; 6\\]](https://www.reddit.com/r/KDRAMA/comments/t0yfys/twentyfive_twentyone_episodes_5_6/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 7\\]](https://www.reddit.com/r/KDRAMA/comments/t74moh/twentyfive_twentyone_episode_7/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 8\\]](https://www.reddit.com/r/KDRAMA/comments/t7rpbf/twentyfive_twentyone_episode_8/?utm_source=share&amp;utm_medium=web2x&amp;context=3)[\\[Episode 9\\]](https://www.reddit.com/r/KDRAMA/comments/tcckit/twentyfive_twentyone_episode_9/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 10\\]](https://www.reddit.com/r/KDRAMA/comments/td1cuy/twentyfive_twentyone_episode_10/?utm_source=share&amp;utm_medium=web2x&amp;context=3)* **Conduct Reminder**:&gt;We encourage our users to read the following before participating in any discussions on [/r/KDRAMA](https://www.reddit.com/r/KDRAMA): (1) [Reddiquette](https://www.reddithelp.com/hc/en-us/articles/205926439), (2) our [Conduct Rules](https://www.reddit.com/r/KDRAMA/wiki/rules#wiki_1._conduct) (3) our [Policies](https://www.reddit.com/r/KDRAMA/wiki/policies), and (4) the [When Discussions Get Personal Post](https://www.reddit.com/r/KDRAMA/comments/jqrjx6/meta_when_discussions_get_personal/).  Any users who are displaying negative conduct (including but not limited to bullying, harassment, or personal attacks) will be given a warning, repeated behaviour will lead to increasing exclusions from our community. Any extreme cases of misconduct (such as racism or hate speech) will result in an immediate permanent ban from our community and a report to Reddit admin.  Additionally, mentions of down-voting, unpopular opinions, and the use of profanity may see your comments locked or removed without notice.* **Spoiler Tag Reminder**:&gt;Be mindful of others who may not have yet seen this drama, and use spoiler tags when discussing key plot developments or other important information. You can create a spoiler tag by writing &gt; ! this! &lt; without the spaces in between to get this &gt;!spoiler!&lt;. For more information about when and how to use spoiler tags see our [Spoiler Tag Wiki](https://www.reddit.com/r/KDRAMA/wiki/spoilertags).\nThirty Nine [Episodes 11 &amp; 12].* **Drama**: [Thirty Nine](https://asianwiki.com/Thirty_Nine_(Korean_Drama))   * Korean Title: ì„œë¥¸, ì•„í™‰* **Network**: jTBC* **Premiere Date**: February 16, 2022* **Airing Schedule**: Wednesday &amp; Thursday, 22:30 KST* **Episodes**: 12* **Director**: [Kim Sang Ho](https://mydramalist.com/people/20965-kim-sang-ho) (Age of Youth 2)* **Writer**: [Yoo Young Ah](https://asianwiki.com/Yoo_Young-A_(screenwriter)) (Entertainer, Encounter)* **Cast**: [Son Ye Jin](https://asianwiki.com/Son_Ye-Jin) as Cha Mi Jo, [Jeon Mi Do](https://asianwiki.com/Jeon_Mi-Do) as Jung Chan Young, [Kim Ji Hyun](https://asianwiki.com/Kim_Ji-Hyun_(1982)) as Jang Joo Hee, [Yeon Woo Jin](https://asianwiki.com/Yeon_Woo-Jin) as Kim Sun Woo, [Lee Moo Saeng](https://asianwiki.com/Lee_Moo-Saeng) as Kim Jin Seok, [Lee Tae Hwan](https://asianwiki.com/Lee_Tae-Hwan) as Park Hyun Joon* **Streaming Source**: [Netflix](https://www.netflix.com/title/81568400)* **Plot** **Synopsis**: Leaning on each other through thick and thin, a trio of best friends stand together as they experience life, love and loss on the brink of turning 40. (Source: Netflix)* **Previous Discussions**: [\\[Episodes 1 &amp; 2\\]](https://www.reddit.com/r/KDRAMA/comments/sto0ew/thirty_nine_episodes_1_2/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 3 &amp; 4\\]](https://www.reddit.com/r/KDRAMA/comments/szbtr0/thirty_nine_episodes_3_4/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 5 &amp; 6\\]](https://www.reddit.com/r/KDRAMA/comments/t4wofp/thirty_nine_episodes_5_6/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 7 &amp; 8\\]](https://www.reddit.com/r/KDRAMA/comments/tfdli9/thirty_nine_episodes_7_8/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 9 &amp; 10\\]](https://www.reddit.com/r/KDRAMA/comments/tksi74/thirty_nine_episodes_9_10/?utm_source=share&amp;utm_medium=web2x&amp;context=3)* **Conduct Reminder**:&gt;We encourage our users to read the following before participating in any discussions on [/r/KDRAMA](https://www.reddit.com/r/KDRAMA): (1) [Reddiquette](https://www.reddithelp.com/hc/en-us/articles/205926439), (2) our [Conduct Rules](https://www.reddit.com/r/KDRAMA/wiki/rules#wiki_1._conduct) (3) our [Policies](https://www.reddit.com/r/KDRAMA/wiki/policies), and (4) the [When Discussions Get Personal Post](https://www.reddit.com/r/KDRAMA/comments/jqrjx6/meta_when_discussions_get_personal/).  Any users who are displaying negative conduct (including but not limited to bullying, harassment, or personal attacks) will be given a warning, repeated behaviour will lead to increasing exclusions from our community. Any extreme cases of misconduct (such as racism or hate speech) will result in an immediate permanent ban from our community and a report to Reddit admin.  Additionally, mentions of down-voting, unpopular opinions, and the use of profanity may see your comments locked or removed without notice.* **Spoiler Tag Reminder**:&gt;Be mindful of others who may not have yet seen this drama, and use spoiler tags when discussing key plot developments or other important information. You can create a spoiler tag by writing &gt; ! this! &lt; without the spaces in between to get this &gt;!spoiler!&lt;. For more information about when and how to use spoiler tags see our [Spoiler Tag Wiki](https://www.reddit.com/r/KDRAMA/wiki/spoilertags).\nTwenty-Five, Twenty-One [Episode 15].* **Drama**: [Twenty-Five, Twenty-One](https://asianwiki.com/Twenty-Five,_Twenty-One)   * Korean Title: ìŠ¤ë¬¼ë‹¤ì„¯ ìŠ¤ë¬¼í•˜ë‚˜* **Network**: tvN* **Premiere Date**: February 12, 2022* **Airing Schedule**: Saturday &amp; Sunday, 21:10 KST* **Episodes**: 16* **Director**: [Jung Ji Hyun](https://asianwiki.com/Jung_Ji-Hyun_(director)) (Mr. Sunshine, The King: Eternal Monarch, Search: WWW)* **Writer**: [Kwon Do Eun](https://mydramalist.com/people/21763-kwon-eun-sol) (Search: WWW)* **Cast**: [Kim Tae Ri](https://asianwiki.com/Kim_Tae-Ri) as Na Hee Do, [Nam Joo Hyuk](https://asianwiki.com/Nam_Joo-Hyuk) as Baek Yi Jin, [Bona](https://asianwiki.com/Bona_(WJSN)) as Go Yoo Rim, [Choi Hyun Wook](https://asianwiki.com/Choi_Hyun-Wook_(2002)) as Moon Ji Woong, [Lee Joo Myoung](https://asianwiki.com/Lee_Joo-Myoung) as Ji Seung Wan* **Streaming Source**: [Netflix](https://www.netflix.com/title/81517168)* **Plot** **Synopsis**: In a time when dreams seem out of reach, a teen fencer pursues big ambitions and meets a hardworking young man who seeks to rebuild his life. (Source: Netflix)* **Previous Discussions**: [\\[Episodes 1 &amp; 2\\]](https://www.reddit.com/r/KDRAMA/comments/sql71t/twentyfive_twentyone_episodes_1_2/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 3 &amp; 4\\]](https://www.reddit.com/r/KDRAMA/comments/sw5kqi/twentyfive_twentyone_episodes_3_4/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 5 &amp; 6\\]](https://www.reddit.com/r/KDRAMA/comments/t0yfys/twentyfive_twentyone_episodes_5_6/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 7\\]](https://www.reddit.com/r/KDRAMA/comments/t74moh/twentyfive_twentyone_episode_7/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 8\\]](https://www.reddit.com/r/KDRAMA/comments/t7rpbf/twentyfive_twentyone_episode_8/?utm_source=share&amp;utm_medium=web2x&amp;context=3)[\\[Episode 9\\]](https://www.reddit.com/r/KDRAMA/comments/tcckit/twentyfive_twentyone_episode_9/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 10\\]](https://www.reddit.com/r/KDRAMA/comments/td1cuy/twentyfive_twentyone_episode_10/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 11\\]](https://www.reddit.com/r/KDRAMA/comments/thopwa/twentyfive_twentyone_episode_11/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 12\\]](https://www.reddit.com/r/KDRAMA/comments/tihdh7/twentyfive_twentyone_episode_12/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 13\\]](https://www.reddit.com/r/KDRAMA/comments/tojaik/twentyfive_twentyone_episode_13/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episode 14\\]](https://www.reddit.com/r/KDRAMA/comments/tpfvu1/twentyfive_twentyone_episode_14/?utm_source=share&amp;utm_medium=web2x&amp;context=3)* **Conduct Reminder**:&gt;We encourage our users to read the following before participating in any discussions on [/r/KDRAMA](https://www.reddit.com/r/KDRAMA): (1) [Reddiquette](https://www.reddithelp.com/hc/en-us/articles/205926439), (2) our [Conduct Rules](https://www.reddit.com/r/KDRAMA/wiki/rules#wiki_1._conduct) (3) our [Policies](https://www.reddit.com/r/KDRAMA/wiki/policies), and (4) the [When Discussions Get Personal Post](https://www.reddit.com/r/KDRAMA/comments/jqrjx6/meta_when_discussions_get_personal/).  Any users who are displaying negative conduct (including but not limited to bullying, harassment, or personal attacks) will be given a warning, repeated behaviour will lead to increasing exclusions from our community. Any extreme cases of misconduct (such as racism or hate speech) will result in an immediate permanent ban from our community and a report to Reddit admin.  Additionally, mentions of down-voting, unpopular opinions, and the use of profanity may see your comments locked or removed without notice.* **Spoiler Tag Reminder**:&gt;Be mindful of others who may not have yet seen this drama, and use spoiler tags when discussing key plot developments or other important information. You can create a spoiler tag by writing &gt; ! this! &lt; without the spaces in between to get this &gt;!spoiler!&lt;. For more information about when and how to use spoiler tags see our [Spoiler Tag Wiki](https://www.reddit.com/r/KDRAMA/wiki/spoilertags).\nThirty Nine [Episodes 9 &amp; 10].* **Drama**: [Thirty Nine](https://asianwiki.com/Thirty_Nine_(Korean_Drama))   * Korean Title: ì„œë¥¸, ì•„í™‰* **Network**: jTBC* **Premiere Date**: February 16, 2022* **Airing Schedule**: Wednesday &amp; Thursday, 22:30 KST* **Episodes**: 12* **Director**: [Kim Sang Ho](https://mydramalist.com/people/20965-kim-sang-ho) (Age of Youth 2)* **Writer**: [Yoo Young Ah](https://asianwiki.com/Yoo_Young-A_(screenwriter)) (Entertainer, Encounter)* **Cast**: [Son Ye Jin](https://asianwiki.com/Son_Ye-Jin) as Cha Mi Jo, [Jeon Mi Do](https://asianwiki.com/Jeon_Mi-Do) as Jung Chan Young, [Kim Ji Hyun](https://asianwiki.com/Kim_Ji-Hyun_(1982)) as Jang Joo Hee, [Yeon Woo Jin](https://asianwiki.com/Yeon_Woo-Jin) as Kim Sun Woo, [Lee Moo Saeng](https://asianwiki.com/Lee_Moo-Saeng) as Kim Jin Seok, [Lee Tae Hwan](https://asianwiki.com/Lee_Tae-Hwan) as Park Hyun Joon* **Streaming Source**: [Netflix](https://www.netflix.com/title/81568400)* **Plot** **Synopsis**: Leaning on each other through thick and thin, a trio of best friends stand together as they experience life, love and loss on the brink of turning 40. (Source: Netflix)* **Previous Discussions**: [\\[Episodes 1 &amp; 2\\]](https://www.reddit.com/r/KDRAMA/comments/sto0ew/thirty_nine_episodes_1_2/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 3 &amp; 4\\]](https://www.reddit.com/r/KDRAMA/comments/szbtr0/thirty_nine_episodes_3_4/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 5 &amp; 6\\]](https://www.reddit.com/r/KDRAMA/comments/t4wofp/thirty_nine_episodes_5_6/?utm_source=share&amp;utm_medium=web2x&amp;context=3) [\\[Episodes 7 &amp; 8\\]](https://www.reddit.com/r/KDRAMA/comments/tfdli9/thirty_nine_episodes_7_8/?utm_source=share&amp;utm_medium=web2x&amp;context=3)* **Conduct Reminder**:&gt;We encourage our users to read the following before participating in any discussions on [/r/KDRAMA](https://www.reddit.com/r/KDRAMA): (1) [Reddiquette](https://www.reddithelp.com/hc/en-us/articles/205926439), (2) our [Conduct Rules](https://www.reddit.com/r/KDRAMA/wiki/rules#wiki_1._conduct) (3) our [Policies](https://www.reddit.com/r/KDRAMA/wiki/policies), and (4) the [When Discussions Get Personal Post](https://www.reddit.com/r/KDRAMA/comments/jqrjx6/meta_when_discussions_get_personal/).  Any users who are displaying negative conduct (including but not limited to bullying, harassment, or personal attacks) will be given a warning, repeated behaviour will lead to increasing exclusions from our community. Any extreme cases of misconduct (such as racism or hate speech) will result in an immediate permanent ban from our community and a report to Reddit admin.  Additionally, mentions of down-voting, unpopular opinions, and the use of profanity may see your comments locked or removed without notice.* **Spoiler Tag Reminder**:&gt;Be mindful of others who may not have yet seen this drama, and use spoiler tags when discussing key plot developments or other important information. You can create a spoiler tag by writing &gt; ! this! &lt; without the spaces in between to get this &gt;!spoiler!&lt;. For more information about when and how to use spoiler tags see our [Spoiler Tag Wiki](https://www.reddit.com/r/KDRAMA/wiki/spoilertags).\n[Naver TV](https://tv.naver.com/mbc.Itsshowtime/)&gt;â€œIt's Showtime!â€ is a fantasy rom-com drama about a popular magician named Cha Cha Woong (Park Hae Jin) who can summon ghosts and a passionate police constable named Go Seul Hae (Jin Ki Joo) who has supernatural powers.&gt;&gt;The cast also includes Jung Joon Ho, Jung Suk Yong, Go Kyu Pil, and Kim Hee Jae.[Source](https://www.soompi.com/article/1516283wpp/park-hae-jin-and-jin-ki-joos-upcoming-rom-com-from-now-showtime-confirms-premiere-date)\n[Naver TV](https://tv.naver.com/mbc.Itsshowtime/)&gt;â€œIt's Showtime!â€ is a fantasy rom-com drama about a popular magician named Cha Cha Woong (Park Hae Jin) who can summon ghosts and a passionate police constable named Go Seul Hae (Jin Ki Joo) who has supernatural powers.&gt;&gt;The cast also includes Jung Joon Ho, Jung Suk Yong, Go Kyu Pil, and Kim Hee Jae.[Source](https://www.soompi.com/article/1516283wpp/park-hae-jin-and-jin-ki-joos-upcoming-rom-com-from-now-showtime-confirms-premiere-date)\nmvp!\nSquid Game finished\nCPU times: user 15.2 s, sys: 1.12 s, total: 16.3 s\nWall time: 17.5 s\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 64,
     "data": {
      "application/vnd.deepnote.dataframe.v3+json": {
       "column_count": 5,
       "row_count": 404,
       "columns": [
        {
         "name": "Entity",
         "dtype": "object",
         "stats": {
          "unique_count": 404,
          "nan_count": 0,
          "categories": [
           {
            "name": "SEARCH",
            "count": 1
           },
           {
            "name": "TWENTY - FIVE TWENTY - ONE",
            "count": 1
           },
           {
            "name": "402 others",
            "count": 402
           }
          ]
         }
        },
        {
         "name": "CRF_Model_Found",
         "dtype": "float64",
         "stats": {
          "unique_count": 11,
          "nan_count": 116,
          "min": "1.0",
          "max": "16.0",
          "histogram": [
           {
            "bin_start": 1,
            "bin_end": 2.5,
            "count": 262
           },
           {
            "bin_start": 2.5,
            "bin_end": 4,
            "count": 5
           },
           {
            "bin_start": 4,
            "bin_end": 5.5,
            "count": 13
           },
           {
            "bin_start": 5.5,
            "bin_end": 7,
            "count": 2
           },
           {
            "bin_start": 7,
            "bin_end": 8.5,
            "count": 2
           },
           {
            "bin_start": 8.5,
            "bin_end": 10,
            "count": 0
           },
           {
            "bin_start": 10,
            "bin_end": 11.5,
            "count": 1
           },
           {
            "bin_start": 11.5,
            "bin_end": 13,
            "count": 1
           },
           {
            "bin_start": 13,
            "bin_end": 14.5,
            "count": 1
           },
           {
            "bin_start": 14.5,
            "bin_end": 16,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "Exact_CRF",
         "dtype": "float64",
         "stats": {
          "unique_count": 29,
          "nan_count": 116,
          "min": "0.0",
          "max": "385.0",
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 38.5,
            "count": 279
           },
           {
            "bin_start": 38.5,
            "bin_end": 77,
            "count": 4
           },
           {
            "bin_start": 77,
            "bin_end": 115.5,
            "count": 2
           },
           {
            "bin_start": 115.5,
            "bin_end": 154,
            "count": 1
           },
           {
            "bin_start": 154,
            "bin_end": 192.5,
            "count": 0
           },
           {
            "bin_start": 192.5,
            "bin_end": 231,
            "count": 0
           },
           {
            "bin_start": 231,
            "bin_end": 269.5,
            "count": 1
           },
           {
            "bin_start": 269.5,
            "bin_end": 308,
            "count": 0
           },
           {
            "bin_start": 308,
            "bin_end": 346.5,
            "count": 0
           },
           {
            "bin_start": 346.5,
            "bin_end": 385,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "Exact_Whitelist",
         "dtype": "float64",
         "stats": {
          "unique_count": 33,
          "nan_count": 276,
          "min": "1.0",
          "max": "403.0",
          "histogram": [
           {
            "bin_start": 1,
            "bin_end": 41.2,
            "count": 115
           },
           {
            "bin_start": 41.2,
            "bin_end": 81.4,
            "count": 6
           },
           {
            "bin_start": 81.4,
            "bin_end": 121.60000000000001,
            "count": 3
           },
           {
            "bin_start": 121.60000000000001,
            "bin_end": 161.8,
            "count": 3
           },
           {
            "bin_start": 161.8,
            "bin_end": 202,
            "count": 0
           },
           {
            "bin_start": 202,
            "bin_end": 242.20000000000002,
            "count": 0
           },
           {
            "bin_start": 242.20000000000002,
            "bin_end": 282.40000000000003,
            "count": 0
           },
           {
            "bin_start": 282.40000000000003,
            "bin_end": 322.6,
            "count": 0
           },
           {
            "bin_start": 322.6,
            "bin_end": 362.8,
            "count": 0
           },
           {
            "bin_start": 362.8,
            "bin_end": 403,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "CRF_Recall",
         "dtype": "float64",
         "stats": {
          "unique_count": 36,
          "nan_count": 116,
          "min": "0.0025974025974025974",
          "max": "inf",
          "histogram": [
           {
            "bin_start": 0.0025974025974025974,
            "bin_end": 0.10233766233766234,
            "count": 16
           },
           {
            "bin_start": 0.10233766233766234,
            "bin_end": 0.20207792207792208,
            "count": 15
           },
           {
            "bin_start": 0.20207792207792208,
            "bin_end": 0.3018181818181818,
            "count": 9
           },
           {
            "bin_start": 0.3018181818181818,
            "bin_end": 0.40155844155844156,
            "count": 15
           },
           {
            "bin_start": 0.40155844155844156,
            "bin_end": 0.5012987012987012,
            "count": 14
           },
           {
            "bin_start": 0.5012987012987012,
            "bin_end": 0.601038961038961,
            "count": 2
           },
           {
            "bin_start": 0.601038961038961,
            "bin_end": 0.7007792207792208,
            "count": 4
           },
           {
            "bin_start": 0.7007792207792208,
            "bin_end": 0.8005194805194805,
            "count": 3
           },
           {
            "bin_start": 0.8005194805194805,
            "bin_end": 0.9002597402597402,
            "count": 0
           },
           {
            "bin_start": 0.9002597402597402,
            "bin_end": 1,
            "count": 152
           }
          ]
         }
        },
        {
         "name": "_deepnote_index_column",
         "dtype": "int64"
        }
       ],
       "rows": [
        {
         "Entity": "SEARCH",
         "CRF_Model_Found": 16,
         "Exact_CRF": 249,
         "Exact_Whitelist": "nan",
         "CRF_Recall": 0.0642570281124498,
         "_deepnote_index_column": 272
        },
        {
         "Entity": "TWENTY - FIVE TWENTY - ONE",
         "CRF_Model_Found": 14,
         "Exact_CRF": 23,
         "Exact_Whitelist": "nan",
         "CRF_Recall": 0.6086956521739131,
         "_deepnote_index_column": 363
        },
        {
         "Entity": "KINGDOM",
         "CRF_Model_Found": 12,
         "Exact_CRF": 83,
         "Exact_Whitelist": "nan",
         "CRF_Recall": 0.14457831325301204,
         "_deepnote_index_column": 145
        },
        {
         "Entity": "PANCAKES|3 LUTZ|6 | CROCHET HOOKS |3 CORINNA|6 | HAIR ORNAMENTS |3OTTO|7 | CHOPSTICKS |8BENNO|10 | PAPER",
         "CRF_Model_Found": 10,
         "Exact_CRF": 0,
         "Exact_Whitelist": "nan",
         "CRF_Recall": "inf",
         "_deepnote_index_column": 236
        },
        {
         "Entity": "NOW I",
         "CRF_Model_Found": 8,
         "Exact_CRF": 152,
         "Exact_Whitelist": "nan",
         "CRF_Recall": 0.05263157894736842,
         "_deepnote_index_column": 223
        },
        {
         "Entity": "HEALER GIRL",
         "CRF_Model_Found": 8,
         "Exact_CRF": 12,
         "Exact_Whitelist": "nan",
         "CRF_Recall": 0.6666666666666666,
         "_deepnote_index_column": 99
        },
        {
         "Entity": "DRESS UP DARLING",
         "CRF_Model_Found": 6,
         "Exact_CRF": 48,
         "Exact_Whitelist": "nan",
         "CRF_Recall": 0.125,
         "_deepnote_index_column": 65
        },
        {
         "Entity": "THE KING : ETERNAL MONARCH",
         "CRF_Model_Found": 6,
         "Exact_CRF": 9,
         "Exact_Whitelist": "nan",
         "CRF_Recall": 0.6666666666666666,
         "_deepnote_index_column": 317
        },
        {
         "Entity": "MY DRESS - UP DARLING",
         "CRF_Model_Found": 5,
         "Exact_CRF": 15,
         "Exact_Whitelist": "nan",
         "CRF_Recall": 0.3333333333333333,
         "_deepnote_index_column": 196
        },
        {
         "Entity": "VAGABOND",
         "CRF_Model_Found": 5,
         "Exact_CRF": 13,
         "Exact_Whitelist": 8,
         "CRF_Recall": 0.38461538461538464,
         "_deepnote_index_column": 371
        }
       ]
      },
      "text/plain": "                                                Entity  CRF_Model_Found  \\\n272                                             SEARCH             16.0   \n363                         TWENTY - FIVE TWENTY - ONE             14.0   \n145                                            KINGDOM             12.0   \n236  PANCAKES|3 LUTZ|6 | CROCHET HOOKS |3 CORINNA|6...             10.0   \n223                                              NOW I              8.0   \n..                                                 ...              ...   \n392                           WHEN THE WEATHER IS FINE              NaN   \n394                                            WHISPER              NaN   \n401                                              YOUTH              NaN   \n402                                       YOUTH OF MAY              NaN   \n403                                   ZOMBIE DETECTIVE              NaN   \n\n     Exact_CRF  Exact_Whitelist  CRF_Recall  \n272      249.0              NaN    0.064257  \n363       23.0              NaN    0.608696  \n145       83.0              NaN    0.144578  \n236        0.0              NaN         inf  \n223      152.0              NaN    0.052632  \n..         ...              ...         ...  \n392        NaN              4.0         NaN  \n394        NaN              6.0         NaN  \n401        NaN             14.0         NaN  \n402        NaN              1.0         NaN  \n403        NaN              1.0         NaN  \n\n[404 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Entity</th>\n      <th>CRF_Model_Found</th>\n      <th>Exact_CRF</th>\n      <th>Exact_Whitelist</th>\n      <th>CRF_Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>272</th>\n      <td>SEARCH</td>\n      <td>16.0</td>\n      <td>249.0</td>\n      <td>NaN</td>\n      <td>0.064257</td>\n    </tr>\n    <tr>\n      <th>363</th>\n      <td>TWENTY - FIVE TWENTY - ONE</td>\n      <td>14.0</td>\n      <td>23.0</td>\n      <td>NaN</td>\n      <td>0.608696</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>KINGDOM</td>\n      <td>12.0</td>\n      <td>83.0</td>\n      <td>NaN</td>\n      <td>0.144578</td>\n    </tr>\n    <tr>\n      <th>236</th>\n      <td>PANCAKES|3 LUTZ|6 | CROCHET HOOKS |3 CORINNA|6...</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>inf</td>\n    </tr>\n    <tr>\n      <th>223</th>\n      <td>NOW I</td>\n      <td>8.0</td>\n      <td>152.0</td>\n      <td>NaN</td>\n      <td>0.052632</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>392</th>\n      <td>WHEN THE WEATHER IS FINE</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>394</th>\n      <td>WHISPER</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>401</th>\n      <td>YOUTH</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>14.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>402</th>\n      <td>YOUTH OF MAY</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>403</th>\n      <td>ZOMBIE DETECTIVE</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>404 rows Ã— 5 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "a129de1cb648484681e78e52fe39bb31",
    "tags": [],
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 0,
     "w": 12,
     "h": 5
    },
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4a34693b",
    "execution_start": 1650852491532,
    "execution_millis": 2,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 765
   },
   "source": "def make_wordcloud(df, title = 'Top CRF Results'):  \n\n    res = dict(zip(df['Entity'].dropna(), df['CRF_Model_Found'].dropna().astype('int32')))\n\n    def custom_color_func(word, font_size, position, orientation, random_state=None,\n                        **kwargs):\n        return \"hsl(0, 100%%, %d%%)\" % random.randint(30, 70)\n\n    reddit_mask = np.array(Image.open(\"./streamlit/images/reddit-logo5-2.jpg\"))\n\n    wordcloud = WordCloud(width=800,\n                    height=800,\n                    background_color=\"white\", \n                        mask=reddit_mask,\n                        contour_width=0, \n                        repeat=True,\n                        min_font_size=3,\n                        contour_color='red')\n\n    # Generate a wordcloud\n    wordcloud.generate_from_frequencies(res)\n\n    # store to file\n    #wordcloud.to_file(\"img_dir/dove_wordcloud.png\")\n\n    # show\n    plt.figure( figsize=(10,10) )\n    #plt.imshow(wordcloud)\n\n    plt.annotate(title, xy =(200, 200),\n                xytext =(20, 20),\n                fontsize=20\n                )\n\n    plt.imshow(wordcloud.recolor(color_func=custom_color_func, random_state=3),\n            interpolation=\"bilinear\")\n\n    plt.axis(\"off\")\n    plt.show()",
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "7b1f6645dd4243319b8bdaff3f6bb3c6",
    "tags": [],
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 0,
     "w": 12,
     "h": 5
    },
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a34870b5",
    "execution_start": 1650852491536,
    "execution_millis": 3,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 135,
    "deepnote_output_heights": [
     565
    ]
   },
   "source": "\n# df = pd.read_csv('/work/MADS_698_Capstone/output_step5/crf_results_Elon_Musk.csv')\n# make_wordcloud(df)\n",
   "execution_count": 66,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "dc18a8c6a0544fc5bdee0b7530ecfb3a",
    "tags": [],
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 0,
     "w": 12,
     "h": 5
    },
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e85c8f1a",
    "execution_start": 1650852491544,
    "execution_millis": 4,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 783
   },
   "source": "def wordloud_from_list(word_list, title = ''):\n    \n\n    def custom_color_func(word, font_size, position, orientation, random_state=None,\n                        **kwargs):\n        return \"hsl(0, 100%%, %d%%)\" % random.randint(30, 70)\n\n    reddit_mask = np.array(Image.open(\"./streamlit/images/reddit-logo5-2.jpg\"))\n\n    wordcloud = WordCloud(width=800,\n                    height=800,\n                    background_color=\"white\", \n                        mask=reddit_mask,\n                        contour_width=0, \n                        repeat=True,\n                        min_font_size=3,\n                        contour_color='red')\n\n    # Generate a wordcloud\n    random.shuffle(word_list)\n    wordcloud.generate(' '.join(word_list))\n\n    # store to file\n    #wordcloud.to_file(\"img_dir/dove_wordcloud.png\")\n\n    # show\n    plt.figure( figsize=(10,10) )\n    #plt.imshow(wordcloud)\n\n    plt.annotate(title, xy =(200, 200),\n                xytext =(20, 20),\n                fontsize=20\n                )\n\n    plt.imshow(wordcloud.recolor(color_func=custom_color_func, random_state=3),\n            interpolation=\"bilinear\")\n\n    plt.axis(\"off\")\n    plt.show()\n",
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "69b8d66905c54789acf8b0795001ee5e",
    "tags": [],
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 0,
     "w": 12,
     "h": 5
    },
    "deepnote_to_be_reexecuted": false,
    "source_hash": "63167d73",
    "execution_start": 1650852491593,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 369
   },
   "source": "thanks = ['Thanks',\n'Merci',\n'Gracias',\n'Danke',\n'Grazie',\n'Obrigado',\n\"Mamnoon\",\n'Dhanyavaad',\n'XiÃ¨xiÃ¨',\n'Shukraan',\n'Arigato',\n'KhÃ awpkhun'\n'Ngiyabonga',\n\"AhÃ©heeâ€™\",\n'MÄlÅ',\n'Dankon',\n'Spasibo']",
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "49e60fa9f42f4c10a786cd5e6e5f5936",
    "tags": [],
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 0,
     "w": 12,
     "h": 5
    },
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e8d3594f",
    "execution_start": 1650852491594,
    "execution_millis": 0,
    "owner_user_id": "d1c3db5a-4505-48be-99b3-4a4061f5f711",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81,
    "deepnote_output_heights": [
     560
    ]
   },
   "source": "#wordloud_from_list(thanks)",
   "execution_count": 69,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=90b052a7-f47d-474e-888f-9345355cfd9a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "009217dd-14b6-42c6-b78c-be29bb1bd40a",
  "deepnote_execution_queue": [],
  "deepnote_app_layout": "article"
 }
}