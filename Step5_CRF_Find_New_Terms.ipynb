{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d65a962864e949f3be65bb8e64e21a1e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650225410406,
    "execution_millis": 1433261,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": null,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "fff9c202-f3eb-414b-8e09-6e41c40bc822",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "6f7927de",
    "execution_start": 1650225410406,
    "execution_millis": 13322,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 6,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 822.21875
   },
   "source": "import pickle\nimport re\n\nimport stanza\nstanza.download('en') # download English model\nnlp = stanza.Pipeline(lang='en', processors='tokenize,pos')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom itertools import chain, groupby\n\nimport pycrfsuite\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\n#import itertools\nfrom collections import Counter, OrderedDict\nfrom os.path import exists as file_exists\n\n",
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ebdfa1e175414daa8976bf6b0f456701"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "2022-04-17 19:56:52 INFO: Downloading default packages for language: en (English)...\n",
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.3.0/models/default.zip:   0%|          | 0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4360bed0e803424aad54319a49b645f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "text": "2022-04-17 19:57:02 INFO: Finished downloading models and saved to /root/stanza_resources.\n2022-04-17 19:57:02 INFO: Loading these models for language: en (English):\n========================\n| Processor | Package  |\n------------------------\n| tokenize  | combined |\n| pos       | combined |\n========================\n\n2022-04-17 19:57:02 INFO: Use device: cpu\n2022-04-17 19:57:02 INFO: Loading: tokenize\n2022-04-17 19:57:02 INFO: Loading: pos\n2022-04-17 19:57:02 INFO: Done loading processors!\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Tag goes here",
   "metadata": {
    "cell_id": "eb0bd74ed8bd4559a3258da068f32f9a",
    "tags": [],
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 234,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 82
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "60b61936db604bf188c3926fc7e308d3",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4cc3f422",
    "execution_start": 1650225423729,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 210,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 153
   },
   "source": "def remove_spaces_from_term(term_with_spaces):\n    return term_with_spaces.replace(' ', '')\n\ndef swap_spaces_for_underscore(term_with_spaces):\n    return term_with_spaces.replace(' ', '_')",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1cb19d3162b04d56bd28bf00c6e35098",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "831f92f0",
    "execution_start": 1650225423777,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 12,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 189
   },
   "source": "# term_content = dict(itertools.islice(term_content.items(), 2))\n# temp_dict = dict()\n# for k, v in term_content.items():\n#     temp_dict[k] = dict(itertools.islice(v.items(), 100))\n\n# term_content = temp_dict\n# term_content",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "f926382c2d8e426cb642fb2ed3fd7db3",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1bb8d57e",
    "execution_start": 1650225423778,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 18,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 171
   },
   "source": "# key = \"MicroMacro: Crime City\" #'The Castles of Burgundy'\n\n# key_nothing_in_parens = re.sub(r'\\([^)]*\\)', '', key).strip()\n# key_nothing_in_parens = re.sub(r'[^\\w\\s]', '', key_nothing_in_parens).split()\n\n# key_nothing_in_parens",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ff14bdf7a1294f0cb4280663f3a48b4e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "903c850d",
    "execution_start": 1650225423779,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 24,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 387
   },
   "source": "def get_clean_parsed_key_from_parsed_sentence(nlp_sent):\n    \n    parens_idx = -4        \n    # for word in nlp_sent.tokens:\n    #     if word.text == '(': # remove explanatory text in parens after wiki_key\n    #         parens_idx = word.id[0] - 1\n    #         break    \n\n    #print('parens:', parens_idx)\n    parsed_key = [term.text for term in nlp_sent.tokens[:parens_idx]] # length of seperator above\n    parsed_key = parsed_key[1:-1] # length of seperator above\n    #print(\"new:\", parsed_key)\n\n    return parsed_key   \n\n# test_key = nlp('\"Puerto Rico\" in a thing.').sentences[0]\n# test_key.tokens\n# get_clean_parsed_key_from_parsed_sentence(test_key)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Tag 2",
   "metadata": {
    "cell_id": "d87bbcd6182d438f870341a7d3900640",
    "tags": [],
    "is_collapsed": false,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 240,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "524598779fe848298a909dfc587a9e77",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "97117c6c",
    "execution_start": 1650225423781,
    "execution_millis": 9,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 30,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 225
   },
   "source": "def get_idxs_of_sublist_matches(big_list, substring_to_find):\n    indexes = []\n\n    # https://stackoverflow.com/questions/10459493/find-indexes-of-sequence-in-list-in-python\n    for i in range(len(big_list)):\n        if big_list[i:i+len(substring_to_find)] == substring_to_find:\n            indexes.append((i, i+len(substring_to_find)))\n\n    return indexes",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c3efc8e6fb314a93b7df0277834635fd",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "fe66fcde",
    "execution_start": 1650225423791,
    "execution_millis": 10,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 36,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "#get_idxs_of_substring_matches(term_content[test_key], test_key)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ed17b75e1669479791d43e2094af0b3b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "30f03af0",
    "execution_start": 1650225423856,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 42,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 171
   },
   "source": "def expand_all_tups_to_list(tup_list):\n    \n    exp_list = []\n    for tup in tup_list:\n        exp_list.extend(list(range(tup[0], tup[1])))\n    return exp_list",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "9f862030b69248249f4c8aef1804999d",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "71e8df3b",
    "execution_start": 1650225423858,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 48,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 189
   },
   "source": "def clean_text_for_crf_parsing(text):\n    text = text.replace('\\n', '')\n    text = text.replace('+/u/sodogetip', '')\n    sent_remove_between_equals = re.sub(r'==[^=]*==', '', text).strip() \n\n    return sent_remove_between_equals\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c79c82912a9d46a2a3e4c74c8a0c6d7a",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "432ec7ff",
    "execution_start": 1650225423900,
    "execution_millis": 6,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 54,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 657
   },
   "source": "def return_parsed_words_and_pos_tuples(stanza_nlp_doc, first_is_key=False):\n    \n    all_sents = []\n    parsed_key = ''\n    for idx, sent in enumerate(stanza_nlp_doc.sentences):\n        if (first_is_key & (idx == 0)): # first sentence(\"xxx\" is a thing.), because we concatenated the 'key' to the text\n\n            #print('first sent:', [term.text for term in sent.tokens])\n\n            # parens_idx = -4        \n            # for word in sent.tokens:\n            #     if word.text == '(': # remove explanatory text in parens after wiki_key\n            #         parens_idx = word.id[0] - 1\n            #         break    \n\n            # #print('parens:', parens_idx)\n            # parsed_key = [term.text for term in sent.tokens[:parens_idx]] # length of seperator above\n            # parsed_key = parsed_key[1:-1] # length of seperator above\n            # print(\"new:\", parsed_key)\n            parsed_key = get_clean_parsed_key_from_parsed_sentence(sent)\n        else:            \n            # real wiki text starts now\n            nlp_parsed_sent = [(word.text, word.upos) for token in sent.tokens for word in token.words]\n            # all_sents is a list of tuples, (the_word, part_of_speech) for each word in a sentence\n            all_sents.append(nlp_parsed_sent)\n\n            # if (idx == 1):\n            #     print('nlp_parsed:', nlp_parsed_sent)\n        # add 'word.xpos,' later\n\n    #key_words = [ss.upper() for ss in parsed_key]\n\n    return parsed_key, all_sents ",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "7edb2cb879c84de5b1398a5ac33fdc98",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "12578b8a",
    "execution_start": 1650225423963,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 60,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 783
   },
   "source": "def label_traing_data_for_crf(key_words, stanza_word_pos_tuples):\n    \n    list_of_labeled_sentences_in_cat = []\n    \n    for idx_all, sent in enumerate(stanza_word_pos_tuples):\n        #print(sent) \n        list_of_single_sentence = []   \n        list_of_only_words_from_parsed_tuple = [st[0].upper() for st in sent]\n\n        key_words = [kw.upper() for kw in key_words]    \n\n        indexes = get_idxs_of_sublist_matches(list_of_only_words_from_parsed_tuple, key_words)\n\n        # # https://stackoverflow.com/questions/10459493/find-indexes-of-sequence-in-list-in-python\n        # for i in range(len(list_of_only_words_from_parsed_tuple)):\n                        \n        #     if list_of_only_words_from_parsed_tuple[i:i+len(key_words)] == key_words:\n        #         indexes.append((i, i+len(key_words)))\n\n        # #all_sents[0], indexes\n\n        # This is to expand the indexes tuple above (3,6), into a list [3,4,5,6]\n        # to know all the indexes in which to put 'Y's, the rest being 'N's\n        for idx, tup in enumerate(sent):\n            if idx in expand_all_tups_to_list(indexes):\n                tup = tup+('Y',)\n            else:\n                tup = tup+('N',)\n\n            # (',', 'PUNCT', 'N'),\n            if (tup[0] == ',') & (tup[1] == 'PUNCT') & (tup[2] == 'Y'):\n                tup = (',', 'PUNCT', 'N')\n            list_of_single_sentence.append(tup)\n\n        list_of_labeled_sentences_in_cat.append(list_of_single_sentence)\n\n        # if idx_all == 0:\n        #     print('labeled:', list_of_single_sentence)\n\n    return list_of_labeled_sentences_in_cat",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "43c13debf5fa4c459997294e844ec686",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5871981e",
    "execution_start": 1650225423998,
    "execution_millis": 4,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 66,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1104.375
   },
   "source": "%%time\n# New version with single call to stanza nlp library per term\n\ndef get_parsed_labeled_sentences_from_cat_dict(term_content, mvp_flag, wiki_term):\n    labeled_sentences_per_cat = dict()\n    pkl_file_name = './output_step5/dict200_labeled_cat_sentences_' + swap_spaces_for_underscore(wiki_term) + '.pickle'\n\n    if mvp_flag and file_exists(pkl_file_name):\n        print('mvp! wikipedia article already parsed and labeled.')\n        with open(pkl_file_name, 'rb') as handle:\n            labeled_sentences_per_cat = pickle.load(handle)\n    else:\n\n\n        for k_cat,v_cat_dicts in term_content.items():\n            list_of_labeled_sentences_in_cat = []\n            for wiki_key,v_wiki_text in v_cat_dicts.items():\n                \n                clean_key = re.sub(r'\\([^)]*\\)', '', wiki_key).strip()\n                #parsed_key = get_clean_parsed_key(wiki_key)\n\n                clean_text_to_parse = clean_text_for_crf_parsing(v_wiki_text)\n                seperator = '\" is a thing. ' # 4 items parsed from here: 3 words and period\n\n                #print('seperator:', seperator)\n                # added double-ticks into here , to seperate out the 'key' for stanza\n                doc = nlp('\"' + clean_key + seperator + clean_text_to_parse)\n\n                \n                key_words, parsed_word_pos_sentences = return_parsed_words_and_pos_tuples(doc, True)\n                #print(key_words)\n                \n                list_of_labeled_sentences_in_cat.append(label_traing_data_for_crf(key_words, parsed_word_pos_sentences))\n\n\n            # result is a dictionary, with wiki categories as keys, and values being a list of lists of sentences\n            # 'Category:Territorial acquisition and development games': \n            #    [[('Space', 'PROPN', 'Y'),\n            #    ('Empires', 'PROPN', 'Y'),\n            #    ('4X', 'PROPN', 'Y'),\n            #    ('is', 'AUX', 'N'),\n            #    ('a', 'DET', 'N'),\n            #    ('4X', 'ADJ', 'N'),.......\n\n            labeled_sentences_per_cat[k_cat] = list_of_labeled_sentences_in_cat\n\n        print('Dumping:', pkl_file_name)\n        with open(pkl_file_name, 'wb') as handle:\n            pickle.dump(labeled_sentences_per_cat, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n        # This is Conditional Random Field training data for each category\n    return labeled_sentences_per_cat\n\n# dict_labeled_cat_sentences = get_parsed_labeled_sentences_from_cat_dict(term_content, mvp_flag, wiki_term)\n",
   "outputs": [
    {
     "name": "stdout",
     "text": "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\nWall time: 6.68 µs\n",
     "output_type": "stream"
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "4a6380cbb31242a4bc39ec3c5491db30",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d904828b",
    "execution_start": 1650225424000,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 72,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "# https://notebook.community/thammegowda/notes/nlproc/hw3/notes/pycrf_tutorial\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "cea2ba7f14f8437daa38ff68fdd0b5f1",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650225424005,
    "execution_millis": 3,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 78,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "9b441a1d5eb64ff18151df930f92f79d",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "65d96bd8",
    "execution_start": 1650225424015,
    "execution_millis": 5,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 84,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "#list_combined_cats_labeled_sentences[0]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c09f8b3ea9984a6aabe8d02c06962838",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650225424028,
    "execution_millis": 1419862,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 90,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ff3edac27b604ec28122686fd21e8c4b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "19efc2f2",
    "execution_start": 1650225424030,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 204,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 657
   },
   "source": "def create_labeled_training_data_from_wikipedia(wiki_term, mvp_flag):\n    print(\"Wikipedia term:\", wiki_term)\n    \n    # Load Wikipedia articles created in Step 1\n    with open('./output_step1/wiki_200_cat_content_' + swap_spaces_for_underscore(wiki_term) + '.pickle', 'rb') as handle:\n        term_content = pickle.load(handle)\n\n    # See how many wikipedia article we're workign with\n    print(\"See wikipedia categories and number of article saved as training data to be labeled.\")\n    cume_number_of_items = 0\n    for k,v in term_content.items():\n        cume_number_of_items = cume_number_of_items + len(v)\n        print(k, cume_number_of_items, len(v))\n\n    dict_labeled_cat_sentences = get_parsed_labeled_sentences_from_cat_dict(term_content, mvp_flag, wiki_term)\n\n    list_combined_cats_labeled_sentences = []\n\n    for k_category,v_cat_items in dict_labeled_cat_sentences.items():\n        for sent in v_cat_items:\n            list_combined_cats_labeled_sentences.extend(sent)\n        \n    print(len(list_combined_cats_labeled_sentences), 'sentences from wikipedia available to train on')\n\n    labels = Counter()\n    for sentence in list_combined_cats_labeled_sentences:\n        for word_pos_tup in sentence:\n            labels.update(word_pos_tup[2])\n    print(labels, 'numbers of each label from wikipedia')\n\n    return list_combined_cats_labeled_sentences\n\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0e0f6495746b496db2e82f1188e7a60a",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "dc2d114c",
    "execution_start": 1650225424048,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 96,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 333
   },
   "source": "def train_test_split_labeled_sentences(docs):\n    random.seed(42)\n    random.shuffle(docs)\n\n    tot_sentences = len(docs)\n\n\n    len_train_sents = math.floor((tot_sentences * .8))\n    len_test_sents = tot_sentences - len_train_sents\n    assert len_train_sents + len_test_sents == tot_sentences, \"boo\"\n\n    train_sents = docs[:len_train_sents]\n    test_sents = docs[len_train_sents:]\n\n    return train_sents, test_sents",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3eb8fc3f6f6a4aa69612843820db931c",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "74b42a7f",
    "execution_start": 1650225424065,
    "execution_millis": 6,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 102,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 981
   },
   "source": "def word2features(sent, i):\n    word = sent[i][0]\n    postag = sent[i][1]\n    features = [\n        'bias',\n        'word.lower=' + word.lower(),\n        'word[-3:]=' + word[-3:],\n        'word[-2:]=' + word[-2:],\n        'word.isupper=%s' % word.isupper(),\n        'word.istitle=%s' % word.istitle(),\n        'word.isdigit=%s' % word.isdigit(),\n        'postag=' + postag,\n        'postag[:2]=' + postag[:2],\n    ]\n    if i > 0:\n        word1 = sent[i-1][0]\n        postag1 = sent[i-1][1]\n        features.extend([\n            '-1:word.lower=' + word1.lower(),\n            '-1:word.istitle=%s' % word1.istitle(),\n            '-1:word.isupper=%s' % word1.isupper(),\n            '-1:postag=' + postag1,\n            '-1:postag[:2]=' + postag1[:2],\n        ])\n    else:\n        features.append('BOS')\n        \n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        postag1 = sent[i+1][1]\n        features.extend([\n            '+1:word.lower=' + word1.lower(),\n            '+1:word.istitle=%s' % word1.istitle(),\n            '+1:word.isupper=%s' % word1.isupper(),\n            '+1:postag=' + postag1,\n            '+1:postag[:2]=' + postag1[:2],\n        ])\n    else:\n        features.append('EOS')\n                \n    return features\n\n\ndef sent2features(sent):\n    return [word2features(sent, i) for i in range(len(sent))]\n\ndef sent2labels(sent):\n    return [label for token, postag, label in sent]\n\ndef sent2tokens(sent):\n    return [token for token, postag, label in sent]",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "da8780cb7ac84b02bf26c39c4344d023",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e39772e",
    "execution_start": 1650225424087,
    "execution_millis": 4,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 108,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 207
   },
   "source": "def get_x_y_features_labels(train_sents, test_sents):\n    X_train = [sent2features(s) for s in train_sents]\n    y_train = [sent2labels(s) for s in train_sents]\n\n    X_test = [sent2features(s) for s in test_sents]\n    y_test = [sent2labels(s) for s in test_sents]\n\n    return X_train, y_train, X_test, y_test",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "22279c9952654052aa9cfcb8739fdd99",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650225424091,
    "execution_millis": 1419877,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 216,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "8ee3636776f84930853655087c686079",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1b7b9cf3",
    "execution_start": 1650225424108,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 114,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 693
   },
   "source": "def return_trained_tagger(X_train, y_train, wiki_term, mvp_flag):\n    \n    crf_model_file_name = './output_step5/tagger_200_' + swap_spaces_for_underscore(wiki_term) + '.crfsuite'\n\n    tagger = pycrfsuite.Tagger()\n\n    if not (mvp_flag and file_exists(crf_model_file_name)):\n        trainer = pycrfsuite.Trainer(verbose=False)\n        max_sentences = 10000\n        print('Trainer created, will use max of', max_sentences, 'of', len(X_train), 'items')\n        for xseq, yseq in zip(X_train[:max_sentences], y_train[:max_sentences]):\n            trainer.append(xseq, yseq)\n        print('Training data joined')\n        trainer.set_params({\n        'c1': 1.0,   # coefficient for L1 penalty\n        'c2': 1e-3,  # coefficient for L2 penalty\n        'max_iterations': 200,  # stop earlier\n\n        # include transitions that are possible, but not observed\n        'feature.possible_transitions': True\n        })\n\n        print('training with params:', trainer.params())\n\n        # Train model\n        trainer.train(crf_model_file_name) # 50- 10secs, 200-38secs\n\n        print('trainer_features:', trainer.logparser.last_iteration)\n    else:\n        print('mvp! using existing tagger file')\n    \n    tagger.open(crf_model_file_name)\n\n    return tagger\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "49db52fb615a4b2ca8a5742897904896",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "ff4afbcc",
    "execution_start": 1650225424109,
    "execution_millis": 10,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 120,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 243
   },
   "source": "\n\ndef print_example_sentence(test_sents, tagger):\n    print(\"Sample test sentence and predicted/actual tags\")\n    example_sent = test_sents[0]\n    print(' '.join(sent2tokens(example_sent)), end='\\n\\n')\n\n    print(\"Predicted:\", ' '.join(tagger.tag(sent2features(example_sent))))\n    print(\"Actual:  \", ' '.join(sent2labels(example_sent)))\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d2974890422e48c4ba68eebe53370c17",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "72829a2",
    "execution_start": 1650225424133,
    "execution_millis": 17,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 126,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 459
   },
   "source": "def bio_classification_report(y_true, y_pred):\n    \"\"\"\n    Classification report for a list of BIO-encoded sequences.\n    It computes token-level metrics and discards \"O\" labels.\n    \n    Note that it requires scikit-learn 0.15+ (or a version from github master)\n    to calculate averages properly!\n    \"\"\"\n    lb = LabelBinarizer()\n    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n        \n    tagset = set(lb.classes_) - {'O'}\n    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n    \n    return classification_report(\n        y_true_combined,\n        y_pred_combined,\n        labels = [class_indices[cls] for cls in tagset],\n        target_names = tagset,\n    )",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e7f5d99b29ac42ee96cf2c90a7e8b2bc",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "911c232d",
    "execution_start": 1650225424151,
    "execution_millis": 19,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 132,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 567
   },
   "source": "def classification_report_df(report):\n    report_data = []\n    lines = report.split('\\n')\n    del lines[-5]\n    del lines[-1]\n    del lines[1]\n    for line in lines[1:3]:\n        row = OrderedDict()\n        row_data = line.split()\n        #print(row_data)\n        row_data = list(filter(None, row_data))\n        row['class'] = row_data[0]\n        row['precision'] = float(row_data[1])\n        row['recall'] = float(row_data[2])\n        row['f1_score'] = float(row_data[3])\n        row['support'] = int(row_data[4])\n        report_data.append(row)\n    df = pd.DataFrame.from_dict(report_data)\n    df.set_index('class', inplace=True)\n    return df\n\ndef get_model_accuracy_df(tagger, X_test, y_test):\n    y_pred = [tagger.tag(xseq) for xseq in X_test]\n\n    report = bio_classification_report(y_test, y_pred)\n    return classification_report_df(report).reset_index()\n\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "16f589a45b3d426d9d55065a0a713300",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1f8102cd",
    "execution_start": 1650225424170,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 138,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 369
   },
   "source": "\ndef get_clean_unseen_text_to_predict(wiki_term):\n\n    pkl_file_name = './output_step4/posts_' + swap_spaces_for_underscore(wiki_term) + '.pickle'\n    with open(pkl_file_name, 'rb') as handle:\n        related_posts = pickle.load(handle)\n\n    unseen_text = [clean_text_for_crf_parsing(i) for i in related_posts[:]]\n\n    print(\"Number of Sentences in Reddit text:\", len(unseen_text))\n    print(\"Some example sentences:\")\n    for i in unseen_text[:10]:\n        print(i) \n\n    return unseen_text\n\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "da64c912112d47eabfa62a8c9e383553",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5b9d29bf",
    "execution_start": 1650225424177,
    "execution_millis": 1419838,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 144,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 225
   },
   "source": "# related_posts = ['Good shonen anime similar to Demon Slayer or Hunter X Hunter?',\n#  'Looking for shows like Ratched, Undercover, Collateral, Brand New Cherry Flavor, Zero Zero Zero, Mr. Robot, The Haunting Of The Hill House, Bordertown, Better Call Saul, or Bob?',\n#  'Korean wave be like :. well the drama is fun but its nowhere near to best writing. Seem like you are just new with kdramas. Because best writing being associated more with kdramas like signal, kingdom, mr sunshine, misaeng,etc.',\n#  \"[REQUEST] tv shows similar to Mcmafia. I watched it recently on AMC+.  Avoided it for so long because of the name, surprisingly ended up really enjoying it.  I loved the international feeling of it.  Can't wait for season 2.\",\n#  'Korean wave be like :. Wc👌',\n#  'Korean wave be like :. Even squid game is much better drama than CLOY. I watched both dramas. Both are fun of course. Personally, i found squid game much better drama than CLOY too. I am pretty sure you not yet to watch dramas that i suggested.',\n#  \"My flat mate has taken the security of the milk in next level. He cut one milk bottle into half and put on top of the new one and lock it so nobody can try to steal milk from him.. That's nothing a little steak knife can't fix.\"] + \\\n#  related_posts\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c9e83ca5b9154986a4d1f4b3aef3368b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "44533a50",
    "execution_start": 1650225424198,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 150,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1485
   },
   "source": "\n\ndef get_new_crf_phrases_counts_from_clean_unseen_posts(unseen_text, tagger, mvp_flag, wiki_term):\n    \n    crf_tagged_pkl_file = './output_step5/counter_crf_tagged_phrases_' + swap_spaces_for_underscore(wiki_term) + '.pickle'\n    crf_exact_pkl_file = './output_step5/counter_crf_exact_phrases_' + swap_spaces_for_underscore(wiki_term) + '.pickle'\n    crf_tagged_phrases_in_text = Counter()\n    crf_exact_phrases_in_text = Counter()\n\n    if mvp_flag and file_exists(crf_tagged_pkl_file) and file_exists(crf_exact_pkl_file):\n        print('mvp!')\n        with open(crf_tagged_pkl_file, 'rb') as handle:\n            crf_tagged_phrases_in_text = pickle.load(handle)\n        with open(crf_exact_pkl_file, 'rb') as handle:\n            crf_exact_phrases_in_text = pickle.load(handle)\n    else:            \n\n        for sentence in unseen_text[:]:\n            doc = nlp(sentence)\n            _, unseen_parsed_word_pos_sentences = return_parsed_words_and_pos_tuples(doc, False)\n            #print('uns:', unseen_parsed_word_pos_sentences)\n            X_unseen = [sent2features(s) for s in unseen_parsed_word_pos_sentences]\n            y_unseen_pred = [tagger.tag(xseq) for xseq in X_unseen]\n\n            #print('yun:', y_unseen_pred)\n\n            #new_big_list_sentences = []\n            for idx_big, parsed_sent in enumerate(unseen_parsed_word_pos_sentences):\n                new_small_sentence = []\n                \n                # new_phrases_found = []\n                # idx_seen = []\n                #'n,n,Y,Y,n,n,n,n,Y,Y,Y,n,n'\n                for idx_small, tup in enumerate(parsed_sent[:]):\n                    #unseen_pred = y_unseen_pred[idx_big][idx_small]\n                    \n                    new_small_sentence.append((tup[0], y_unseen_pred[idx_big][idx_small] ))\n\n                    # https://stackoverflow.com/questions/71746563/finding-consecutive-groupings-of-items-in-list-of-lists-in-python\n                found_phrases_in_sent = [ ' '.join(i[0] for i in b)for a, b in groupby(new_small_sentence, key=lambda x:x[1]) if a=='Y']\n                \n                if len(found_phrases_in_sent)> 0:\n                    #print('Found:', found_phrases_in_sent)\n\n                    for found_phrase in found_phrases_in_sent:\n\n                        # for each item in found_phrases_in_sent, only once\n                        # if it's in the counter, we've seen it\n                        # for each found entity, we're going to spin through all the posts\n                        # and look for exact matches\n                        if found_phrase.upper() not in crf_tagged_phrases_in_text:\n                            for post in unseen_text: # checking for exact matches of CRF terms in posts\n                                \n                                remove_spaces = [' .', ' ?', ' !', ' :', \" N'T\", ' -', '- ', \" 'S\", \" 'D\", \" 'VE\", \" 'RE\", \" 'M\", \" 'LL\"]\n                                found_phrase_no_spaces = str(found_phrase)\n                                for rem_sp in remove_spaces:\n                                    found_phrase_no_spaces = found_phrase_no_spaces.replace(rem_sp, rem_sp.replace(' ', ''))\n                                                                \n                                crf_exact_phrases_in_text.update({found_phrase.upper(): post.upper().count(found_phrase_no_spaces.upper()) })\n\n\n                    crf_tagged_phrases_in_text.update([i.upper() for i in found_phrases_in_sent])\n\n                    #new_big_list_sentences.append(new_small_sentence)\n                    #print('prs:', parsed_sent)\n\n            #print([tup for sent in new_big_list_sentences for tup in sent])\n            #print(new_big_list_sentences)\n\n        print('Dumping:', crf_tagged_pkl_file, crf_exact_pkl_file)\n        with open(crf_tagged_pkl_file, 'wb') as handle:\n            pickle.dump(crf_tagged_phrases_in_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n        with open(crf_exact_pkl_file, 'wb') as handle:\n            pickle.dump(crf_exact_phrases_in_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    return crf_tagged_phrases_in_text, crf_exact_phrases_in_text\n\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "ffaa6905ebe145f09216ebe84a5f6301",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f101446b",
    "execution_start": 1650225424199,
    "execution_millis": 11,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 156,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "#new_phrases_found_from_crf.most_common()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "c07bd672704f4b7bb984ca5e191c85ed",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "cb6a0593",
    "execution_start": 1650225424248,
    "execution_millis": 1419790,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 162,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 117
   },
   "source": "# try to evaluate true counts from unseen data, based on exact match\n# should have identical keys as above\n#exact_crf_phrase_in_text.most_common()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "243ab1f3a44e4d158f964b6d6c4c9b96",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "77036c90",
    "execution_start": 1650225424249,
    "execution_millis": 0,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 168,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 819
   },
   "source": "\ndef check_unseen_against_wiki_whitelist(wiki_term, unseen_text):\n\n    pkl_file_name = './output_step1/white_list_200_' + swap_spaces_for_underscore(wiki_term) + '.pickle'\n    with open(pkl_file_name, 'rb') as handle:\n        wiki_white_list = pickle.load(handle)\n\n    exact_wiki_phrase_in_text = Counter()\n    all_wiki_articles = set()\n\n    for key_cat, val_list_articles in wiki_white_list.items():\n            all_wiki_articles.update(val_list_articles)\n\n    for sentence in unseen_text:\n        sentence = sentence.upper()  \n\n        for article in all_wiki_articles:\n            if article.upper() in sentence:\n                exact_wiki_phrase_in_text.update([article.upper()])\n    \n    return exact_wiki_phrase_in_text\n    \ndef return_final_df(wiki_term, exact_wiki_phrase_in_text, new_phrases_found_from_crf, exact_crf_phrase_in_text):    \n    df_wiki_white = pd.DataFrame.from_dict(exact_wiki_phrase_in_text.most_common())\n    df_wiki_white.columns = ['Word', 'Exact_Whitelist']\n    df_wiki_white = df_wiki_white.set_index('Word')\n\n    df_crf_results = pd.DataFrame({'CRF_Model_Found': new_phrases_found_from_crf, 'Exact_CRF': exact_crf_phrase_in_text})\n    df_crf_results = df_crf_results.join(df_wiki_white , how='outer')\n\n    df_crf_results['CRF_Recall'] = df_crf_results['CRF_Model_Found']/df_crf_results['Exact_CRF']\n\n    df_crf_results = df_crf_results.reset_index()\n    df_crf_results = df_crf_results.rename(columns={'index': 'Entity'})\n\n    df_crf_results = df_crf_results.sort_values(by=['CRF_Model_Found', 'Exact_CRF', 'CRF_Recall', 'Entity'], ascending=[False, False, False, True], na_position='last')\n    df_crf_results.to_csv('./output_step5/crf_results_' + swap_spaces_for_underscore(wiki_term) + '.csv', encoding='utf-8', index=False)\n\n    return df_crf_results\n\n\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "105edac930cb49d0ae10b0df38789f65",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a37ab8f3",
    "execution_start": 1650225424258,
    "execution_millis": 1419773,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 174,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "#wiki_white_list",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "904e2273b36048228bc97de96f165760",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b31fd813",
    "execution_start": 1650225424286,
    "execution_millis": 1419776,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 180,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "70530d94aa9d47f599e997d083f89827",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650225424297,
    "execution_millis": 1419787,
    "deepnote_table_state": {
     "pageSize": 100,
     "pageIndex": 0,
     "filters": [],
     "sortBy": []
    },
    "deepnote_table_loading": false,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 186,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "59a78861ab7a4542856c5c33e723b83e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "a6ac5a83",
    "execution_start": 1650225424298,
    "execution_millis": 1419779,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 192,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 207
   },
   "source": "# see if any are not in white list\n# check for upper and letter upper/lower case\n# eval accuracy at each step\n# github readme\n# Context of wikipedia is different from reddit for CRF model\n# try getting rid of commas and 'Y'\n# try exact match against whitelist\n# try Spacy near",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "1366ed1c48894424bd826230dac56ef6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "4b562698",
    "execution_start": 1650225424304,
    "execution_millis": 1419767,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 198,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "#intro and motivation, novel item, from scikit",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "98647342de674f4c9faccf58312a1601",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1650225424326,
    "execution_millis": 1419775,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 228,
     "w": 12,
     "h": 5
    },
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "d2d86589bedf4c189d9433c14c3c73d6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "21fb4236",
    "execution_start": 1650225424333,
    "execution_millis": 1637225,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 222,
     "w": 12,
     "h": 5
    },
    "owner_user_id": "9187ece3-39a6-4bea-aed6-9a68f93aaf4f",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1347.5625,
    "deepnote_output_heights": [
     null,
     40.375
    ]
   },
   "source": "%%time\n\n\ndef calculate_final_results_for_wiki_term(wiki_term, mvp_flag):\n    docs = create_labeled_training_data_from_wikipedia(wiki_term, mvp_flag)\n\n    train_sents, test_sents = train_test_split_labeled_sentences(docs)\n    X_train, y_train, X_test, y_test = get_x_y_features_labels(train_sents, test_sents)\n    print('Ready to train.')\n    tagger = return_trained_tagger(X_train, y_train, wiki_term, mvp_flag)\n\n    print_example_sentence(test_sents, tagger)\n\n    print(swap_spaces_for_underscore(wiki_term))\n    print(get_model_accuracy_df(tagger, X_test, y_test ))\n\n    unseen_text = get_clean_unseen_text_to_predict(wiki_term)\n\n    new_phrases_found_from_crf, exact_crf_phrase_in_text = get_new_crf_phrases_counts_from_clean_unseen_posts(unseen_text, tagger, mvp_flag, wiki_term)\n\n    exact_wiki_phrase_in_text = check_unseen_against_wiki_whitelist(wiki_term, unseen_text)\n    df_final = return_final_df(wiki_term, exact_wiki_phrase_in_text, new_phrases_found_from_crf, exact_crf_phrase_in_text)\n    print(wiki_term, 'finished')\n    return df_final\n\ncalculate_final_results_for_wiki_term('Catan', True)\n",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Wikipedia term: Catan\nSee wikipedia categories and number of article saved as training data to be labeled.\nCategory:Board games introduced in 2018 6 6\nCategory:Embracer Group franchises 89 83\nCategory:Board games introduced in 2020 94 5\nCategory:Territorial acquisition and development games 95 1\nCategory:Multiplayer games 157 62\nCategory:Board games with a modular board 200 43\nCategory:Board games introduced in 2019 208 8\nCategory:Board games introduced in 1995 222 14\nmvp! wikipedia article already parsed and labeled.\n13999 sentences from wikipedia available to train on\nCounter({'N': 322591, 'Y': 5738}) numbers of each label from wikipedia\nReady to train.\nmvp! using existing tagger file\nSample test sentence and predicted/actual tags\nThe player with the most victory points at the end of the game is the winner .\n\nPredicted: N N N N N N N N N N N N N N N N N\nActual:   N N N N N N N N N N N N N N N N N\nCatan\n  class  precision  recall  f1_score  support\n0     N        1.0    1.00      1.00    65581\n1     Y        0.9    0.86      0.88     1123\nNumber of Sentences in Reddit text: 4466\nSome example sentences:\nWhen updates first started coming out it was all stuff locked away behind difficulties I would never conquer and was annoyed. However they've put out so much awesome content that addresses so much of the game and people who play that it's been great. So they put out some stuff I'll never see? Who cares they added early, mid, and late biomes, weapons, and enemies I will\nIt's a game that largely went ignored at release and has gathered steam, a following, and more attention/recognition. To me a forgettable game is one where the attention drops off, doesn't grow.\nBut I feel like the game has a lot of highs, there really isn't a lot like it and has a lot more great about it than it has flaws. There's a reason it's become more and more popular and while ignored a lot at release has become something you'll always find people to speak vocally about their love of the game.\nAn extremely low IGN review definitely took down Prey's average\nSeems like most people's hype died a bit at the reveal trailer when they saw it was a QD game.\nSeems like someone at QD couldn't stand bad press and put out this nothing of a PR statement that seems deliberately misunderstanding the original reporting. Luckily I have zero expectations for the game and still think we won't see anything for a long time.\nIf you're gaming to feel a better cultural connection to your friends and the world that hardly feels like a waste. However you do you, if you think of gaming as a waste of time there are a ton of other hobbies and things you can do.\nI think saying the store wasn't popular as a sign there isn't demand I don't think is a fair conclusion. This was well into the Past life when they wanted to shut it down and the audience for the games had moved onto other platforms. People weren't using the store because people weren't using the PS3. That doesn't necessarily mean they wouldn't buy PS3 games.\nShrug, I cared more about some of his other points like saving builds. However you seem pretty combative so I have no desire to elaborate or respond more than I already have.\nWell still have to use your upgrade materials but honestly I never saw the point of spending your ruins/souls/etc. Prices have always been at an amount it was a trivial amount to farm to spend it so I don't actually see the purpose. Also I feel like you're taking some of their points and pushing it the the extreme to the point of misrepresenting what they said.\n",
     "output_type": "stream"
    },
    {
     "output_type": "error",
     "ename": "KernelInterrupted",
     "evalue": "Execution interrupted by the Jupyter kernel.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKernelInterrupted\u001b[0m: Execution interrupted by the Jupyter kernel."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=90b052a7-f47d-474e-888f-9345355cfd9a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "009217dd-14b6-42c6-b78c-be29bb1bd40a",
  "deepnote_execution_queue": [],
  "deepnote_app_layout": "article"
 }
}