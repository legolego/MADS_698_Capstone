{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### Step 4: Find Influencer Relevant Posts\nThis step finds all posts made by the influential users in the identified subreddits in a provided timeframe. If there are more posts than the parameter limit, cosine similarity between the post and the best_wiki_cats is used to determine the top posts to move to the next step of the process.",
   "metadata": {
    "cell_id": "476cd4a013ed41a1863229aad00c1ad4",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 145.171875
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0c4eea50afd24f399d91bb3ffd4e2de6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "92268fa6",
    "execution_start": 1649704697861,
    "execution_millis": 6741,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 261
   },
   "source": "import praw \nimport pandas as pd\nimport time\nimport datetime as dt\nimport pmaw\nfrom datetime import datetime, timezone, timedelta, date\n\nimport pickle\n\nfrom sentence_transformers import SentenceTransformer, util\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "48bf774825be4aa3a9574a7ac648a609",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "288886e9",
    "execution_start": 1649704704629,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 189
   },
   "source": "## reddit and app credentials\n\nREDDIT_USERNAME = 'nbt_capstone '\nAPP_ID = '38HKH06bLOdSlpZIVtO5-w'\nAPP_SECRET = 'dmU0nd_1tYIDn9AldwjEDgYU-hj-Lw' \nAPP_NAME = 'nextbigthing'\nPASSWORD = 'Capstone698!'",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3e94fb56893c47ca8b7510c882b25a4f",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5d533a11",
    "execution_start": 1649704704640,
    "execution_millis": 54,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 189
   },
   "source": "#Set up the Reddit API in python\nreddit = praw.Reddit(client_id=APP_ID,\n                     client_secret=APP_SECRET,\n                     user_agent=REDDIT_USERNAME)\n\n#Set up pushshift\npmaw_api = pmaw.PushshiftAPI()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "698c04ad241548a9896a0f6acb0c5631",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e6c39962",
    "execution_start": 1649704704699,
    "execution_millis": 1233,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "#Load Sentence Transformer model\nmodel = SentenceTransformer('all-MiniLM-L12-v2')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "b493eeda-259f-46e3-b7cf-9ec9763589a5",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7092d6cf",
    "execution_start": 1649704705932,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 963
   },
   "source": "def mdy_to_timestamp(value: str):\n    '''Converts a date to timestamp\n\n    Input - date in mm/dd/yyyy format\n    Output - date in timestamp format\n    '''\n    my_val = dt.datetime.strptime(value, \"%m/%d/%Y\")\n    return int(my_val.timestamp())\n\ndef get_influencer_comments(author, subreddits, start_date, end_date,limit):\n    ''' Use pushshift to get comments based on author and subreddits within a time period\n\n        Inputs: author - string containing influential user\n                subreddits - list of subreddits found for our item\n                start_date - first date to include for comments\n                end_date - last date to include for comments\n                limit - max number of comments to return\n               \n        Output: df - comments created by the author in the subreddits  '''\n\n    before = mdy_to_timestamp(end_date)\n    after = mdy_to_timestamp(start_date)\n\n    comment_list = []\n    for subreddit in subreddits:\n        #print('Finding comments in the {} subreddit...'.format(subreddit))\n        comments = pmaw_api.search_comments(subreddit=subreddit, author=author, limit=limit, \n                                before=before, after=after,\n                                filter=['body','id','score','author','subreddit','created_utc'])\n    \n\n        comment_list.append(comments)\n\n    #Flatten comment list\n    comment_list = [item for sublist in comment_list for item in sublist]  \n\n    #If no comments are returned, exit\n    if len(comment_list) == 0:\n        return pd.DataFrame()  \n    \n    df = pd.DataFrame(comment_list)\n    \n    df['created'] = df['created_utc'].apply(lambda x: dt.datetime.fromtimestamp(int(x)).strftime('%Y-%m-%d'))\n    df['author'] = author\n    df['type'] = 'comment'\n    df['text_to_process'] = df['body']\n    df = df.drop(columns=['body'])\n\n    return df\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "306faa3bc65042f9a89047c58d526c18",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "740c0bf2",
    "allow_embed": false,
    "execution_start": 1649704705966,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 837
   },
   "source": "def get_influencer_submissions(author, subreddits, start_date, end_date,limit):\n    ''' Use pushshift to get submissions based on author and subreddits within a time period\n\n        Inputs: author - string containing influential user\n                subreddits - list of subreddits found for our item\n                start_date - first date to include for comments\n                end_date - last date to include for comments\n                limit - max number of comments to return\n               \n        Output: df - submissions created by the author in the subreddits  '''\n\n    before = mdy_to_timestamp(end_date)\n    after = mdy_to_timestamp(start_date)\n\n    submission_list = []\n    for subreddit in subreddits:\n        #print('Finding submissions in the {} subreddit...'.format(subreddit))\n        submissions = pmaw_api.search_submissions(subreddit=subreddit, author=author, limit=limit, \n                                before=before, after=after,\n                                filter=['selftext','id','score','author','title','subreddit',\n                                        'created_utc'])\n    \n\n        submission_list.append(submissions)\n\n    #Flatten submission list\n    submission_list = [item for sublist in submission_list for item in sublist]  \n\n    #If no submissions are returned, exit\n    if len(submission_list) == 0:\n        return pd.DataFrame()  \n    \n    df = pd.DataFrame(submission_list)\n    \n    df['created'] = df['created_utc'].apply(lambda x: dt.datetime.fromtimestamp(int(x)).strftime('%Y-%m-%d'))\n    df['author'] = author\n    df['type'] = 'submission'\n    df['text_to_process'] = df['title'] + '.' + df['selftext']\n    df = df.drop(columns=['title','selftext'])\n\n    return df\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "99439093bcf342f08ccf2000e423e535",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9ca1abdc",
    "execution_start": 1649704706027,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 603
   },
   "source": "def get_post_cos_sim_score(list_of_text, posts_df):\n    '''Computes cosine similarity for all posts against each item in a list, then calculates\n       the average cosine similarity for each post. Returns a dataframe where the posts are \n       sorted by descending cosine similiarity\n\n       Inputs:\n       list_of_text - list of strings that will be used to compare to each post\n       post_df - dataframe of reddit comments and submissions\n\n       Output:\n       final_df - dataframe of posts sorted by descending cosine similarity \n    '''\n    cosine_result = []\n    \n    posts = list(posts_df['text_to_process'])\n    posts_embeddings = model.encode(posts)\n\n    for text in list_of_text: \n        text_embedding= model.encode(text)\n\n        for post_num, post in enumerate(posts):\n            cos_similarity = util.pytorch_cos_sim(text_embedding, posts_embeddings[post_num]).numpy()[0][0]\n            cosine_result.append([text, post_num, post,cos_similarity])\n\n    cos_df = pd.DataFrame(cosine_result,columns=['text','post_num','post','cosine_similarity'])\n\n    final_df = cos_df.groupby(['post_num', 'post']).mean().reset_index().sort_values(by='cosine_similarity',ascending=False)\n    \n    \n    return final_df",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "b14d62b106a94a72899ba78f3c6a95f8",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f12a797f",
    "execution_start": 1649704706025,
    "execution_millis": 1,
    "owner_user_id": "d1c3db5a-4505-48be-99b3-4a4061f5f711",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 2187
   },
   "source": "def get_relevant_posts(orig_entry, authors, subreddits, start_date, end_date, wiki_categories, \n                        per_author_limit, post_limit, mvp_flag):\n    '''\n    This function retrieves comments and submissions for a group of Reddit authors\n    in specific subreddits based on a start date and end date. The number of comments/submissions\n    retrieved per author is limited by the per_author_limit parameter. Once all posts for all \n    authors are retrieved, a cosine similarity calculation is performed against the wikipedia\n    categories for the original item entered by the user. Then, the top posts are returned\n    based on the post_limit parameter\n\n    Inputs: \n        orig_entry - string containing original item entered by our user for the NBT\n        authors - list of reddit users\n        subreddits - list of subreddits\n        start_date - start date to use for posts\n        end_date - end date to use for posts\n        wiki_categories - list of wikipedia categories for the original item entered by the user\n        per_author_limit - max number of comments/submissions (separately) for each author in \n                           a subreddit\n        post_limit - max number of posts to return\n        mvp_flag - indicates if we are running a pre-aggregated data set for our minimum viable\n                   product\n\n    Output: \n        relevant_posts - a list of relevant comments and submissions \n    '''\n    #Set pickle file name, will either be creating it or loading it\n    orig_entry_mod = orig_entry.replace(' ','_')\n    file_name = './output_step4/posts_' + orig_entry_mod + '.pickle'\n\n    #If we are using a pre-aggregated comment set, load its pickle file\n    if mvp_flag:\n        print('MVP mode - loading pickle file...')\n\n        #Load pickle file of relevant posts\n        try:\n            with open(file_name, 'rb') as f:\n                relevant_posts = pickle.load(f)\n        except:\n            print('Unable to find pickle file for', orig_entry)\n            relevant_posts = []\n\n    else:\n        #Remove duplicate users in our influential users list\n        authors = list(set(authors))\n        \n        print('Start time for retrieving comments: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n        \n        #Remove 'Category:' from beginning of wikipedia categories\n        wiki_categories = [cat[9:] for cat in wiki_categories]\n\n        #Get comments for our influential users\n        comments_df_list = []\n        for author in authors:\n            print('Finding comments for {}...'.format(author))\n            author_comments_df = get_influencer_comments(author, subreddits, start_date, end_date,per_author_limit)\n            \n            #If there are comments for the author then add to our list\n            if len(author_comments_df) > 0:\n                comments_df_list.append(author_comments_df)\n\n        #Put all of the comments in one dataframe \n        if len(comments_df_list) > 0:\n            comments_df = pd.concat(comments_df_list)\n        else:\n            comments_df = pd.DataFrame()\n        print('Number of influential user comments: ', len(comments_df))\n        \n\n        #Get submissions for our influential users\n        print('Start time for retrieving submissions: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n        \n        submissions_df_list = []\n        for author in authors:\n            print('Finding submissions for {}...'.format(author))\n            author_submissions_df = get_influencer_submissions(author, subreddits,start_date, end_date, per_author_limit)\n\n        #If there are submissions for the author then add to our list\n            if len(author_submissions_df) > 0:\n                submissions_df_list.append(author_submissions_df)\n\n        #Put all of the submissions in one dataframe \n        if len(submissions_df_list) > 0:\n            submissions_df = pd.concat(submissions_df_list)\n        else:\n            submissions_df = pd.DataFrame()\n        print('Number of influential user submissions: ', len(submissions_df))\n       \n        #Put comments and submissions together\n        frames = [comments_df, submissions_df]\n        posts_df = pd.concat(frames)\n\n        #If we have too many posts, evaluate relevance and rank\n        print('Ranking posts start time: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n\n        if len(posts_df) > 0:\n        \n            if len(posts_df) > post_limit:\n                #Find average cosine similarity between posts and all wikipedia categories\n                wiki_categories_avg_score_df = get_post_cos_sim_score(wiki_categories, posts_df)\n                relevant_posts = list(wiki_categories_avg_score_df['post'])[:post_limit]\n\n            else:\n                #Return all posts\n                relevant_posts = list(posts_df['text_to_process'])\n        else:\n            print('No posts for these authors during the time period')\n            relevant_posts = []\n\n        #Create pickle file from our posts\n       \n        with open (file_name, 'wb') as f:\n            pickle.dump(relevant_posts, f)\n\n        print('Ranking posts end time: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n\n\n    return relevant_posts",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=90b052a7-f47d-474e-888f-9345355cfd9a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "83ee0863-890d-4e3b-8157-82c7047d9a8c",
  "deepnote_execution_queue": []
 }
}