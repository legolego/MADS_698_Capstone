{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### Step 4: Find Influencer Relevant Posts\nThis step finds all posts made by the influential users in the identified subreddits in a provided timeframe. If there are more posts than the parameter limit, cosine similarity between the post and the best_wiki_cats is used to determine the top posts to move to the next step of the process.",
   "metadata": {
    "cell_id": "476cd4a013ed41a1863229aad00c1ad4",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 145.171875
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "0c4eea50afd24f399d91bb3ffd4e2de6",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "92268fa6",
    "execution_start": 1649704697861,
    "execution_millis": 6741,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 351
   },
   "source": "import praw \nimport pandas as pd\nimport time\nimport datetime as dt\nimport pmaw\nfrom datetime import datetime, timezone, timedelta, date\n\nimport pickle\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nfrom sentence_transformers import SentenceTransformer, util\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "7c16d0572bbf4e13a1b4747a02e2b821",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "69658a85",
    "execution_start": 1649704704613,
    "execution_millis": 482,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 136.1875,
    "deepnote_output_heights": [
     21.1875
    ]
   },
   "source": "date.today().strftime(\"%m/%d/%Y\") \n(date.today() + timedelta(days=1)).strftime(\"%m/%d/%Y\") #'3/1/2022'",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 2,
     "data": {
      "text/plain": "'04/12/2022'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "48bf774825be4aa3a9574a7ac648a609",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "288886e9",
    "execution_start": 1649704704629,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 189
   },
   "source": "## reddit and app credentials\n\nREDDIT_USERNAME = 'kdicam'  \nREDDIT_PASSWORD = 'h3x69jGYiLesJPW' \nAPP_ID = 'wBJnSmnLH0JD5sg0yfkEDw' \nAPP_SECRET = 'a2w2ZgS8pCnzurFQSJrU5immYzRyvA' \nAPP_NAME = 'next-big-thing' ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "3e94fb56893c47ca8b7510c882b25a4f",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "5d533a11",
    "execution_start": 1649704704640,
    "execution_millis": 54,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 189
   },
   "source": "#Set up the Reddit API in python\nreddit = praw.Reddit(client_id=APP_ID,\n                     client_secret=APP_SECRET,\n                     user_agent=REDDIT_USERNAME)\n\n#Set up pushshift\npmaw_api = pmaw.PushshiftAPI()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "698c04ad241548a9896a0f6acb0c5631",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "e6c39962",
    "execution_start": 1649704704699,
    "execution_millis": 1233,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "#Load Sentence Transformer model\nmodel = SentenceTransformer('all-MiniLM-L12-v2')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "b493eeda-259f-46e3-b7cf-9ec9763589a5",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7092d6cf",
    "execution_start": 1649704705932,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 1431
   },
   "source": "def mdy_to_timestamp(value: str):\n    my_val = dt.datetime.strptime(value, \"%m/%d/%Y\")\n    return int(my_val.timestamp())\n\ndef get_influencer_comments_pmaw(author, subreddits, start_date, end_date,limit):\n    ''' Use pmaw pushshift to get comments based on author and subreddits within a time period\n\n        Inputs: author - string containing influential user\n                subreddits - list of subreddits found for our item\n                start_date - first date to include for comments\n                end_date - last date to include for comments\n                limit - max number of comments to return\n               \n        Output: df - comments created by the author in the subreddits  '''\n\n    #Note: Consider adding search term in here as well using q\n\n    before = mdy_to_timestamp(end_date)\n    after = mdy_to_timestamp(start_date)\n\n    comment_list = []\n    for subreddit in subreddits:\n        #print('Finding comments in the {} subreddit...'.format(subreddit))\n        comments = pmaw_api.search_comments(subreddit=subreddit, author=author, limit=limit, \n                                before=before, after=after,\n                                filter=['body','id','score','author','subreddit','created_utc'])\n    \n\n        comment_list.append(comments)\n\n    #Flatten comment list\n    comment_list = [item for sublist in comment_list for item in sublist]  \n\n    #If no comments are returned, exit\n    if len(comment_list) == 0:\n        return pd.DataFrame()  \n    \n    df = pd.DataFrame(comment_list)\n    \n    df['created'] = df['created_utc'].apply(lambda x: dt.datetime.fromtimestamp(int(x)).strftime('%Y-%m-%d'))\n    df['author'] = author\n    df['type'] = 'comment'\n    df['text_to_process'] = df['body']\n    df = df.drop(columns=['body'])\n\n    #print(df.head())\n    #print('# records is: ',len(df))\n    #print('Columns are: ',list(df.columns))\n    return df\n\ndef get_influencer_comments_w_submission(author, subreddits, start_date, end_date,limit):\n    ''' Get comment using pmaw pushshift, then use praw to grab original submission title\n        based on author and subreddits within a time period\n\n        Inputs: author - string containing influential user\n                subreddits - list of subreddits found for our NBT item\n                start_date - first date to include for comments\n                end_date - last date to include for comments\n               \n        Output: comments_df - comments created by the author in the subreddits, \n                with original submission title  '''\n\n    #print('Start time for retrieving comments: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n    comments_df = get_influencer_comments_pmaw(author, subreddits, start_date, end_date,limit)\n    #print('End time for retrieving comments: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n   \n    #If there are any comments, retrieve the original submission for each comment\n    #Commenting this if statement out - it is taking too long to get the title - can condense\n    # these two functions now\n    # if len(comments_df) > 0:\n    #     print('Start time for retrieving submission title: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n    #     comments_df['submission_title'] = comments_df['id'].apply(lambda x: reddit.comment(x).submission.title)\n    #     print('End time for retrieving submission title: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n    #     comments_df['title_and_body'] = comments_df['submission_title'] + '. ' + comments_df['body']\n    \n    return comments_df",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "306faa3bc65042f9a89047c58d526c18",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "740c0bf2",
    "allow_embed": false,
    "execution_start": 1649704705966,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 927
   },
   "source": "def get_influencer_submissions(author, subreddits, start_date, end_date,limit):\n    ''' Use pushshift to get submissions based on author and subreddits within a time period\n\n        Inputs: author - string containing influential user\n                subreddits - list of subreddits found for our item\n                start_date - first date to include for comments\n                end_date - last date to include for comments\n                limit - max number of comments to return\n               \n        Output: df - submissions created by the author in the subreddits  '''\n\n    #Note: Consider adding search term in here as well using q\n\n    before = mdy_to_timestamp(end_date)\n    after = mdy_to_timestamp(start_date)\n\n    submission_list = []\n    for subreddit in subreddits:\n        #print('Finding submissions in the {} subreddit...'.format(subreddit))\n        submissions = pmaw_api.search_submissions(subreddit=subreddit, author=author, limit=limit, \n                                before=before, after=after,\n                                filter=['selftext','id','score','author','title','subreddit',\n                                        'created_utc'])\n    \n\n        submission_list.append(submissions)\n\n    #Flatten submission list\n    submission_list = [item for sublist in submission_list for item in sublist]  \n\n    #If no submissions are returned, exit\n    if len(submission_list) == 0:\n        return pd.DataFrame()  \n    \n    df = pd.DataFrame(submission_list)\n    \n    df['created'] = df['created_utc'].apply(lambda x: dt.datetime.fromtimestamp(int(x)).strftime('%Y-%m-%d'))\n    df['author'] = author\n    df['type'] = 'submission'\n    df['text_to_process'] = df['title'] + '.' + df['selftext']\n    df = df.drop(columns=['title','selftext'])\n\n    #print(df.head())\n    #print('# records is: ',len(df))\n    #print('Columns are: ',list(df.columns))\n    return df\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "b14d62b106a94a72899ba78f3c6a95f8",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "f12a797f",
    "execution_start": 1649704706025,
    "execution_millis": 1,
    "owner_user_id": "f9c4ad17-97d3-4f24-b7b0-bc099d48c5ec",
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 2169
   },
   "source": "def get_relevant_posts(orig_entry, authors, subreddits, start_date, end_date, wiki_categories, \n                        per_author_limit, post_limit, mvp_flag):\n    '''\n    This function retrieves comments and submissions (not yet) for a group of Reddit authors\n    in specific subreddits based on a start date and end date. The number of comments/submissions\n    retrieved per author is limited by the per_author_limit parameter. Once all posts for all \n    authors are retrieved, a cosine similarity calculation is performed against the wikipedia\n    categories for the original item entered by the user. Then, the top posts are returned\n    based on the post_limit parameter\n\n    Inputs: \n        orig_entry - string containing original item entered by our user for the NBT\n        authors - list of reddit users\n        subreddits - list of subreddits\n        start_date - start date to use for posts\n        end_date - end date to use for posts\n        wiki_categories - list of wikipedia categories for the original item entered by the user\n        per_author_limit - max number of comments/submissions (separately) for each author in \n                           a subreddit\n        post_limit - max number of posts to return\n        mvp_flag - indicates if we are running a pre-aggregated data set for our minimum viable\n                   product\n\n    Output: \n        relevant_posts - a list of relevant comments and submissions \n    '''\n    #Set pickle file name, will either be creating it or loading it\n    orig_entry_mod = orig_entry.replace(' ','_')\n    file_name = './output_step4/posts_' + orig_entry_mod + '.pickle'\n\n    #If we are using a pre-aggregated comment set, load its pickle file\n    if mvp_flag:\n        print('MVP mode - loading pickle file...')\n\n        #Load pickle file of relevant posts\n        try:\n            with open(file_name, 'rb') as f:\n                relevant_posts = pickle.load(f)\n        except:\n            print('Unable to find pickle file for', orig_entry)\n            relevant_posts = []\n\n    else:\n        #Remove duplicate users in our influential users list\n        authors = list(set(authors))\n        \n        print('Start time for retrieving comments: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n        #Remove 'Category:' from beginning of wikipedia categories\n        wiki_categories = [cat[9:] for cat in wiki_categories]\n\n        #Get comments for our influential users\n        comments_df_list = []\n        for author in authors:\n            print('Finding comments for {}...'.format(author))\n            author_comments_df = get_influencer_comments_w_submission(author, subreddits, start_date, end_date,per_author_limit)\n            \n            #If there are comments for the author then add to our list\n            if len(author_comments_df) > 0:\n                comments_df_list.append(author_comments_df)\n\n        #Put all of the comments in one dataframe \n        if len(comments_df_list) > 0:\n            comments_df = pd.concat(comments_df_list)\n        else:\n            comments_df = pd.DataFrame()\n        print('Number of relevant comments: ', len(comments_df))\n        \n\n        #Get submissions for our influential users\n        print('Start time for retrieving submissions: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n        \n        submissions_df_list = []\n        for author in authors:\n            print('Finding submissions for {}...'.format(author))\n            author_submissions_df = get_influencer_submissions(author, subreddits,start_date, end_date, per_author_limit)\n\n        #If there are submissions for the author then add to our list\n            if len(author_submissions_df) > 0:\n                submissions_df_list.append(author_submissions_df)\n\n        #Put all of the submissions in one dataframe \n        if len(submissions_df_list) > 0:\n            submissions_df = pd.concat(submissions_df_list)\n        else:\n            submissions_df = pd.DataFrame()\n        print('Number of relevant submissions: ', len(submissions_df))\n       \n        #Put comments and submissions together\n        frames = [comments_df, submissions_df]\n        posts_df = pd.concat(frames)\n\n        #If we have too many posts, evaluate relevance and rank\n        print('Ranking posts start time: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n\n        if len(posts_df) > 0:\n        \n            if len(posts_df) > post_limit:\n                #Find average cosine similarity between posts and all wikipedia categories\n                wiki_categories_avg_score_df = get_post_cos_sim_score(wiki_categories, posts_df)\n                relevant_posts = list(wiki_categories_avg_score_df['post'])[:post_limit]\n\n            else:\n                #Return all posts\n                relevant_posts = list(posts_df['text_to_process'])\n        else:\n            print('No posts for these authors during the time period')\n            relevant_posts = []\n\n        #Create pickle file from our posts\n       \n        with open (file_name, 'wb') as f:\n            pickle.dump(relevant_posts, f)\n\n        print('Ranking posts end time: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n\n\n    return relevant_posts",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "b486d5f0becb4a38b5de6ac1a4201e9b",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "1224bacd",
    "execution_start": 1649704706026,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 693
   },
   "source": "# def get_post_cos_sim_score(list_of_text, posts_df, text_type):\n#     '''Computes cosine similarity for all posts against each item in a list, then calculates\n#        the average cosine similarity for each post. Returns a dataframe where the posts are \n#        sorted by descending cosine similiarity\n\n#        Inputs:\n#        list_of_text - list of strings that will be used to compare to each post\n#        post_df - dataframe of reddit comments and submissions\n#        text_type - indicates what the list_of_text represents, at this point we are only using\n#                    wiki_categories as this proved to have the best results\n\n#        Output:\n#        final_df - dataframe of posts sorted by descending cosine similarity \n#     '''\n#     cosine_result = []\n    \n#     posts = list(posts_df['text_to_process'])\n#     posts_embeddings = model.encode(posts)\n\n#     for text in list_of_text: \n#         text_embedding= model.encode(text)\n\n#         for i, post in enumerate(posts):\n#             cos_similarity = util.pytorch_cos_sim(text_embedding, posts_embeddings[i]).numpy()[0][0]\n#             cosine_result.append([text, post,cos_similarity])\n\n#     #Assign column name based on type of data that was sent in\n#     cos_df = pd.DataFrame(cosine_result,columns=[text_type,'post','cosine_similarity'])\n\n#     final_df = cos_df.groupby(['post']).mean().sort_values(by='cosine_similarity',ascending=False)\n\n#     final_df = final_df.rename(columns={'cosine_similarity':text_type + 'cos_sim'})\n#     #print(final_df)\n    \n#     return final_df",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "99439093bcf342f08ccf2000e423e535",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "9ca1abdc",
    "execution_start": 1649704706027,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 621
   },
   "source": "def get_post_cos_sim_score(list_of_text, posts_df):\n    '''Computes cosine similarity for all posts against each item in a list, then calculates\n       the average cosine similarity for each post. Returns a dataframe where the posts are \n       sorted by descending cosine similiarity\n\n       Inputs:\n       list_of_text - list of strings that will be used to compare to each post\n       post_df - dataframe of reddit comments and submissions\n\n       Output:\n       final_df - dataframe of posts sorted by descending cosine similarity \n    '''\n    cosine_result = []\n    \n    posts = list(posts_df['text_to_process'])\n    posts_embeddings = model.encode(posts)\n\n    for text in list_of_text: \n        text_embedding= model.encode(text)\n\n        for post_num, post in enumerate(posts):\n            cos_similarity = util.pytorch_cos_sim(text_embedding, posts_embeddings[post_num]).numpy()[0][0]\n            cosine_result.append([text, post_num, post,cos_similarity])\n\n    #Assign column name based on type of data that was sent in\n    cos_df = pd.DataFrame(cosine_result,columns=['text','post_num','post','cosine_similarity'])\n\n    final_df = cos_df.groupby(['post_num', 'post']).mean().reset_index().sort_values(by='cosine_similarity',ascending=False)\n    \n    \n    return final_df",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6b0dfc077ef549ea8dade4ddc80069f8",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "805da905",
    "execution_start": 1649704706027,
    "execution_millis": 1,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 513
   },
   "source": "#No longer using this\n# def vectorize_wiki_articles(articles_pickle_file, vectorizer_type):\n#     '''This function creates an average vector over all wikipedia articles in a pickle file\n    \n#     Inputs:\n#     articles_pickle_file - contains a dictionary for each wikipedia article related to our item\n#     vectorizer_type - indicates which vectorizer should be used\n    \n#     Outputs:\n#     avg_article_vector - a single row containing an average of each feature over all articles\n#     vectorizer - trained vectorizer'''\n    \n#     #Load in pickle file\n#     with open(articles_pickle_file, 'rb') as f:\n#         articles = pickle.load(f)\n    \n#     articles_values = [v for k,v in articles.items()]\n\n#     #vectorizer = CountVectorizer(stop_words = 'english', max_df = 0.8)#,ngram_range = (1,2))\n#     vectorizer = vectorizer_type(stop_words = 'english', max_df = 0.8)#,ngram_range = (1,2))\n#     article_vectors = vectorizer.fit_transform(articles_values)\n#     avg_article_vector = np.asarray(article_vectors.mean(axis=0))\n    \n#     return avg_article_vector, vectorizer\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "8e3f2959b62d469c8d3f414c9872199e",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "433863c3",
    "execution_start": 1649704706036,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 333
   },
   "source": "#No longer using this\n# def get_articles_post_cos_sim(avg_article_vector, vectorizer, posts_df):\n\n#     vectorized_posts = vectorizer.transform(posts_df['title_and_body'])\n#     print(vectorized_posts.shape)\n\n#     article_post_cos_sim_result = []\n#     for post in vectorized_posts:\n#         article_post_cos_sim_result.append(cosine_similarity(avg_article_vector, post)[0][0])\n\n\n#     df = posts_df[['title_and_body']]\n#     df['article_post_cos_sim_result'] = article_post_cos_sim_result\n#     df = df.sort_values('article_post_cos_sim_result', ascending = False)\n#     return df",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Main Program",
   "metadata": {
    "cell_id": "aa9bdacc8a444026b1e570f7350e5189",
    "tags": [],
    "deepnote_cell_type": "markdown",
    "deepnote_cell_height": 62
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "7b8d93e2199e4c78b421866a301a9938",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "19107b20",
    "execution_start": 1649704706037,
    "execution_millis": 21,
    "deepnote_table_state": {
     "pageSize": 10,
     "pageIndex": 3,
     "filters": [],
     "sortBy": []
    },
    "deepnote_table_loading": false,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 2007
   },
   "source": "# print('Start time: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n\n# orig_entry = 'Squid Game'\n# #authors = ['ZockerDog619','92sn','avidwatcherz']\n# # authors = ['Pizzacakecomic','None','avidwatcherz','ZockerDog619','92sn','palanquin_dva','SQUID_FUCKER',\n# #  'Invoke-the-Sunbird','MarvelsGrantMan136','Sprizys','thekyledavid','SolomonCRand','TrickyTalon','reddituser45673',\n# #  'Ok_Bite8099','tired_succulent','Alberts_Hat','spencermiddleton','Unknown_User_66','sergiocamposnt']\n\n# authors = ['asilvertintedrose',\n#  'AutoLovepon',\n#  'WhoiusBarrel',\n#  'Lovro26',\n#  'AutoShonenpon',\n#  'realrimurutempest',\n#  'Celized',\n#  'elfratar',\n#  'DekMelU',\n#  'magnwn',\n#  'Ok-Okra-5033',\n#  'Ragernarate',\n#  'zenzen_0',\n#  'meh_potato',\n#  'SocialLoser739',\n#  'Yaggamy',\n#  'Mhogen',\n#  'Ghoste-Face',\n#  'LeonKevlar',\n#  'dorkmax_executives',\n#  'Kirosh2',\n#  'Turbostrider27',\n#  'hell-schwarz',\n#  'shanks_you',\n#  'Hachirumi',\n#  'OTPh1l25',\n#  'DogusEUW',\n#  'Prince-Dizzytoon',\n#  'MarvelsGrantMan136',\n#  'MD_AM',\n#  'Abysswatcherbel',\n#  'Mazen141',\n#  'Aerodynamic41',\n#  'Rulaku',\n#  '_non_royal',\n#  'Kezja',\n#  'Se7en_Sinner',\n#  'TurkeyPhat',\n#  'Zaibatsu_HQ',\n#  'fanime693',\n#  'Ni7roM',\n#  'xX_Edgyname_Xx',\n#  'EdSaPro',\n#  'Xanirran',\n#  'steven4869',\n#  'Aniboy43',\n#  'pietya',\n#  'Sneakynation',\n#  'Fools_Requiem',\n#  'Shimmering-Sky']\n\n# subreddits = ['korea',\n#  'squidgame',\n#  'KDRAMA',\n#  'MangaCollectors',\n#  'NANIKPosting',\n#  'manga',\n#  'yourturntodie',\n#  'ObscureMedia',\n#  'anime',\n#  'GlobalOffensive',\n#  'WoT',\n#  'scifi',\n#  'ActionFigures',\n#  'television',\n#  'Sonsofanarchy',\n#  'startrek',\n#  'dvdcollection',\n#  'DunderMifflin',\n#  'XboxSeriesS',\n#  'Documentaries']\n \n# wiki_categories = ['2021 South Korean television series debuts', 'South Korean action television series', 'South Korean horror fiction television series', 'South Korean thriller television series']\n# ['Category:2021 South Korean television series debuts',\n#   'Category:South Korean action television series',\n#   'Category:South Korean horror fiction television series',\n#   'Category:South Korean thriller television series',\n#   'Category:Television shows set in Seoul']\n\n# # wiki_summary = ['''Squid Game (Korean: 오징어 게임; RR: Ojing-eo Geim) is a South Korean survival drama television series created by Hwang Dong-hyuk for Netflix. Its cast includes Lee Jung-jae, Park Hae-soo, Wi Ha-joon, HoYeon Jung, O Yeong-su, Heo Sung-tae, Anupam Tripathi, and Kim Joo-ryoung.\n# # The series revolves around a contest where 456 players, all of whom are in deep financial debt, risk their lives to play a series of deadly children's games for the chance to win a ₩45.6 billion[a] prize. The title of the series draws from a similarly named Korean children's game. Hwang had conceived of the idea based on his own economic struggles early in life, as well as the class disparity in South Korea. Though he had initially written it in 2009, he was unable to find a production company to fund the idea until Netflix took an interest around 2019 as part of their drive to expand their foreign programming offerings.\n# # Squid Game was released worldwide on September 17, 2021, to critical acclaim and international attention. It is Netflix's most-watched series, becoming the top-viewed program in 94 countries and attracting more than 142 million member households and amassing 1.65 billion viewing hours during its first four weeks from launch, surpassing Bridgerton for the title of most watched show. The series has also received numerous accolades, including the Golden Globe Award for Best Supporting Actor – Series, Miniseries or Television Film for O Yeong-su and the Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Drama Series and Outstanding Performance by a Female Actor in a Drama Series for Lee Jung-jae and HoYeon Jung, respectively, with all three making history as the first Korean actors to win in those categories. A second season is in development.''']\n\n\n# per_author_limit=500\n# post_limit = 2000\n\n#Set dates to today and today - 30 days (api uses after start_date and before end_date)\n# end_date = (date.today() + timedelta(days=1)).strftime(\"%m/%d/%Y\") \n# start_date = (date.today() - timedelta(days=31)).strftime(\"%m/%d/%Y\") \n# mvp_flag = False #True uses an existing pickle file\n\n# #Get relevant posts for our influential users\n# print('Retrieving posts start time: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n# relevant_posts = get_relevant_posts(orig_entry, authors, subreddits, start_date, end_date,wiki_categories, per_author_limit, post_limit,mvp_flag)\n# print('Retrieving posts end time: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n\n\n# print('End time: ',datetime.now(timezone(timedelta(hours=-4), 'EST')).strftime('%Y-%m-%d %H:%M:%S'))\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "57e97299a3f94fff91694042c5a906b7",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "c273045",
    "execution_start": 1649704706058,
    "execution_millis": 0,
    "deepnote_table_state": {
     "pageSize": 10,
     "pageIndex": 20,
     "filters": [],
     "sortBy": []
    },
    "deepnote_table_loading": false,
    "deepnote_table_invalid": false,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 99
   },
   "source": "#relevant_posts\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "e1f4a3cf65cd425d8c646b0c45ac5ec8",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "bf1a50b8",
    "execution_start": 1649704706071,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "#len(relevant_posts)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6aa7b870b14c48d9864ed8890618cef8",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "eca37dd6",
    "execution_start": 1649704706079,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 531
   },
   "source": "\n\n# #Evaluate posts relevance\n# if len(posts_df) > 0:\n    \n#     #Find average cosine similarity between posts and all wikipedia categories\n#     wiki_categories_avg_score_df = get_post_cos_sim_score(wiki_categories, posts_df, 'wiki_categories')\n    \n#     # #The evaluation was completed and the best cosine similarity comes from the wikipedia \n#     # # categories\n#     # #Find cosine similarity between posts and the wikipedia summary\n#     # wiki_summary_score_df = get_post_cos_sim_score(wiki_summary, posts_df, 'wiki_summary')\n    \n#     # #Find cosine similarity between posts and all wikipedia articles using count vectorizer\n#     # avg_article_vector, vectorizer = vectorize_wiki_articles('/work/wiki_100summaries_Squid_Game.pkl',CountVectorizer)\n#     # wiki_articles_count_score_df = get_articles_post_cos_sim(avg_article_vector, vectorizer, posts_df)\n#     # wiki_articles_count_score_df = wiki_articles_count_score_df.rename(columns={'article_post_cos_sim_result':'article_post_count_cos_sim_result'})\n    \n#     # #Find cosine similarity between posts and all wikipedia articles using tfidf vectorizer\n#     # avg_article_vector, vectorizer = vectorize_wiki_articles('/work/wiki_100summaries_Squid_Game.pkl',TfidfVectorizer)\n#     # wiki_articles_tfidf_score_df = get_articles_post_cos_sim(avg_article_vector, vectorizer, posts_df)\n#     # wiki_articles_tfidf_score_df = wiki_articles_tfidf_score_df.rename(columns={'article_post_cos_sim_result':'article_post_tfidf_cos_sim_result'})\n\n# else:\n#     print('No comments for these authors during the time period')\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "6df5aed1d646471da761172742216279",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2bbb024d",
    "execution_start": 1649704706079,
    "execution_millis": 13,
    "deepnote_table_state": {
     "pageSize": 10,
     "pageIndex": 38,
     "filters": [],
     "sortBy": [
      {
       "id": "wiki_summarycos_sim",
       "type": "desc"
      }
     ]
    },
    "deepnote_table_loading": false,
    "deepnote_table_invalid": false,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 297
   },
   "source": "#Comparison is complete - no longer needed\n# comparison_df = wiki_categories_avg_score_df.merge(wiki_summary_score_df, how = 'inner', \n#                         left_index = True, right_index = True)\n\n# comparison_df = comparison_df.merge(wiki_articles_count_score_df, how = 'inner', left_on = comparison_df.index, \n#                         right_on = 'title_and_body')\n\n# comparison_df = comparison_df.merge(wiki_articles_tfidf_score_df, how = 'inner', on = 'title_and_body')\n\n# comparison_df.to_csv('/work/output/compare_post_cosine_similarities.csv')\n\n# comparison_df\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "7b7b791b78b34b81b79df111e0c92081",
    "tags": [],
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b623e53d",
    "execution_start": 1649704706093,
    "execution_millis": 0,
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 81
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=90b052a7-f47d-474e-888f-9345355cfd9a' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "83ee0863-890d-4e3b-8157-82c7047d9a8c",
  "deepnote_execution_queue": []
 }
}